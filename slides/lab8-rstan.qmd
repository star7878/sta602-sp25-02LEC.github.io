---
title: "Easy Bayesian linear modeling"
format: 
    revealjs:
      mainfont: Lato
      smaller: true
---

# `rstanarm` and `bayesplot`

## Download

To download `rstanarm` and `bayesplot` run the code below

```{r}
#| echo: true
#| eval: false
install.packages("rstanarm", "bayesplot")
```

To load the packages, run 

```{r}
#| warning: false
library(rstanarm)
library(bayesplot)
```


## Overview

- `rstanarm` contains a host of functions to make Bayesian linear modeling in R easy. See [https://mc-stan.org/rstanarm/articles/](https://mc-stan.org/rstanarm/articles/) for a variety of tutorials.

  - pros: easy to test Bayesian linear models, can be fast (uses Hamiltonian Monte Carlo proposals)
  
  - cons: limited in scope, e.g. requires differentiable objective and small model adjustments can be cumbersome to implement, e.g. placing a  prior on variance versus standard deviation of normal model.

- `bayesplot` contains many useful plotting wrappers that work out of the box with objects created by `rstanarm` in an intuitive way.

## Example

:::panel-tabset

## Load data
```{r}
#| warning: false
#| message: false
library(tidyverse)
spam = read_csv(
  "https://sta360-fa24.github.io/data/spam.csv")
```

## Glimpse data

```{r}
glimpse(spam)
```
:::

**Description**

4601 emails sent to the inbox of someone named "George" that are classified as `type` = 1 (spam) or 0 (non-spam). The data was collected at Hewlett-Packard labs and contains 58 variables. The first 48 variables are specific keywords and each observation is the percentage of appearance (frequency) of that word in the message. Click [here](https://rdrr.io/cran/kernlab/man/spam.html) to read more.

## Exercise Overview

You want to build a spam filter that blocks emails that have a high probability of being spam.

In statistical terms, your outcome $Y$ is the type of the email (spam or not). The 57 predictors from the data set are contained in $x$ and include: the frequency of certain words, the occurrence of certain symbols and the use of capital letters in each email.

Let $X = \{x_1, \ldots, x_n\}$ where $x_i \in \mathbb{R}^{58}$ (number of predictors + 1 for intercept) and $n =$ the number of emails in the data set. $Y \in \mathbb{R}^n$.

$$
\begin{aligned}
p(y_i =1 | x_i, \beta) = \theta_i\\
\text{logit}(\theta_i) = X\beta
\end{aligned}
$$

A priori, you believe that many of the predictors included in the data set do not in fact help you predict whether the email is spam. You express your beliefs with the prior designated below:

::: panel-tabset

## Prior on intercept

Let $\beta_0$ be the intercept term.

$$
\beta_0 \sim Normal(0, 100)
$$

## Prior on the rest

For each $\beta$ associated with the 57 predictors:

$$
\beta_i \sim \text{iid}~Laplace(0, .5) \ \ \text{for }i \in \{1, 57\}{}
$$
:::

## Standardize the data 

:::panel-tabset

## Standardize data

- Before we fit our model to the data, we need to standardize the predictors (columns of $X$). Why is this important? Discuss.

```{r}
# scale functions re-scales columns of a df
spam2 = cbind("type" = spam$type, 
              scale(select(spam, -"type"))) %>%
  data.frame()
```

## Training set

To validate our model we will separate it into non-overlapping sets -- a training set and a testing set. 

```{r}
set.seed(360) # ensures we get the same subset
N = nrow(spam2)
indices = sample(N, size = 0.8 * N)
spam_train = spam2[indices,]
spam_test = spam2[-indices,]
# sanity check 
nrow(spam_train) + nrow(spam_test) == N
```

:::

## Exercise 1 

Read about how to fit Bayesian logistic regression using `rstanarm` here: [https://mc-stan.org/rstanarm/articles/binomial.html](https://mc-stan.org/rstanarm/articles/binomial.html) and write code to fit the `spam_train` data set. 

Hint: use the `stan_glm` function. If you set arguments  `chains = 1`, this will run 1 Markov chain instead of the default 4. You can use the argument `iter=2000` to manually set the number of iterations in your Markov chain to 2000. This may take anywhere from 1-4 minutes to run locally on your machine. If you are pressed for time, you can load the resulting object directly from the website using the code below.

```{r}
fit1 = readRDS(url("https://sta360-fa24.github.io/data/spam-train-fit.rds"))
```

## Exercise 2

**Examining the output**

- Did `stan_glm` do what we think it did? Did the Markov chain converge? Which parameters, if any, have a posterior mean rounded to 0? 

::: panel-tabset

## quick look

Notice `sample: 1000` since half get thrown away by `stan` and is called "burn-in" i.e. a period that the chain spends reaching the target distribution gets discarded.

```{r}
summary(fit1)
```

## check priors

```{r}
prior_summary(fit1)
```
## trace plots

```{r}
#| fig-width: 5
#| fig-height: 3
betaNames = names(spam_train)[2:7]
betaNames
mcmc_trace(fit1, pars = betaNames)
```
## marginal posteriors

```{r}
#| warning: false
#| fig-width: 5
#| fig-height: 3
betaNames = names(spam_train)[2:7]
betaNames
mcmc_hist(fit1, pars = c(betaNames))
```
## plotting tips

To plot specific parameters, use the arguemnt `pars`, e.g.

- `mcmc_trace(fit1, pars = c("internet", "george")`
- `mcmc_hist(fit1, pars = "make")`

To read more about `bayesplot` functionality, see [https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html](https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html)

<!-- ## residual plot -->

<!-- ```{r} -->
<!-- #| fig-width: 5 -->
<!-- #| fig-height: 3 -->
<!-- df = data.frame(yhat = fit1$fitted.values, -->
<!--                 residual = fit1$residuals) -->

<!-- df %>% -->
<!--   ggplot(aes(x = yhat, y = residual)) + -->
<!--   geom_point() + -->
<!--   theme_bw() -->

<!-- ``` -->
## get chain

```{r}
chain_draws = as_draws(fit1)
chain_draws$george[1:5] # first 5 samples of the first chain run by stan
```

- try the following command: `View(chain_draws)`

## summarize

Report posterior mean, posterior median and 90% posterior CI.

```{r}
#| warning: false
posteriorMean = apply(chain_draws, 2, mean)
posteriorMedian = fit1$coefficients
posteriorCI = posterior_interval(fit1, prob = 0.9)
cbind(posteriorMean, posteriorMedian, posteriorCI)
```
:::

## Exercise 3

- Test your spam filter on the `spam_test` data set.

- Make a table showing correct and incorrect number of classifications.


# Solutions

## Exercise 1 solution

```{r}
#| echo: true
#| eval: false
fit1 = stan_glm(type ~ ., data = spam_train,
                 family = binomial(link = "logit"),
                 prior = laplace(0, 0.5),
                 prior_intercept = normal(0, 10),
                 cores = 2, seed = 360,
                chains = 1, iter = 2000)
```

## Exercise 2 Solution

- Trace plots look good, can look through more by subsetting others.

- ESS is high

- `receive`, `report`, `num1999`, `capitalAve` 

## Exercise 3 Solution

```{r}
predicted_spam = posterior_predict(object = fit1, newdata = spam_test)

data.frame(y = spam_test$type, 
           yhat = predict(object = fit1, newdata = spam_test[,-1], type = "response")) %>%
  mutate(yhat = ifelse(yhat >= 0.5, 1, 0)) %>%
  count(y, yhat)
```

856/921 classifications correct with a cutoff of 0.5. 

