[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian methods and modern statistics",
    "section": "",
    "text": "Week\nDate\nTopic\nReading\nNotes\nAssignment\n\n\n\n\n1\nMon Aug 26\nintro, history, notation\nCh. 2\n\nhw 0\n\n\n\nWed Aug 28\nprobability, exchangeability\nCh. 2\nüóí üìù\nhw 1\n\n\n\nThu Aug 29\nlab: welcome\n\nüíª\nhello R\n\n\n2\nMon Sep 02\nNO CLASS\nCh. 3\n\n\n\n\n\nWed Sep 04\nbeta-binomial model\nCh. 3\nüóí üìù\nhw 2\n\n\n\nThu Sep 05\nlab: MLE and MAP estimator\n\nüíª\n\n\n\n3\nMon Sep 09\nPoisson-gamma model, exp families\nCh. 3\nüóí üìù\n\n\n\n\nWed Sep 11\nreliability (conf. intervals, hpd, Laplace approx.)\nCh. 3\nüóí üìù\nhw 3\n\n\n\nThu Sep 12\nlab: exp. families and transformations\n\nüíª\n\n\n\n4\nMon Sep 16\nintro to Monte Carlo\nCh. 4\nüóí üìù\n\n\n\n\nWed Sep 18\npredictive checks and MC error\nCh. 4\nüóí\nhw 4\n\n\n\nThu Sep 19\nlab: mixture densities\n\nüíª\n\n\n\n5\nMon Sep 23\nthe normal model\nCh. 5\nüóíüìù\n\n\n\n\nWed Sep 25\nthe normal model II\nCh. 5\nüìù\n\n\n\n\nThu Sep 26\nlab: normal data\n\nüíª\n\n\n\n6\nMon Sep 30\nreview\n\n\n\n\n\n\nWed Oct 02\nExam I\n\n\n\n\n\n\nThu Oct 03\nNO LAB\n\n\n\n\n\n7\nMon Oct 07\nMetropolis algorithm\nCh. 10\nüóí\n\n\n\n\nWed Oct 09\nMCMC diagnostics\nCh. 6\nüóí\nhw 5\n\n\n\nThu Oct 10\nlab: Metropolis algo.\n\nüíª\n\n\n\n8\nMon Oct 14\nNO CLASS\n\n\n\n\n\n\nWed Oct 16\nwrap-up diagnostics and MH\nCh. 6, 10\nüìù\n\n\n\n\nThu Oct 17\nlab: MCMC and conf. bands\n\nüíª\n\n\n\n9\nMon Oct 21\nGibbs sampling\nCh. 6\nüóí\n\n\n\n\nWed Oct 23\nMultivariate normal\nCh. 7\nüóí\nhw 6\n\n\n\nThu Oct 24\nlab: MCMC diagnostics\n\nüíª\n\n\n\n10\nMon Oct 28\nintro to Bayesian regression\nCh. 9\nüóíüìù\n\n\n\n\nWed Oct 30\nBayesian regression II\nCh. 9\nüóí\nhw 7\n\n\n\nThu Oct 31\nlab: rstanarm\n\nüíª\n\n\n\n11\nMon Nov 04\nEstimators\nCh. 5 sec.¬†4\nüóíüìù\n\n\n\n\nWed Nov 06\nHierarchical modeling\nCh. 8\nüóíüìù\nhw 8\n\n\n\nThu Nov 07\nlab: estimators\n\nüíª\n\n\n\n12\nMon Nov 11\nreview\n\n\n\n\n\n\nWed Nov 13\nExam II\n\n\n\n\n\n\nThu Nov 14\nNO LAB\n\n\n\n\n\n13\nMon Nov 18\nPriors\n\nüóí\n\n\n\n\nWed Nov 20\nBayesian inverse problems\nüìñ\nüóí\nhw 9\n\n\n\nThu Nov 21\nlab\n\n\n\n\n\n14\nMon Nov 25\nHamiltonian Monte Carlo\n\n\n\n\n\n\nWed Nov 27\nNO CLASS\n\n\n\n\n\n\nThu Nov 28\nNO CLASS\n\n\n\n\n\n15\nMon Dec 02\nModel averaging\n\n\n\n\n\n\nWed Dec 04\nreview\n\n\n\n\n\n\nThu Dec 05\nlab"
  },
  {
    "objectID": "notes/lec02-estimation.html",
    "href": "notes/lec02-estimation.html",
    "title": "Is this a fair coin?",
    "section": "",
    "text": "outputcode\n\n\n\n\n [1] 0 1 0 0 0 0 0 0 0 0\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(latex2exp)\nlibrary(glue)\nlibrary(patchwork)\n\nset.seed(3)\nflips = rbinom(5000, size = 1, prob = 0.25)\nflips %>% head(10)\nWe observe 10 flips from the same coin above, where 0 is ‚Äútails‚Äù and 1 is ‚Äúheads‚Äù. In summary, we see Y = 1 heads in 10 coin flips. Is this a fair coin?\nTo articulate this mathematically, let \\(\\theta \\in [0, 1]\\) be the bias-weighting (the chance of heads) of the coin. Fundamentally, we want \\(p(\\theta | y)\\), which we can expand via Bayes‚Äô rule,\n\\[\np(\\theta | y) = \\frac{p(y|\\theta) p(\\theta)}{\\int_{\\theta \\in \\Theta} p(y|\\theta) p(\\theta) d\\theta}\n\\]\nLikelihood: the data generative process. The joint probability (or density) of the data given the parameters of the model. Most often thought of as a function of the parameter. Note: not a density of the parameter.\nPrior: Our a priori (beforehand) beliefs about the true population characteristics.\nPosterior: Our a posteriori (afterwards) beliefs about the true population characteristics after having observed the data set \\(y\\).\nNormalizing constant: A number that enables a pmf or pdf to integrate to 1."
  },
  {
    "objectID": "notes/lec02-estimation.html#uniform-prior",
    "href": "notes/lec02-estimation.html#uniform-prior",
    "title": "Is this a fair coin?",
    "section": "Uniform prior",
    "text": "Uniform prior\nLet \\(y\\) be the number of heads in \\(n\\) coin flips.\n\\[\np(\\theta | y) \\propto \\theta^{y}(1-\\theta)^{n-y}\n\\]\nThis is the kernel of a ___ density, where \\(\\alpha = y + 1\\) and \\(\\beta = n - y + 1\\), hence\n\\[\np(\\theta | y) = \\frac{\\Gamma(n + 2)}{\\Gamma(y + 1)\\Gamma(n-y+1)} \\theta^{y}(1-\\theta)^{n-y}\n\\]\nand the posterior mean is \\(\\frac{y + 1}{n + 2}\\) and the posterior variance is \\(\\frac{(y+1)(n - y + 1)}{(n + 2)^2 (n + 1)}\\).\nLet‚Äôs examine how the posterior evolves with each successive coin flip.\n\nplotscode\n\n\n\n\n\n\n\n\n\n\nN = c(0, 1, 2, 3, 4, 10, 100, 1000, 5000)\n\nfor (i in seq_along(N)) {\nn = N[i]\nif(n == 0) {\n  y = 0\n}\nelse {\n  y = sum(flips[1:n])\n}\n\nx = 0:1 # range\ndf = data.frame(x)\nassign(paste0(\"p\", i),\n  df %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun=dbeta, \n                args = list(shape1 = y + 1, shape2 = n - y + 1)) +\n  labs(y = TeX(\"$p(\\\\theta | y)$\"), x = TeX(\"$\\\\theta$\"),\n       title = glue(\"n = {n}\")) +\n  theme_bw()\n)\n}\n\n(p1 + p2 + p3) / \n  (p4 + p5 + p6) / \n  (p7 + p8 + p9) +\n  plot_annotation(title = \"Figure 1\")"
  },
  {
    "objectID": "notes/lec02-estimation.html#conjugacy",
    "href": "notes/lec02-estimation.html#conjugacy",
    "title": "Is this a fair coin?",
    "section": "Conjugacy",
    "text": "Conjugacy\nIf \\(\\theta \\sim\\) Uniform(0, 1) then \\(p(\\theta)\\) = 1 for all \\(\\theta \\in [0, 1]\\).\nSimilarly, if \\(\\theta \\sim\\) beta(1, 1), then \\(p(\\theta) = 1\\).\nClaim:\nIf\n\\[\n\\begin{aligned}\n\\theta &\\sim \\text{beta}(a, b)\\\\\nY | \\theta &\\sim \\text{binomial}(n, \\theta)\n\\end{aligned}\n\\]\nthen\n\\[\np(\\theta | Y) = \\text{beta}(y + a, n - y + b)\n\\]\n\n\n\n\n\n\nDefinition\n\n\n\nA prior \\(p(\\theta)\\) is said to be conjugate to the data generative model \\(p(y|\\theta)\\) if the family of the posterior is necessarily in the same family as the prior. In math, \\(p(\\theta)\\) is conjugate to \\(p(y|\\theta)\\) if\n\\[\np(\\theta) \\in \\mathcal{P} \\implies p(\\theta | y) \\in \\mathcal{P}\n\\]\n\n\nWhile conjugate priors make calculation easy, they may not accurately reflect our prior beliefs.\n\n\n\n\n\n\nExercise\n\n\n\nProve the claim above."
  },
  {
    "objectID": "notes/lec02-estimation.html#other-priors",
    "href": "notes/lec02-estimation.html#other-priors",
    "title": "Is this a fair coin?",
    "section": "Other priors",
    "text": "Other priors\nIncidentally, people are often satisfied with the choice of likelihood but are worried about the choice of prior.\nLet‚Äôs examine the effect of another couple of priors.\nGiven the coin‚Äôs dubious origin, we might believe a priori that the coin is biased. How could we represent this belief?\n\\[\n\\theta \\sim \\text{beta}(.5, .5)\n\\]\nOr alternatively, we might be strongly believe a priori that the coin is fair. How could we represent this belief?\n\\[\n\\theta \\sim \\text{beta}(20, 20)\n\\]\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nHow would you update the code of the previous example to show posterior inference under the prior \\(\\theta \\sim\\) beta(2,3)?"
  },
  {
    "objectID": "notes/lec02-estimation.html#prior-data",
    "href": "notes/lec02-estimation.html#prior-data",
    "title": "Is this a fair coin?",
    "section": "Prior data",
    "text": "Prior data\nIn the example above, the parameters, a and b, of the conjugate prior are often thought of as prior data.\n\na: ‚Äúprior number of 1s‚Äù\nb: ‚Äúprior number of 0s‚Äù\na + b: ‚Äúprior sample size‚Äù\n\n\n\n\n\n\n\nExercise\n\n\n\nWe saw above that when a = 20 and b = 20, we needed more data to move the posterior.\nShow that the posterior mean, \\(E(\\theta | y) = \\frac{a + y}{a + b + n}\\) converges to the sample average as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "notes/lec03-reliability.html",
    "href": "notes/lec03-reliability.html",
    "title": "Posterior summaries and reliability",
    "section": "",
    "text": "Posterior mode: sometimes called ‚ÄúMAP‚Äù or ‚Äúmaximum a posteriori‚Äù estimate, this quantity is given by \\(\\hat{\\theta} = \\arg \\max_{\\theta} p(\\theta | y)\\).\n\nNotice this unwinds to be \\(\\hat{\\theta} = \\arg \\max_{\\theta} p(y | \\theta) p(\\theta)\\).\n\n\n\n\n\n\n\nExercise\n\n\n\n\nShow that, for the uniform prior, \\(\\hat{\\theta} = y / n\\)\nCompare to maximum likelihood estimate (MLE); see notes on likelihoods\n\n\n\nOne way to report the reliability of the posterior mode is to look at the width of the posterior near the mode, which we can sometimes approximate with a Gaussian distribution:\n\\[\np(\\theta | y) \\approx C e^{\\frac{1}{2} \\frac{d^2L}{d\\theta^2}|_{\\hat{\\theta}} (\\theta - \\hat{\\theta})^2}\n\\]\nwhere \\(C\\) is a normalization constant and \\(L\\) is the log-posterior, \\(\\log p(\\theta | y)\\).\nTaken together, the fitted Gaussian with a mean equal to the posterior mode is called the Laplace approximation.\n\nLet‚Äôs derive the Laplace approximation offline"
  },
  {
    "objectID": "notes/lec03-reliability.html#confidence-regions",
    "href": "notes/lec03-reliability.html#confidence-regions",
    "title": "Posterior summaries and reliability",
    "section": "Confidence regions",
    "text": "Confidence regions\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(\\Phi\\) be the support of \\(\\theta\\). An interval \\((l(y), u(y)) \\subset \\Phi\\) has 95% posterior coverage if\n\\[\np(l(y) < \\theta < u(y) | y ) = 0.95\n\\]\nInterpretation: after observing \\(Y = y\\), our probability that \\(\\theta \\in (l(y), u(y))\\) is 95%.\nSuch an interval is called 95% posterior confidence interval (CI). It may also sometimes be referred to as a 95% ‚Äúcredible interval‚Äù to distinguish it from a frequentist CI.\n\n\nContrast posterior coverage to frequentist coverage:\n\n\n\n\n\n\nDefinition\n\n\n\nA random interval \\((l(Y), u(Y)\\)) has 95% frequentist coverage for \\(\\theta\\) if before data are observed,\n\\[\np(l(Y) < \\theta < u(Y) | \\theta) = 0.95\n\\]\nInterpretation: if \\(Y \\sim P_\\theta\\) then the probability that \\((l(Y), u(Y)\\) will cover \\(\\theta\\) is 0.95.\n\n\nIn practice, for many applied problems\n\\[\np(l(y) < \\theta < u(y) | y ) \\approx p(l(Y) < \\theta < u(Y) | \\theta)\n\\]\nsee section 3.1.2. in the book."
  },
  {
    "objectID": "notes/lec03-reliability.html#high-posterior-density",
    "href": "notes/lec03-reliability.html#high-posterior-density",
    "title": "Posterior summaries and reliability",
    "section": "High posterior density",
    "text": "High posterior density\n\n\n\n\n\n\nDefinition\n\n\n\nA \\(100 \\times (1-\\alpha)\\)% high posterior density (HPD) region is a set \\(s(y) \\subset \\Theta\\) such that\n\n\\(p(\\theta \\in s(y) | Y = y) = 1 - \\alpha\\)\nIf \\(\\theta_a \\in s(y)\\) and \\(\\theta_b \\not\\in s(y)\\), then \\(p(\\theta_a | Y = y) > p(\\theta_b | Y = y)\\)\n\n\n\n\nNote: all points inside an HPD region have higher posterior density than points outside the region.\n\n\n\n\n\n\n\nExercise\n\n\n\nIs the HPD region always an interval?"
  },
  {
    "objectID": "notes/lec04-prediction.html",
    "href": "notes/lec04-prediction.html",
    "title": "Prediction and Intro to Monte Carlo",
    "section": "",
    "text": "Load packages:"
  },
  {
    "objectID": "notes/lec04-prediction.html#prediction-example",
    "href": "notes/lec04-prediction.html#prediction-example",
    "title": "Prediction and Intro to Monte Carlo",
    "section": "Prediction example",
    "text": "Prediction example\nGeneral social survey (1998)\nSetup:\n\nSuppose \\(X_i = 1\\) if the ith person is happy. \\(X_i = 0\\) otherwise.\nLet \\(Y = \\sum_{i = 1}^{n} X_i\\), where \\(n\\) is the number of people sampled.\n\\(Y_i | \\theta \\sim \\text{binomial}(\\theta)\\) for some fixed \\(n\\).\n\\(\\theta \\sim \\text{uniform}(0, 1)\\)\n\nScenario: We sample \\(n = 10\\) people. \\(y = 6\\) are happy. If we sample another \\(n = 10\\), what is the probability that \\(\\tilde{y}\\) are happy?\nWe fundamentally want the posterior predictive distribution, \\(p(\\tilde{y} | y)\\).\nFollowing the offline notes, and given conditional independence, we want\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\int p(\\tilde{y} | \\theta) p(\\theta | y) d\\theta\n&=\n\\int {n \\choose \\tilde{y}}\n(\\theta)^\\tilde{y} (1-\\theta)^{n-\\tilde{y}}\n\\cdot\n\\frac{1}{\\text{B(y + 1, n - y + 1)}}\\theta^{y}(1-\\theta)^{n-y}\nd\\theta\\\\\n&= {n \\choose \\tilde{y}} \\frac{1}{\\text{B(y + 1, n - y + 1)}} \\int \\theta^{\\tilde{y}+y} \\cdot\n(1-\\theta)^{(n-\\tilde{y}) + (n - y)} d\\theta\\\\\n&= {n \\choose \\tilde{y}}\n\\frac{\\text{B}(\\tilde{y} + y + 1, 2n - y - \\tilde{y} + 1)}{\\text{B(y + 1, n - y + 1)}}\n\\end{aligned}\n\\]\nwhere \\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\). We can of course simplify, since this is really a bunch of factorials, but we can also naively use the beta() function in R and push forward.\n\nplotcode\n\n\n\n\n\n\n\n\n\n\ny = 6\nn = 10\n\n# posterior predictive probability of ytilde\nprobYT = function(ytilde) {\n  choose(n, ytilde) * \n    beta(ytilde + y + 1, (2*n) - y - ytilde + 1) / \n    beta(y + 1, n - y + 1)\n}\n\n# construct data frame\ndf = data.frame(ytilde = 0:10) %>%\n  mutate(postPredict = probYT(ytilde))\n\n# plot data frame\ndf %>%\n  ggplot(aes(x = ytilde, y = postPredict)) +\n  geom_bar(stat = 'identity') +\n  labs(x = TeX(\"$\\\\tilde{y}$\"), y = TeX(\"$p(\\\\tilde{y}|y)$\"),\n       title = \"Posterior predictive probability\") +\n  theme_bw()"
  },
  {
    "objectID": "notes/lec04-prediction.html#monte-carlo-motivation",
    "href": "notes/lec04-prediction.html#monte-carlo-motivation",
    "title": "Prediction and Intro to Monte Carlo",
    "section": "Monte Carlo motivation",
    "text": "Monte Carlo motivation\nGeneral social survey from the 90s gathered data on the number of children to women of two categories: those with and without a bachelor‚Äôs degree.\nSetup:\n\n\\(Y_{i1}\\): number of children of \\(i\\)th woman in group 1 (no bachelor‚Äôs)\n\\(Y_{i2}\\): number of children of \\(i\\)th woman in group 2 (bachelor‚Äôs)\n\nModel:\n\n\\(Y_{11}, \\ldots, Y_{n_1 1} | \\theta_1 \\overset{\\mathrm{iid}}{\\sim} \\text{Poisson}(\\theta_1)\\)\n\\(Y_{12} \\ldots, Y_{n_2 2} | \\theta_2 \\overset{\\mathrm{iid}}{\\sim} \\text{Poisson}(\\theta_2)\\)\n\nPrior:\n\n\\(\\theta_1 \\sim \\text{gamma}(2, 1)\\)\n\\(\\theta_2 \\sim \\text{gamma}(2, 1)\\)\n\nData:\n\n\\(n_1 = 111\\), \\(\\bar{y_1} = 1.95\\), \\(\\sum y_{i 1} = 217\\)\n\\(n_2 = 44\\), \\(\\bar{y_1} = 1.5\\), \\(\\sum y_{i 1} = 66\\)\n\nPosterior:\n\n\\(\\theta_1 | \\vec{y_1} \\sim \\text{gamma}(219, 112)\\)\n\\(\\theta_2 | \\vec{y_2} \\sim \\text{gamma}(68, 45)\\)\n\nWe already know how to compute\n\nposterior mean: \\(E~\\theta | y = \\alpha / \\beta\\) (shape, rate parameterization)\nposterior density (dgamma)\nposterior quantiles and confidence intervals (qgamma)\n\nWhat about‚Ä¶\n\n\\(p(\\theta \\in \\mathcal{A} | y)\\),\n\\(E~g(\\theta) | y\\),\n\\(Var~g(\\theta) | y\\)?\n\nWhat about posterior distribution of \\(|\\theta_1 - \\theta_2\\), \\(\\theta_1 / \\theta_2\\), \\(\\text{max} \\{\\theta_1, \\theta_2 \\}\\)?"
  },
  {
    "objectID": "notes/lec04-prediction.html#monte-carlo-integration",
    "href": "notes/lec04-prediction.html#monte-carlo-integration",
    "title": "Prediction and Intro to Monte Carlo",
    "section": "Monte Carlo integration",
    "text": "Monte Carlo integration\n\napproximates an integral by a stochastic average\nshines when other methods of integration are impossible (e.g.¬†high dimensional integration)\nworks because of law of large numbers: for a random variable \\(X\\), the sample mean \\(\\bar{x}_N\\) converges to the true mean \\(\\mu\\) as the number of samples \\(N\\) tends to infinity.\n\nThe key idea is: we obtain independent samples from the posterior,\n\\[\n\\theta^{(1)}, \\ldots \\theta^{(N)} \\overset{\\mathrm{iid}}{\\sim} p(\\theta |\\vec{y})\n\\]\nthen the empirical distribution of the samples approximates the posterior (approximation improves as \\(N\\) increases).\nRecall\n\\[\nE~g(\\theta)|y = \\int_\\mathcal{\\theta} g(\\theta) f_\\theta(\\theta | y)dx \\approx \\frac{1}{N} \\sum_{i = 1}^N g(\\theta^{(i)})\n\\]\nwhere \\(f_x(x)\\) is the probability density function for a random variable \\(X\\).\nThe law of large numbers says that if our samples \\(\\theta^{(i)}\\) are independent, \\(\\frac{1}{N} \\sum_{i = 1}^N g(\\theta^{(i)})\\) to \\(E~\\theta|y\\).\n\n\n\n\n\n\nNote\n\n\n\nIntegrals are expectations, and expectations are integrals."
  },
  {
    "objectID": "notes/lec04-prediction.html#examples",
    "href": "notes/lec04-prediction.html#examples",
    "title": "Prediction and Intro to Monte Carlo",
    "section": "Examples",
    "text": "Examples\n\n\\(\\theta_1 | \\vec{y_1} \\sim \\text{gamma}(219, 112)\\)\n\\(\\theta_2 | \\vec{y_2} \\sim \\text{gamma}(68, 45)\\)\n\n\n(1) proof of concept: the mean\n\nset.seed(123)\nN = 5000\nrgamma(N, shape = 219, rate = 112) %>%\n  mean()\n\n[1] 1.95294\n\n\nPretty close to the true mean, 1.9553571.\n\n\n(2) posterior of \\(\\theta_1 - \\theta_2\\)\n\nset.seed(123)\ntheta1 = rgamma(N, shape = 219, rate = 112)\ntheta2 = rgamma(N, shape = 68, rate = 45)\n\ndf = data.frame(diff = theta1 - theta2)\n\ndf %>%\n  ggplot(aes(x = diff)) + \n  geom_density() +\n  theme_bw() +\n  labs(x = TeX(\"$\\\\theta_1 - \\\\theta_2$\"),\n       y = TeX(\"$p(\\\\theta_1 - \\\\theta_2 | {y}_1, {y}_2)$\"))\n\n\n\n\n\n\n(3) \\(p(\\theta_1 - \\theta_2> .5)\\)\n\nmean(df$diff > .5)\n\n[1] 0.4106\n\n\n\nExerciseFull solutionQuick Monte Carlo\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\\(\\theta \\sim \\text{uniform}(0, 1)\\)\nLet \\(\\gamma = \\log \\theta\\)\nVisualize \\(p(\\gamma)\\) using Monte Carlo simulation, then show using the change of variables formula and plotting the closed form of the density.\n\n\n\n\n\n\n# sample from p(theta)\ntheta = runif(10000, 0, 2)\n\n# define transform function\nf = function(x) {\n  return(0.5 *exp(x))\n}\n\n# create a df for each plot\ndf = data.frame(gamma = -7:0)\ndf2 = data.frame(gammaSamples = log(theta))\n\n# make plots\ndf %>%\n  ggplot(aes(x = gamma)) +\n  stat_function(fun = f, col = 'red', alpha = 0.5) +\n  geom_histogram(data = df2, aes(x = gammaSamples,\n                                 y = ..density..),\n               fill = 'steelblue', alpha = 0.5)\n\n\n\n\n\n\n\n# Just making the Monte Carlo part of the plot \n# in 3 lines\ntheta = runif(10000, 0, 2)\nphi = log(theta)\nhist(phi)"
  },
  {
    "objectID": "notes/lec05-MonteCarlo.html",
    "href": "notes/lec05-MonteCarlo.html",
    "title": "Monte Carlo Integration",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\nlibrary(latex2exp)"
  },
  {
    "objectID": "notes/lec05-MonteCarlo.html#monte-carlo-error",
    "href": "notes/lec05-MonteCarlo.html#monte-carlo-error",
    "title": "Monte Carlo Integration",
    "section": "Monte Carlo error",
    "text": "Monte Carlo error\n\nHow many values should we simulate?\nRecall: expected values are integrals, and integrals are expected values. Since central limit theorem (CLT) deals with expected values‚Ä¶\nRecall: CLT states that if \\(\\theta_i |\\vec{y}\\) iid with mean \\(\\theta\\) and finite variance \\(\\sigma^2\\), for \\(i \\in \\{1, \\ldots, N\\}\\), then the sample mean\n\\[\n\\bar{\\theta} \\sim N(\\theta, \\frac{\\sigma^2}{N} ).\n\\]\n\nHow to remember this/show this? Offline notes.\n\nSo to estimate \\(\\theta\\), we can generate \\(\\bar{\\theta}\\) by Monte Carlo simulation and report a confidence interval using quantiles of the normal given above in conjunction with the Monte Carlo standard error \\(\\frac{\\hat{\\sigma}}{\\sqrt{N}}\\)\nThis means we get convergence at the rate \\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)\\) regardless of the dimension of the integral!\nRecall:\n\nsd1 = pnorm(1) - pnorm(-1)\nsd2 = pnorm(2) - pnorm(-2)\nsd3 = pnorm(3) - pnorm(-3)\n\n\na 0.6826895% confidence interval can be obtained using \\(\\pm 1\\cdot \\hat{\\sigma}/\\sqrt{N}\\)\na 0.9544997% confidence interval can be obtained using \\(\\pm 2\\cdot \\hat{\\sigma}/\\sqrt{N}\\)\na 0.9973002% confidence interval can be obtained using \\(\\pm 3\\cdot \\hat{\\sigma}/\\sqrt{N}\\)\n\n\n\nExample\n\n# Let theta be \"x\" in the code below\nset.seed(123)\n\n# binomial(n, p)\nn = 20\np = 0.4\n\n# mean, variance, sd of a binomial(n, p)\nEX = n*p # 20*.4 = 8\nVarX = n*p*(1-p) # 20*.4*.6 = 4.8\nsdX = sqrt(VarX) # 2.19089\n\n# Monte Carlo sample of size N\nN = 100\nxSamples = rbinom(N, size = n, prob = p) \n\n# sample mean, var, sd\nxbar = mean(xSamples)\nxvar = var(xSamples)\nxsigma = sd(xSamples) # = sqrt(sum((xSamples - xbar)^2) / (N -1))\n\nse = xsigma / sqrt(N)\n\nlb = round(xbar - (2*se), 3)\nub = round(xbar + (2*se), 3)\n\nFor N = 100 Monte Carlo samples, The posterior mean of \\(\\theta\\) is \\(\\bar{\\theta} =\\) 8.01 with 95% confidence interval (7.57 8.45).\n\n\n\n\n\n\nExercise\n\n\n\nAbove we estimate \\(Var(\\theta)\\) to be 4.838 and the standard error for \\(N = 100\\) was 0.22.\nIf you wanted to state \\(p(\\theta \\in (\\hat{\\theta} \\pm 0.01)) = 0.95\\), how large would \\(N\\) have to be?\nCheck your answer by adjusting \\(N\\) above."
  },
  {
    "objectID": "notes/lec05-MonteCarlo.html#monte-carlo-prediction",
    "href": "notes/lec05-MonteCarlo.html#monte-carlo-prediction",
    "title": "Monte Carlo Integration",
    "section": "Monte Carlo prediction",
    "text": "Monte Carlo prediction\n\nPrior predictive distribution\nWe can use Monte Carlo to sample new observation, \\(\\tilde{y}\\), from the prior predictive distribution\n\\[\np(\\tilde{y}) = \\int p(\\tilde{y}|\\theta)p(\\theta) d\\theta,\n\\]\nwhere we proceed by following the iterative procedure below\n1. sample theta_i from the prior p(theta)\n2. sample ytilde from p(ytilde | theta_i)\n3. repeat steps 1 and 2\n\nthis can be useful to see if a prior for \\(p(\\theta)\\) actually translate to reasonable prior beliefs about the data.\n\n\n\n\n\n\n\nExercise\n\n\n\nFor \\(p(\\theta) = \\text{gamma}(8,2)\\), plot \\(p(\\tilde{y})\\) assuming \\(\\tilde{y} | \\theta \\sim \\text{Poisson}(\\theta)\\).\n\n\n\n\n\n\n\nPosterior predictive distribution\nWe can also sample \\(\\tilde{y}\\) from the posterior predictive distribution,\n\\[\np(\\tilde{y} | y_1, \\ldots y_n) = \\int p(\\tilde{y}|\\theta) p(\\theta|y_1, \\ldots, y_n)d\\theta,\n\\]\nwhere the procedure is the same as before, except step 1 is replace with sampling \\(\\theta\\) from the posterior \\(p(\\theta | y_1,\\ldots, y_n)\\).\nThe resulting sequence \\((\\theta^{(1)}, \\tilde{y}^{(1)}), \\ldots, (\\theta^{(N)}, \\tilde{y}^{(N)})\\) constitutes \\(N\\) independent samples from the joint posterior of \\((\\theta, \\tilde{Y})\\). The sequence \\(\\tilde{y}^{(1)}, \\ldots, \\tilde{y}^{(N)})\\) constitutes \\(N\\) independent samples from the marginal posterior distribution of \\(\\tilde{Y}\\), aka the posterior predictive distribution."
  },
  {
    "objectID": "notes/lec05-MonteCarlo.html#posterior-predictive-model-checking",
    "href": "notes/lec05-MonteCarlo.html#posterior-predictive-model-checking",
    "title": "Monte Carlo Integration",
    "section": "Posterior predictive model checking",
    "text": "Posterior predictive model checking\nWe can assess the fit of a model by comparing the posterior predictive distribution to the empirical distribution.\n\nExample: is our Poisson model flawed?\n\n# load general social survey data\ngss = read_csv(\"https://sta360-fa23.github.io/data/gss.csv\")\n\n\ny1 = gss$CHILDS[gss$FEMALE == 1 &  gss$YEAR >= 1990  & gss$AGE == 40 & \n                   gss$DEGREE < 3 ]\ny1 = y1[!is.na(y1)]\nn = length(y1)\n\nWe are examining the number of children \\(Y_i\\) belonging to \\(n=\\) 111 40 year old women surveyed 1990 or later without a bachelor‚Äôs. These data come from the general social survey.\nSuppose\n\\[\n\\begin{aligned}\nY_i & \\sim \\text{Poisson}(\\theta)\\\\\n\\theta & \\sim \\text{gamma}(2, 1).\n\\end{aligned}\n\\]\nThe empirical and predictive distributions of the data are both plotted below.\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\n# posterior predictive distribution\nytotal = sum(y1)\na = 2 ; b = 1\nN = 10000\ntheta.post.mc = rgamma(N, ytotal + a, b + n)\ny1.mc = rpois(N, theta.post.mc)\n\n# data\ndf = data.frame(y1) # empirical\ndf2 = data.frame(y1.mc) # post predictive\n  \n# make plot\ndf %>%\n  ggplot(aes(x = y1)) +\n  geom_bar(aes(x = y1 + .15, y = (..count..)/sum(..count..),\n               fill = \"empirical\"), alpha = 0.6, width = 0.3) +\n  geom_bar(data = df2, \n                 aes(x = y1.mc -.15, y = (..count..) / sum(..count..),\n                     fill = \"predictive\"), alpha = 0.4, width = 0.3) +\n  labs(x = \"number of children\", \n       y = TeX(\"$p(Y_i = y_i)$\"),\n       fill = \"\") +\n  scale_x_continuous(breaks = c(0:7), labels = c(0:7),\n                     limits = c(-.5,7.5)) +\n  \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLet \\(\\mathbf{y}\\) be a vector of length 111. Let \\(t(\\mathbf{y})\\) be the ratio of \\(2\\)s to \\(1\\)s in \\(\\mathbf{y}\\). For our observed data, this test statistic \\(t(\\mathbf{y}_{obs}) = 38 / 19 = 2\\). What is the tail probability \\(p(t(\\tilde{\\mathbf{Y}}) \\geq t(\\mathbf{y}_{obs}))\\) under the posterior predictive distribution?"
  },
  {
    "objectID": "notes/lec06-normalModel.html",
    "href": "notes/lec06-normalModel.html",
    "title": "The normal model",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\nlibrary(latex2exp)"
  },
  {
    "objectID": "notes/lec06-normalModel.html#background",
    "href": "notes/lec06-normalModel.html#background",
    "title": "The normal model",
    "section": "Background",
    "text": "Background\n\nDefinition and vocabulary\nLet \\(Y\\) be normally distributed with mean \\(\\theta\\) and variance \\(\\sigma^2\\). Mathematically,\n\\[\nY | \\theta, \\sigma^2  \\sim N(\\theta, \\sigma^2).\n\\]\nThe density\n\\[\n\\begin{aligned}\np(y ~|~ \\theta, \\sigma^2 ) &= (2\\pi\\sigma^2)^{-1/2} e^{-\\frac{1}{2\\sigma^2} (y-\\theta)^2},\\\\\ny &\\in \\mathbb{R},\\\\\n\\theta &\\in \\mathbb{R},\\\\\n\\sigma &\\in \\mathbb{R}^+.\n\\end{aligned}\n\\]\n\nlocation, scale\n\n\\(\\theta\\) is called the ‚Äòlocation‚Äô parameter\n\\(\\sigma\\) is called the ‚Äòscale‚Äô parameter\n\n\n\nprecision\nNotice that every time \\(\\sigma^2\\) appears in the density, it is inverted. For this reason, the inverse variance \\((\\frac{1}{\\sigma^2})\\) has a special name, precision. Intuitively, precision tells us how close \\(y\\) is to the mean \\(\\theta\\). (Large precision = small variance = closer).\n\n\n\nplots of normal densities\n\n\n\n\n\n\nWarning\n\n\n\nIn R, the arguments of pnorm, dnorm, rnorm are the mean and standard deviation (not the variance!)\n\n\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\nN = 10000\ny.mc = rnorm(N, mean = 3, sd = 2)\n\ndf = data.frame(y.mc)\n\ndf %>%\n  ggplot(aes(x = y.mc)) +\n  geom_histogram(aes(y = ..density..), alpha = 0.6, fill = 'steelblue') +\n  stat_function(fun = dnorm, args = list(mean = 3, sd = 2), aes(color = \"N(3, 4)\")) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = \"N(0, 1)\")) +\n  theme_bw() +\n  labs(x = \"y\", y = \"density\", title = \"Normal densities\",\n       color = \"\")"
  },
  {
    "objectID": "notes/lec06-normalModel.html#bayesian-inference",
    "href": "notes/lec06-normalModel.html#bayesian-inference",
    "title": "The normal model",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nIn general, we wish to make inference about \\(\\theta\\) and \\(\\sigma^2\\) after observing some data \\(y_1, \\ldots y_n\\) and thus are interested in the posterior \\(p(\\theta, \\sigma^2 | y_1, \\ldots y_n)\\). This is the standard task we have seen thus far, and requires us to specify a joint prior \\(p(\\theta, \\sigma^2)\\). Below, we will work to find a class of conjugate priors over \\(\\theta\\) and \\(\\sigma^2\\).\nWe can break up the joint posterior into two pieces from the axioms of probability:\n\\[\np(\\theta, \\sigma^2 | y_1, \\ldots y_n) = p(\\theta | \\sigma^2, y_1, \\ldots, y_n)p(\\sigma^2|y_1, \\ldots y_n)\n\\]\nThis suggests that we calculate the joint posterior by:\n\nfirst finding the full conditional of \\(\\theta\\): \\(p(\\theta| \\sigma^2, \\vec{y})\\)\nand then finding the marginal posterior of \\(\\sigma^2\\): \\(p(\\sigma^2 | \\vec{y})\\),\n\nwhere \\(\\vec{y} = \\{y_1, \\ldots y_n\\}\\).\n\nThe full conditional of \\(\\theta\\)\nBy Bayes‚Äô theorem,\n\\[\np(\\theta| \\sigma^2, \\vec{y}) \\propto \\underbrace{p(\\vec{y} |\\theta, \\sigma^2)}_{\\text{likelihood}} \\underbrace{p(\\theta|\\sigma^2)}_{\\text{prior}}.\n\\]\nTo arrive at the full conditional posterior of \\(\\theta\\), we must first specify a prior on \\(\\theta\\).\nConsidering we have a normal likelihood, what is a conjugate class of densities for \\(\\theta\\)?\n\n\n\n\n\n\nanswer\n\n\n\n\n\n\\(\\theta | \\sigma^2 \\sim N(\\mu_0, \\tau_0^2)\\) for some \\(\\mu_0 \\in \\mathbb{R}\\) and \\(\\tau_0^2 \\in \\mathbb{R}^+\\) is conjugate.\n\n\n\nWith the conjugate prior, our full conditional posterior \\(\\{ \\theta| \\sigma^2, \\vec{y} \\} \\sim N(\\mu_n, \\tau_n^2)\\) where\n\\[\n\\begin{aligned}\n\\mu_n &=\n\\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma^2} \\bar{y}}{\n\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\n}\n\\\\\n\\\\\n\\tau_n^2 &= \\frac{1}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}}\n\\end{aligned}\n\\]\n\nLet‚Äôs sketch out ‚Äòcompleting the square‚Äô to derive the parameters offline.\n\n\n\nIntuitive posterior parameters\nIf we consider the posterior precision, \\(\\frac{1}{\\tau_n^2}\\), we can re-arrange the terms above to illuminate how posterior information = prior information + data information;.\n\\[\n\\frac{1}{\\tau_n^2}= \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\n\\]\nIn words, posterior precision is equivalent to prior precision plus sampling precision. If we name each precision term, \\(\\lambda_0 = \\frac{1}{\\tau_0}\\), \\(\\lambda_n = \\frac{1}{\\tau_n}\\) and \\(\\lambda = \\frac{1}{\\sigma}\\) then\n\\[\n\\mu_n = \\frac{\\lambda_0^2}{\\lambda_0^2 + n\\lambda^2} \\mu_0 +\n\\frac{n\\lambda^2}{\\lambda_0^2 + n\\lambda^2} \\bar{y}\n\\]\ni.e.¬†the posterior mean is the weighted average of prior and sample mean, where the weights are the relative contribution of each precision!\nWe can re-define \\(\\lambda_0^2 = \\kappa_0 \\lambda^2\\) (or equivalently \\(\\tau_0^2 = \\frac{\\sigma^2}{\\kappa_0}\\)) and obtain\n\\[\n\\begin{aligned}\n\\mu_n &= \\frac{\\kappa_0}{\\kappa_0 + n} \\mu_0 + \\frac{n}{\\kappa_0 + n} \\bar{y},\\\\\n\\frac{1}{\\tau_n^2} &= \\frac{\\kappa_0 + n}{\\sigma^2}\n\\end{aligned}\n\\]\nwhere we can interpret \\(\\kappa_0\\) as the prior sample size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that \\(\\tau_n^2\\) is the posterior variance of the full conditional posterior of \\(\\theta\\). This is distinct from \\(\\sigma_n^2\\), defined below.\n\n\n\n\nPrior on \\(\\sigma^2\\)\nRemember, we want \\(p(\\theta, \\sigma^2 | \\vec{y}) = p(\\theta | \\sigma^2, y_1, \\ldots, y_n)p(\\sigma^2|y_1, \\ldots y_n)\\). We have the first component of the right hand side, what about the second component?\nNotice that\n\\[\np(\\sigma^2 | \\vec{y}) \\propto p(\\sigma^2)\\int p(\\vec{y} | \\theta, \\sigma^2) p(\\theta|\\sigma^2) d\\theta\n\\]\nBut how do we choose \\(p(\\sigma^2)\\) to be conjugate? We can proceed in multiple ways: one is noting that the integral is really a convolution of normals, (thereby a sum of normals) and is therefore a normal density.\nUpon inspection, we can see a suitable choice is \\(\\frac{1}{\\sigma^2} \\sim \\text{gamma}(a, b)\\).\n\n\nThe inverse-gamma\nA random variable \\(X \\in (0, \\infty)\\) has an inverse-gamma(a,b) distribution if \\(\\frac{1}{X}\\) has a gamma(a,b) distribution.\nIf X has an inverse-gamma distribution, the density of X is\n\\[\np(x | a, b) = \\frac{b^a}{\\Gamma(a)} x^{-a-1}e^{-b/x} \\ \\text{for } \\ x > 0\n\\]\nand\n\\[\n\\begin{aligned}\nEX &= \\frac{b}{(a-1)} \\text{ if } a \\geq 1; \\ \\infty \\text{ if } 0<a<1,\\\\\nVar(X) &= \\frac{b^2}{(a-1)^2(a-2)} \\ \\text{if } a \\geq 2; \\ \\infty \\text{ if } 0 < a < 2,\\\\\nMode(X) &= \\frac{b}{a +1}.\n\\end{aligned}\n\\]\n\n\nThe marginal posterior of \\(\\sigma^2\\)\nTaken all together, if we let our sampling model and prior distributions be such that\n\\[\n\\begin{aligned}\nY_i | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\n\\theta | \\sigma^2 & \\sim N(\\mu_0, \\sigma^2/\\kappa_0)\\\\\n\\frac{1}{\\sigma^2} &\\sim \\text{gamma}(\\frac{\\nu_0}{2}, \\frac{\\nu_0}{2} \\sigma_0^2)\n\\end{aligned}\n\\]\nthen the posterior\n\\[\n\\frac{1}{\\sigma^2} | \\vec{y} \\sim \\text{gamma}(\\frac{\\nu_n}{2}, \\frac{\\nu_n \\sigma^2_n}{2}),\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\nu_n &= \\nu_0 + n,\\\\\n\\sigma^2_n &= \\frac{1}{\\nu_n} \\left[\n\\nu_0 \\sigma^2_0 +(n-1)s^2 + \\frac{\\kappa_0 n}{\\kappa_0 + n}(\\bar{y} - \\mu_0)^2\n\\right],\n\\end{aligned}\n\\]\nand \\(s^2\\) is the sample variance, \\(\\frac{1}{n-1} \\sum_i (y_i - \\bar{y})^2\\)."
  },
  {
    "objectID": "notes/lec06-normalModel.html#sampling-from-the-joint-posterior",
    "href": "notes/lec06-normalModel.html#sampling-from-the-joint-posterior",
    "title": "The normal model",
    "section": "Sampling from the joint posterior",
    "text": "Sampling from the joint posterior\nSince \\(p(\\theta, \\sigma^2 | \\vec{y}) = p(\\theta | \\sigma^2, y_1, \\ldots, y_n)p(\\sigma^2|y_1, \\ldots y_n)\\), we can sample from the joint posterior by first sampling from \\(p(\\sigma^2|y_1, \\ldots y_n)\\) and then sampling from \\(p(\\theta | \\sigma^2, y_1, \\ldots, y_n)\\).\n\nExample\nProof of concept\nWe have some data:\n\n# generating 10 samples from the population\nset.seed(123)\ntrue.theta = 4\ntrue.sigma = 1\ny = rnorm(10, true.theta, true.sigma)\n\nybar = mean(y) # sample mean\nn = length(y) # sample size\ns2 = var(y) # sample variance\n\nWe make inference about \\(\\theta\\) and \\(\\sigma^2\\):\n\n# priors\n# theta prior\nmu_0 = 2; k_0 = 1\n# sigma2 prior\nnu_0 = 1; s2_0 = 0.010\n\n# posterior parameters\nkn = k_0 + n\nnun = nu_0 + n\nmun = (k_0 * mu_0 + n * ybar) /kn\ns2n = (nu_0 * s2_0 + (n - 1) * s2 + k_0 * n * (ybar - mu_0)^2 / (kn)) / (nun)\n\ns2.postsample = 1 / rgamma(10000, nun / 2, s2n * nun / 2)\ntheta.postsample = rnorm(10000, mun, sqrt(s2.postsample / kn))\n\ndf = data.frame(theta.postsample, s2.postsample)\n\ndf %>%\n  ggplot(aes(x = theta.postsample, y = s2.postsample)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  labs(x = TeX(\"$\\\\theta$\"),\n       y = TeX(\"$\\\\sigma^2$\"),\n       fill = TeX(\"$p(\\\\theta, \\\\sigma^2 | y_1, \\\\ldots y_n)$\")) +\n  theme_bw()"
  },
  {
    "objectID": "notes/lec07-estimators.html",
    "href": "notes/lec07-estimators.html",
    "title": "Estimators",
    "section": "",
    "text": "Definition\n\n\n\nA point estimator of an unknown parameter \\(\\theta\\) is a function that converts data into a single element of parameter space \\(\\Theta\\).\n\n\nExample: imagine \\(\\theta\\) is the population mean. The following are each point estimators of \\(\\theta\\):\n\n\\(\\bar{y}\\)\n\\(y_1\\)\n\\(\\frac{y_1 + y_2}{2}\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is convention to write the population parameter as a Greek character and the estimator as the same Greek character, but with a ‚Äúhat‚Äù. For example, \\(\\theta\\) is the parameter and \\(\\hat{\\theta}\\) is the estimator.\n\n\nSampling properties of a point estimator refer to the estimator‚Äôs behavior under hypothetical repeatable surveys or experiments.\nThree common sampling properties of estimators we will see again and again are:\n\nbias\nvariance\nmean squared error (MSE)\n\n\n\nBefore we discuss bias, variance and mean squared error of an estimator, it‚Äôs important to understand that an estimator is a statistic (function of the data) and therefore a random variable. Because of this, estimator‚Äôs have a sampling distribution.\nExercise: What does the example below show? What is x?\n\nset.seed(360)\n\nx = vector()\nfor (i in 1:100) {\n  y = rnorm(10)\n  x = append(min(y), x)\n}\nhist(x, freq = FALSE)\nabline(v = mean(x), col= \"steelblue\", lwd = 4)\n\n\n\ncat(\"The variance of x is \", round(var(x), 3))\n\nThe variance of x is  0.291\n\n\n\n\n\nIn the rest of these notes, let \\(\\theta_0\\) be the true value of the population parameter \\(\\theta\\).\n\n\n\n\n\n\nDefinition\n\n\n\nBias is the the difference between the expected value of the estimator and the true value of the parameter.\n\n\\(E[\\hat{\\theta} | \\theta = \\theta_ 0] - \\theta_0\\) is the bias of \\(\\hat{\\theta}\\).\nIf \\(E[\\hat{\\theta} | \\theta = \\theta_0] = \\theta_0\\), then we say \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\).\nIf \\(E[\\hat{\\theta} | \\theta = \\theta_0] \\neq \\theta_0\\), then we say \\(\\hat{\\theta}\\) is a biased estimator of \\(\\theta\\).\n\n\n\nExercise: Imagine \\(\\hat{\\theta}_a\\) and \\(\\hat{\\theta}_b\\) are two different estimators of \\(\\theta\\). The true value of \\(\\theta\\) is \\(\\theta_0 = 0\\). The sampling distributions of the two estimators are given below. Which estimator do you prefer?\n\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nRecall: variance is average squared distance from the mean. In this context, the variance of an estimator refers to the variance of the sampling distribution of \\(\\hat{\\theta}\\). We write this mathematically,\n\\[\nVar[\\hat{\\theta} | \\theta_0] = E[(\\hat{\\theta} - m)^2 |\\theta_0]\n\\]\nwhere \\(m = E[\\hat{\\theta}|\\theta_0]\\).\n\n\nWhile it may seem desirable to have an estimator with zero bias, the estimator may still be far away from the true parameter value if the variance is too large. The mean squared error quantifies how close an estimator is to the true parameter value.\n\n\n\n\n\n\nDefinition\n\n\n\nMean squared error (MSE) is (as the name suggests) the expected value of the squared difference between the estimator and true parameter value. Equivalently, MSE is the variance plus the square bias of the estimator.\n\\[\n\\begin{aligned}\nMSE[\\hat{\\theta}|\\theta_0] &= E[(\\hat{\\theta} - \\theta_0)^2 | \\theta_0]\\\\\n&= Var[\\hat{\\theta} | \\theta_0] + Bias^2[\\hat{\\theta}|\\theta_0]\n\\end{aligned}\n\\]\n\n\n\nLet‚Äôs show this offline."
  },
  {
    "objectID": "notes/lec07-estimators.html#practice",
    "href": "notes/lec07-estimators.html#practice",
    "title": "Estimators",
    "section": "Practice",
    "text": "Practice\nSuppose you wish to make inference about the average bill length of Chinstrap penguins.\nYou make the modeling assumption that \\(Y\\), the bill length of a penguin is normally distributed, i.e.¬†\\(Y \\sim N(\\theta, \\sigma^2)\\) and you set up a conjugate prior as we‚Äôve done before.\nOne can then show that the posterior mean estimator of \\(\\theta\\) is\n\\[\n\\hat{\\theta}_b = E[\\theta | y_1,\\ldots y_n] = \\frac{n}{\\kappa_0 + n} \\bar{y} + \\frac{\\kappa_0}{\\kappa_0 + n} \\mu_0 = w\\bar{y} + (1-w) \\mu_0\n\\]\nExercise: compare \\(\\hat{\\theta}_b\\) to the estimator \\(\\hat{\\theta}_e = \\bar{y}\\). Compute the expected value of each estimator, which one is biased? Compute the variance of each estimator. Which has lower variance?\n\nLet‚Äôs compute the MSE and discuss when the Bayesian estimator \\(\\hat{\\theta}_b\\) has lower MSE than the sample mean offline."
  },
  {
    "objectID": "notes/lec07-estimators.html#extra-practice",
    "href": "notes/lec07-estimators.html#extra-practice",
    "title": "Estimators",
    "section": "Extra practice",
    "text": "Extra practice\n\n\n\n\n\nSuppose you know that you know Gentoo penguins are closely related to Chinstrap penguins. Previously, you‚Äôve measured the bill length of three Gentoo penguins and found their mean bill length to be 46.2. Accordingly, you set \\(\\mu_0 = 46.2\\).\n\n\n\n\n\n\n\n\nSuppose (for illustrative purposes) that you know the true population mean and variance for Chinstrap penguin bill length,\n\\[\n\\begin{aligned}\n\\theta_0 &= 48.8\\\\\n\\sigma^2 &= 3.3.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nCompute \\(MSE[\\hat{\\theta}_e|\\theta_0]\\) and \\(MSE[\\hat{\\theta_b}|\\theta_0]\\) and plot the ratio \\(MSE[\\hat{\\theta}_b]/MSE[\\hat{\\theta}_e|\\theta_0]\\) as a function of \\(n\\) for \\(\\kappa_0 = 0, 1, 2, 3\\)."
  },
  {
    "objectID": "notes/lec08-gibbs.html",
    "href": "notes/lec08-gibbs.html",
    "title": "Gibbs sampling",
    "section": "",
    "text": "Definition\n\n\n\nA semiconjugate or conditionally conjugate prior is a prior that is conjugate to the full conditional posterior.\n\n\nNote: the idea of a semiconjugate prior only makes sense when making inferences about two or more parameters.\nExample:\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\n\\theta & \\sim N(\\mu_0, \\tau_0^2)\\\\\n\\frac{1}{\\sigma^2} &\\sim gamma(\\nu_0/2, \\nu_0\\sigma_0^2/2)\n\\end{aligned}\n\\]\nIn this case, \\(\\tau_0^2\\) is not a function of \\(\\sigma^2\\) and thus \\(p(\\theta, \\sigma^2) = p(\\theta) p(\\sigma^2)\\).\nEach prior is ‚Äúsemiconjugate‚Äù since \\(p(\\theta| \\sigma^2, y_1,\\ldots y_n)\\) is normal and \\(p(\\frac{1}{\\sigma^2} | \\theta, y_1,\\ldots y_n)\\) is gamma but \\(p(\\theta, \\sigma^2)\\) is not conjugate to \\(p(\\theta, \\sigma^2 | y_1,\\ldots y_n)\\).\n\n\n\n\n\n\nDefinition\n\n\n\nA proper prior is a density function that does not depend on data and integrates to 1. If a prior integrates to a positive finite value, it is an unnormalized density that can be renormalized by being multiplied by a constant to integrate to 1. If a prior is not proper, we call the prior improper.\n\n\nExample:\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\np(\\theta, \\sigma^2) &= \\frac{1}{\\sigma^2}\n\\end{aligned}\n\\]\n\\(p(\\theta, \\sigma^2)\\) is an improper prior. \\(p(\\theta, \\sigma^2)\\) does not integrate to a finite value and thus cannot be renormalized. It is not a probability density. However, it yields a tractable posterior for \\(\\theta\\) and \\(\\sigma^2\\) (see p 79 of Hoff).\n\n\n\nPriors are meant to describe our state of knowledge before examining data. In some cases we may wish to describe our ignorance a priori using a vague prior that plays a minimal role in the posterior distribution.\nA common trap is to imagine that a flat, or uniform prior is uninformative. Previously, on homework 3 you showed a uniform prior on binary probability of success \\(\\theta\\) is informative on the log-odds. Additionally, an improper flat prior may carry a lot of information, since most of the mass is infinitely far away.\n\n\n\n\n\n\nDefinition\n\n\n\nThe Jeffreys prior\n\\[\nJ(\\theta) \\propto \\sqrt{I(\\theta)}\n\\]\nwhere \\(I(\\theta) = -E[\\frac{\\partial}{\\partial\\theta^2} \\log p(Y|\\theta) | \\theta]\\).\n\n\nThe defining feature of Jeffreys prior is that it will yield an equivalent result if applied to a transformed parameter. This principle of invariance is one approach to non-informative priors that works well for single parameter priors. Multiparameter extensions are often less useful."
  },
  {
    "objectID": "notes/lec08-gibbs.html#gibbs-sampler",
    "href": "notes/lec08-gibbs.html#gibbs-sampler",
    "title": "Gibbs sampling",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nWhat if we have a non-conjugate prior? How can we can we look at \\(p(\\theta, \\sigma^2 | y_1,\\ldots y_n)\\)?\nIn general, suppose we don‚Äôt know\n\\[\np(\\theta, \\sigma^2 | y_1,\\ldots y_n)\n\\]\nbut we do know the full conditional posteriors\n\\[\n\\begin{aligned}\np(\\theta | \\sigma^2, y_1, \\ldots y_n)\\\\\np(\\sigma^2 | \\theta, y_1,\\ldots y_n)\n\\end{aligned}\n\\]\nwe can generate sample \\(\\theta^{(s)}, \\sigma^{2(s)}\\) from the joint posterior by the following algorithm:\n\nsample \\(\\theta^{(s+1)}\\) from \\(p(\\theta | \\sigma^{2(s)}, y_1,\\ldots y_n)\\)\nsample \\(\\sigma^{2(s+1)}\\) from \\(p(\\sigma^2|\\theta^{(s+1)}, y_1,\\ldots, y_n)\\)\nlet \\(\\phi^{(s+1)} = \\{ \\theta^{(s+1)}, \\sigma^{2(s+1)} \\}\\)\n\niterate steps 1-3 \\(S\\) times.\nThis algorithm is called the Gibbs sampler,\n\nit creates a dependent set of values \\(\\phi^{(1)} \\ldots \\phi^{(S)}\\),\nthe sequence is called a Markov chain,\nthe samples let us approximate the posterior i.e.¬†the histogram of \\((\\phi^{(1)},\\ldots \\phi^{(S)})\\) is a Markov chain Monte Carlo approximation to \\(p(\\phi | y_1,\\ldots y_n)\\).\n\nExample: in the semiconjugate normal model described above, the resulting posteriors are:\n\\[\n\\theta | \\sigma^2, y_1,\\ldots y_n \\sim N(\\mu_n, \\tau_n^2),\n\\]\nwhere \\(\\mu_n = \\frac{\\mu_0/\\tau_0^2 + n\\bar{y} /\\sigma^2}{1/{\\tau_0^2} + n/\\sigma^2}\\) and \\(\\tau_n^2 = \\left( \\frac{1}{\\tau_0^2 }+ \\frac{n}{\\sigma^2} \\right)^{-1}\\) and\n\\[\n\\sigma^2 | \\theta, y_1, \\ldots y_n \\sim invgamma(\\nu_n/2, \\nu_n \\sigma^2_n / 2)\n\\]\nwhere \\(\\nu_n = \\nu_0 + n\\), \\(\\sigma_n^2 = \\frac{1}{\\nu_n} [\\nu_0 \\sigma_0^2 + n s^2_n(\\theta)]\\) and \\(s^2_n(\\theta) = \\frac{1}{n}\\sum (y_i - \\theta)^2\\).\n\n##########################\n# example from Hoff ch6 #\n##########################\n\n# data\ny = c(1.64, 1.70, 1.72, 1.74, 1.82, 1.82, 1.82, 1.90, 2.08)\nmean.y = mean(y) ; var.y = var(y) ; n = length(y)\n\n# priors\nmu0 = 0\nt20 = 100\nnu0 = 1\ns20 = 2\n\n# starting point\nS = 1000\nPHI = matrix(nrow = S, ncol = 2)\nphi = c(mean.y, var(y))\nPHI[1, ] = phi\n\n# Gibbs sampling\nset.seed(360)\nfor(s in 2:S) { \n\n## generate theta from sigma2\nmun = (mu0 / t20 + n * mean.y * phi[2]) / (1 / t20 + n * phi[2])\nt2n = 1 / (1 / t20 + n * phi[2])\nphi[1] = rnorm(1, mun, sqrt(t2n))\n\n## generate 1/sigma2 from theta\nnun = nu0 + n\ns2n = (nu0 * s20 + (n - 1) * var.y + n * (mean.y - phi[1])^2 ) / nun\nphi[2] = rgamma(1, nun/2, nun * s2n / 2)\n\n## update chain\nPHI[s,] = phi\n}\n\nNote: in this code we use the identity \\(n s_n^2(\\theta) = (n-1)s^2 + n (\\bar{y} - \\theta)^2\\).\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(latex2exp)\n# plotting the joint posterior\ndf = as.data.frame(PHI)\nnames(df) = c(\"theta\", \"prec\")\ndf %>%\n  ggplot(aes(x = theta, y = prec)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  labs(x = TeX(\"$\\\\theta$\"),\n       y = TeX(\"$1/\\\\sigma^2$\"),\n       fill = TeX(\"$p(\\\\theta, 1/\\\\sigma^2 | y_1, \\\\ldots y_n)$\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nSince the sequence \\(\\{\\phi^{(s)} \\}\\) depends on \\(\\phi^{(0)}, \\ldots \\phi^{(s-1)}\\) only through \\(\\phi^{(s-1)}\\) we say the sequence is memoryless. This is called the Markov property, and so the sequence is a Markov chain.\n‚ÄúWhat happens next depends only on the state of affairs now‚Äù\n\n\nUnder some conditions,\n\\[\np(\\phi^{(s)} \\in A) \\rightarrow \\int_A p(\\phi) d\\phi \\ \\ \\text{ as } s \\rightarrow \\infty\n\\]\ni.e.¬†the sampling distribution of \\(\\phi^{(s)}\\) approaches the target distribution as \\(s \\rightarrow \\infty\\) regardless of \\(\\phi^{(0)}\\).\nFurthermore,\n\\[\n\\frac{1}{S} \\sum_{s=1}^S g(\\phi^{(s)})  \\rightarrow E[g(\\phi)]\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nBig take-away: if we can sample from the full conditional posteriors, we can construct a Markov chain with samples from the joint posterior! We can then use Monte Carlo approximation to use the samples to summarize aspects of the posterior."
  },
  {
    "objectID": "notes/lec09-mcmc-diagnostics.html",
    "href": "notes/lec09-mcmc-diagnostics.html",
    "title": "MCMC diagnostics",
    "section": "",
    "text": "We setup a data generative model, \\(p(y | \\boldsymbol{\\theta})\\) and a prior on the model parameters \\(p(\\boldsymbol{\\theta})\\) where \\(\\boldsymbol{\\theta} = \\{ \\theta_1, \\theta_2, \\ldots \\theta_n\\}\\).\nNext, we wish to make inferences using the data we collect \\(\\boldsymbol{y} = \\{y_1,\\ldots y_n\\}\\). All inferences we make require the posterior \\(p(\\boldsymbol{\\theta}| \\boldsymbol{y})\\), which we obtain via Bayes‚Äô rule.\nIn general, the inferences we wish to make, e.g.¬†\\(p(g(\\boldsymbol{\\theta}) \\in A)\\), are complicated or impossible to compute analytically. Here, Monte Carlo approximation helps. The key idea is that we use independent samples from the posterior as an empirical approximation to make inference.\nFor non-conjugate models, obtaining samples from the posterior can be hard. We saw last time that Gibbs sampling lets us generate a series of dependent samples from the posterior as an empirical approximation to make inference. The key idea is that if we sample a large number of samples \\(S\\), we should have some number \\(S_{eff}<S\\) effectively independent samples.\n\n\nGibbs sampling is one of many methods (but not the only method) to construct a Markov chain comprised of dependent samples from the target distribution.\nConstructing a Markov chain of dependent samples and using these samples to approximate the target distribution is called Markov chain Monte Carlo (MCMC).\n\nImportantly, MCMC sampling algorithms are not models. They do not generate more information than is in \\(\\boldsymbol{y}\\) and \\(p(\\boldsymbol{\\theta})\\). They are simply ways of ‚Äúlooking at‚Äù \\(p(\\boldsymbol{\\theta}|\\boldsymbol{y})\\).\n\n\n\n\n\n\nDefinition\n\n\n\nA target distribution is a distribution we are interested in sampling. In Bayesian statistics, this is typically the posterior distribution."
  },
  {
    "objectID": "notes/lec09-mcmc-diagnostics.html#properties-of-mcmc",
    "href": "notes/lec09-mcmc-diagnostics.html#properties-of-mcmc",
    "title": "MCMC diagnostics",
    "section": "Properties of MCMC",
    "text": "Properties of MCMC\n\ntoy example\nImagine the following target distribution (the joint probability distribution of two variables, \\(\\theta\\) and \\(\\delta\\)).\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(latex2exp)\nset.seed(360)\n\n## fixed values ## \nmu = c(-3, 0, 3) # conditional means\nsd = rep(sqrt(1 / 3), 3) # conditional sds\nd = c(1, 2, 3) # sample space of delta\nN = 1000 # number of samples\n\ndelta = sample(d, size = N, prob = c(.45, .1, .4), replace = TRUE)\ntheta = rnorm(N, mean = mu[delta], sd = sd[delta])\n\ndf = data.frame(delta, theta)\ndf %>%\n  ggplot(aes(x = theta, y = delta)) + \n  geom_bin2d(bins = 25) +\n  theme_bw() + \n  labs(y = TeX(\"\\\\delta\"), \n       x = TeX(\"\\\\theta\"))\n\n\n\n\nIn this example,\n\\[\n\\begin{aligned}\np(\\delta = d) = \\begin{cases}\n&.45 &\\text{ if } d = 1\\\\\n&.10 &\\text{ if } d = 2\\\\\n&.45 &\\text{ if } d = 3\n\\end{cases}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\{\\theta | \\delta = d\\} \\sim\n\\begin{cases}\n&N(-3, 1/3) &\\text{ if } d = 1\\\\\n&N(0, 1/3) &\\text{ if } d = 2\\\\\n&N(3, 1/3) &\\text{ if } d = 3\n\\end{cases}\n\\end{aligned}\n\\]\nExercise: Construct a Gibbs sampler of the joint density.\nNote: this is a toy example. We can sample from the target distribution directly as seen above. However, we will construct a Gibbs sampler for pedagogical purposes that will become apparent momentarily.\n\n\n\n\n\n\nsolution\n\n\n\n\n\nTo construct a Gibbs sampler, we need the full conditional distributions.\n\n\\(p(\\theta | \\delta)\\) is given.\n\\(p(\\delta| \\theta) = \\frac{p(\\theta | \\delta = d) p(\\delta = d)}{ \\sum_{d=1}^3p(\\theta | \\delta = d)p(\\delta = d)}\\), for \\(d \\in \\{1, 2, 3\\}\\).\n\n\n\n\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n## fixed values ## \nmu = c(-3, 0, 3) # conditional means\ns2 = rep(1 / 3, 3) # conditional sds\nd = c(1, 2, 3) # sample space of delta\nN = 1000 # chain length\nw = c(.45, .1, .4) # delta probabilities\n\n## Gibbs sampler ##\nset.seed(360)\nN = 1000 # number of Gibbs samples\n\ntheta = 0 # initial theta value\nthd.mcmc = NULL\nfor(i in 1:N) {\nd = sample(1:3 , 1, prob = w * dnorm(theta, mu, sqrt(s2))) \ntheta = rnorm(1, mu[d], sqrt(s2[d]))\nthd.mcmc = rbind(thd.mcmc, c(theta,d))\n}\n# note we take advantage that sample() in R does not require the probability\n# to add up to 1\n\ndf = data.frame(theta = thd.mcmc[,1],\n                delta = thd.mcmc[,2])\n\ndf %>%\n  ggplot(aes(x = seq(1, nrow(df)), y = theta)) +\n  geom_line() +\n  theme_bw() +\n  labs(y = TeX(\"\\\\theta\"),\n       x = \"iteration\",\n       title = \"Traceplot of 1000 Gibbs samples\")\n\n\n\n\nExercise:\n\ndescribe how we implement the conditional update for delta in the code above\nwhat do you notice from the traceplot above? Hint: you can imagine hopping from delta islands in the first figure of the joint target over parameter space.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe picture to visualize is that of a particle moving through parameter space.\n\n\n\n\n\nLet‚Äôs see how well our samples of \\(\\theta\\) approximate the true marginal \\(p(\\theta)\\)."
  },
  {
    "objectID": "notes/lec09-mcmc-diagnostics.html#terms-to-describe-mcmc",
    "href": "notes/lec09-mcmc-diagnostics.html#terms-to-describe-mcmc",
    "title": "MCMC diagnostics",
    "section": "Terms to describe MCMC",
    "text": "Terms to describe MCMC\n\nautocorrelation: how correlated consecutive values in the chain are. Mathematically, we compute the sample autocorrelation between elements in the sequence that are \\(t\\) steps apart using\n\n\\[\n\\text{acf}_t(\\boldsymbol{\\phi}) =\n\\frac{\\frac{1}{S - t} \\sum_{s = 1}^{S-t} (\\phi_s - \\bar{\\phi})(\\phi_{s+t} - \\bar{\\phi})}\n{\\frac{1}{S-1} \\sum_{s = 1}^S (\\phi_s - \\bar{\\phi})^2}\n\\]\nwhere \\(\\boldsymbol{\\phi}\\) is a sequence of length \\(S\\) and \\(\\bar{\\phi}\\) is the mean of the sequence. In practice we use acf function in R. Example:\n\nacf(thd.mcmc[,1], plot = FALSE)\n\n\nAutocorrelations of series 'thd.mcmc[, 1]', by lag\n\n    0     1     2     3     4     5     6     7     8     9    10    11    12 \n1.000 0.962 0.959 0.954 0.951 0.948 0.948 0.943 0.941 0.936 0.933 0.931 0.928 \n   13    14    15    16    17    18    19    20    21    22    23    24    25 \n0.927 0.923 0.920 0.915 0.911 0.907 0.906 0.908 0.905 0.902 0.899 0.898 0.897 \n   26    27    28    29    30 \n0.895 0.891 0.891 0.887 0.887 \n\n\nThe higher the autocorrelation, the more samples we need to obtain a given level of precision for our approximation. One way to state how precise our approximation is, is with effective sample size.\n\neffective sample size (ESS): intuitively this is the effective number of exact samples ‚Äúcontained‚Äù in the Markov chain (see Betancourt 2018). For further reading on ESS, see the stan manual. In practice we use coda::effectiveSize() function to compute. Example:\n\n\nlibrary(coda)\neffectiveSize(thd.mcmc[,1])[[1]]\n\n[1] 2.065509\n\n\nMore precisely, the effective sample size (ESS) is the value \\(S_{eff}\\) such that\n\\[\nVar_{MCMC}[\\bar{\\phi}] = \\frac{Var[\\phi]}{S_{eff}}.\n\\]\nIn words, it‚Äôs the number of independent Monte Carlo samples necessary to give the same precision as the MCMC samples. For comparison, recall \\(Var_{MC}[\\bar{\\phi}] = Var[\\phi]/S\\)\n\nStationarity is when samples taken in one part of the chain have a similar distribution to samples taken from other parts of the chain. Intuitively, we want the particle to move from our arbitrary starting point to regions of higher probability\\(^*\\), then we will say it has achieved stationarity.\n\nTraceplots are a great way to visually inspect whether a chain has converged, or achieved stationarity. In the traceplot above we can see that samples from the beginning of the chain look very different than samples at the end.\n\\(^*\\) recall that probability is really a volume in high dimensions of parameter space, and so it is not enough for a pdf to evaluate to a high value, there must also be sufficient volume.\n\nMixing: how well the particle moves between sets of high probability. Some might refer to this as how well the particle sojourns across the ‚Äútypical set‚Äù (regions of high probability)."
  },
  {
    "objectID": "notes/lec09-mcmc-diagnostics.html#extra-practice",
    "href": "notes/lec09-mcmc-diagnostics.html#extra-practice",
    "title": "MCMC diagnostics",
    "section": "Extra practice",
    "text": "Extra practice\nGibbs sample the target above 10 thousand times. Report and discuss both the autocorrelation and ESS."
  },
  {
    "objectID": "notes/lec10-mvn.html",
    "href": "notes/lec10-mvn.html",
    "title": "Multivariate normal",
    "section": "",
    "text": "Example 1: Twenty two students take a reading comprehension test before and after receiving an instructional method. The result for each student is a bivariate vector \\(Y_i\\) that includes a pre- and post- instructional score.\n\n\n\n\n\nExample 2: We measure three features of Gentoo penguins: bill length, bill depth and body mass. For each penguin we record \\(Y_i\\), a three-dimensional vector of trait measurements.\n\n\n\n\n\n\n\n\n\nWe say a \\(p\\) dimensional vector \\(\\boldsymbol{Y}\\) has a multivariate normal distribution if its sampling density is given by\n\\[\np(\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp\\{\n-\\frac{1}{2}(\\boldsymbol{y}-\\boldsymbol{\\theta})^T \\Sigma^{-1} (\\boldsymbol{y}- \\boldsymbol{\\theta})\n\\}\n\\]\nwhere\n\\[\n\\boldsymbol{y}=  \\left[ {\\begin{array}{cc}\n   y_1 \\\\\n   y_2\\\\\n   \\vdots\\\\\n   y_p\n  \\end{array} } \\right]\n  ~~~\n   \\boldsymbol{\\theta}= \\left[ {\\begin{array}{cc}\n   \\theta_1 \\\\\n   \\theta_2\\\\\n   \\vdots\\\\\n   \\theta_p\n  \\end{array} } \\right]\n  ~~~\n  \\Sigma =\n  \\left[ {\\begin{array}{cc}\n   \\sigma_1^2 & \\sigma_{12}& \\ldots & \\sigma_{1p}\\\\\n   \\sigma_{12} & \\sigma_2^2 &\\ldots & \\sigma_{2p}\\\\\n   \\vdots & \\vdots & & \\vdots\\\\\n   \\sigma_{1p} & \\ldots & \\ldots & \\sigma_p^2\n  \\end{array} } \\right].\n\\]\n\n\n\n\n\\(\\boldsymbol{y}\\in \\mathbb{R}^p\\) ; \\(\\boldsymbol{\\theta}\\in \\mathbb{R}^p\\); \\(\\Sigma > 0\\)\n\\(E[\\boldsymbol{y}] = \\boldsymbol{\\theta}\\)\n\\(V[\\boldsymbol{y}] = E[(\\boldsymbol{y}- \\boldsymbol{\\theta})(\\boldsymbol{y}- \\boldsymbol{\\theta})^T] = \\Sigma\\)\nMarginally, \\(y_i \\sim N(\\theta_i, \\sigma_i^2)\\).\nIf \\(\\boldsymbol{\\theta}\\) is a MVN random vector, then the kernel is \\(\\exp\\{-\\frac{1}{2} \\boldsymbol{\\theta}^T A \\boldsymbol{\\theta}+ \\boldsymbol{\\theta}^T \\boldsymbol{b} \\}\\). The mean is \\(A^{-1}\\boldsymbol{b}\\) and the covariance is \\(A^{-1}\\).\n\n\n\nlibrary(mvtnorm) contains functions we need.\n\nrmvnorm() to sample from a multivariate normal\ndmvnorm() to compute the density\npmvnorm() to compute the distribution function\nqmvnorm() to compute quantiles of the multivariate normal"
  },
  {
    "objectID": "notes/lec10-mvn.html#matrix-algebra-fundamentals",
    "href": "notes/lec10-mvn.html#matrix-algebra-fundamentals",
    "title": "Multivariate normal",
    "section": "Matrix algebra fundamentals",
    "text": "Matrix algebra fundamentals\n\nmatrix facts\n\nmatrix multiplication proceeds row \\(\\times\\) column, so if we have the product \\(AB\\), \\(A\\) must have the same number of ___ as B has ___.\nthe determinant of a matrix, \\(|A|\\), measures the size of the matrix\nthe identity matrix is the matrix multiplicative identity. It is represented by \\(\\boldsymbol{I}\\), in general \\(\\boldsymbol{I}_p\\) is a \\(p \\times p\\) matrix with 1 on each diagonal and 0 on every off-diagonal. \\(\\boldsymbol{I}A = A \\boldsymbol{I}= A\\).\nthe inverse of a matrix \\(A^{-1}\\) works as follows: \\(A A^{-1} = A^{-1}A = \\boldsymbol{I}\\).\nthe trace of a matrix, tr(A), is the sum of its diagonal elements\norder matters: \\(AB \\neq BA\\) in general.\n\\(\\Sigma > 0\\) is shorthand for saying the matrix is positive definite. This means that for all vectors \\(\\boldsymbol{x}\\), the quadratic form \\(\\boldsymbol{x}^T \\Sigma \\boldsymbol{x} > 0\\). \\(Sigma > 0 \\iff\\) all eigenvalues of \\(\\Sigma\\) are positive.\n\nExercise:\n\n\\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{b}\\) are \\(p \\times 1\\) vectors, \\(A\\) is a symmetric matrix. Simplify \\(\\boldsymbol{b}^T A \\boldsymbol{\\theta}+ \\boldsymbol{\\theta}^T A \\boldsymbol{b}\\) what is the dimension of the result?\nwhat‚Äôs the dimension of \\(V[\\boldsymbol{y}]\\)?\n\n\n\nmatrix operations in R\n\n# make a matrix A\nA = matrix(c(1,.2, .2, 2), ncol = 2)\nA\n\n     [,1] [,2]\n[1,]  1.0  0.2\n[2,]  0.2  2.0\n\n# invert A (expensive for large matrices)\nAinv = solve(A)\n\n# matrix multiplication\nAinv %*% A\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n# determinant of A\ndet(A)\n\n[1] 1.96\n\n# trace of A\nsum(diag(A))\n\n[1] 3\n\n# create a vector b\nb = matrix(c(1, 2), ncol = 1)\nb\n\n     [,1]\n[1,]    1\n[2,]    2\n\n# transpose the vector b\nt(b)\n\n     [,1] [,2]\n[1,]    1    2\n\n\n\nb %*% A\n\nError in b %*% A: non-conformable arguments\n\n\n\nWhat went wrong in the code above?"
  },
  {
    "objectID": "notes/lec10-mvn.html#semiconjugate-priors",
    "href": "notes/lec10-mvn.html#semiconjugate-priors",
    "title": "Multivariate normal",
    "section": "Semiconjugate priors",
    "text": "Semiconjugate priors\n\nsemiconjugate prior for \\(\\boldsymbol{\\theta}\\)\nIf\n\\[\n\\begin{aligned}\n\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma &\\sim MVN(\\boldsymbol{\\theta}, \\Sigma),\\\\\n\\boldsymbol{\\theta}&\\sim MVN(\\mu_0, \\Lambda_0),\n\\end{aligned}\n\\]\nthen\n\\[\n\\boldsymbol{\\theta}| \\boldsymbol{y}, \\Sigma \\sim MVN(\\boldsymbol{\\mu_n}, \\Lambda_n),\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\Lambda_n &= (\\Lambda_0^{-1} + n \\Sigma^{-1} )^{-1},\\\\\n\\boldsymbol{\\mu_n} &= (\\Lambda_0^{-1} + n \\Sigma^{-1} )^{-1}(\\Lambda_0^{-1} \\boldsymbol{\\mu}_0 + n \\Sigma^{-1} \\bar{\\boldsymbol{y}}).\n\\end{aligned}\n\\]\nExercise: interpret \\(E[\\boldsymbol{\\theta}| \\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n, \\Sigma]\\) and \\(Cov[\\boldsymbol{\\theta}| \\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n, \\Sigma]\\).\n\n\nsemiconjugate prior for \\(\\Sigma\\)\nIf\n\\[\n\\begin{aligned}\n\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma &\\sim MVN(\\boldsymbol{\\theta}, \\Sigma),\\\\\n\\Sigma &\\sim \\text{inverse-Wishart}(\\nu_0, S_0^{-1}),\n\\end{aligned}\n\\]\nthen\n\\[\n\\Sigma | \\boldsymbol{y}, \\boldsymbol{\\theta}\\sim \\text{inverse-Wishart} (\\nu_0 + n, (S_0 + S_{\\theta})^{-1}),\n\\]\nwhere \\(S_\\theta = \\sum_{i=1}^n (\\boldsymbol{y}_i - \\boldsymbol{\\theta})(\\boldsymbol{y}_i - \\boldsymbol{\\theta})^T\\) is the residual sum of squares matrix for the vectors \\(\\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n\\) if the population mean is \\(\\boldsymbol{\\theta}\\).\n\n\n\nthe inverse-Wishart\nthe inverse-Wishart\\((\\nu_0, S_0^{-1})\\) density is given by\n\\[\n\\begin{aligned}\np(\\Sigma | \\nu_0, S_0^{-1}) = \\left[\n2^{\\nu_0 p / 2} \\pi^{{p \\choose 2}/2} |S_0|^{-\\nu_0/2} \\prod_{j = 1}^p \\Gamma([\\nu_0 + 1 - j]/2)\n\\right]^{-1} \\times\\\\\n|\\Sigma|^{-(\\nu_0 + p + 1)/2} \\times \\exp \\{ -\\frac{1}{2}tr(S_0 \\Sigma^{-1})\\}.\n\\end{aligned}\n\\]\n\nKey facts\n\nnotice that the first line is the normalizing constant of the density\nthe support is \\(\\Sigma > 0\\) and \\(\\Sigma\\) symmetric \\(p \\times p\\) matrix. \\(\\nu_0 \\in \\mathbb{N}^+\\) and \\(\\nu_0 \\geq p\\). \\(S_0\\) is a \\(p \\times p\\) symmetric positive definite matrix.\nif \\(\\Sigma\\) is inv-Wishart\\((\\nu_0, S_0^{-1})\\) then \\(\\Sigma^{-1}\\) is Wishart\\((\\nu_0, S_0^{-1})\\).\n\\(E[\\Sigma^{-1}] = \\nu_0 S_0^{-1}\\) and \\(E[\\Sigma] = \\frac{1}{\\nu_0 - p - 1} S_0\\).\nintuition: \\(\\nu_0\\) is prior sample size. \\(S_0\\) is a prior guess of the covariance matrix.\n\n\n\nsampling from the inverse-Wishart\n\npick \\(\\nu_0 > p\\), pick \\(S_0\\)\nsample \\(\\boldsymbol{z}_1, \\ldots \\boldsymbol{z}_{\\nu_0} \\sim \\text{ i.i.d. } MVN(\\boldsymbol{0}, S_0^{-1})\\)\ncalculate \\(\\boldsymbol{Z}^T \\boldsymbol{Z} = \\sum_{i = 1}^{\\nu_0} \\boldsymbol{z}_i \\boldsymbol{z}^T\\)\nset \\(\\Sigma = (\\boldsymbol{Z}^T \\boldsymbol{Z})^{-1}\\)\n\n\nlibrary(mvtnorm) # contains function rmvnorm\n\n# 2x2 example: generating 1 sample from an inv-Wishart\nset.seed(360)\np = 2\nnu0 = 3\nS0 = matrix(c(1, .1, .1, 1), ncol = 2)\nS0inv = solve(S0)\nZ = rmvnorm(n = nu0, # number of observations of the 2D vector Z\n        mean = rep(0, p), # mean 0\n        sigma = S0inv) # prior variance\nSigma = solve(t(Z) %*% Z)\neigen(Sigma)$values\n\n[1] 0.7821737 0.4174527\n\nSigma\n\n           [,1]      [,2]\n[1,]  0.5271834 -0.167273\n[2,] -0.1672730  0.672443\n\n\nExercise/show offline: why does this work? Hint: what is \\(cov[\\boldsymbol{z}]\\)?\nWe can also use the monomvn package to simulate from a Wishart more succinctly,\n\nlibrary(monomvn)\n\nset.seed(360)\nSigma = solve(rwish(nu0, S0inv))\neigen(Sigma)$values\n\n[1] 0.692529 0.160428\n\nSigma\n\n          [,1]      [,2]\n[1,] 0.1899212 0.1217519\n[2,] 0.1217519 0.6630358"
  },
  {
    "objectID": "notes/lec11-missing-data-mvn.html",
    "href": "notes/lec11-missing-data-mvn.html",
    "title": "Inference under MVN with missing data",
    "section": "",
    "text": "This example is from Hoff ch.¬†7.\nLoad libraries and data.\n\nlibrary(tidyverse)\nlibrary(mvtnorm)\nlibrary(monomvn)\nlibrary(coda)\nY = read_csv(\"https://sta360-fa23.github.io/data/Pima.csv\") %>%\n  as.matrix() \ncolnames(Y) = NULL\n\nThis data set contains\n\nglu blood plasma glucose concentration\nbp diastolic blood pressure\nskin skin fold thickness\nbmi body mass index\n\nfor 200 women of Pima Indian heritage living near Phoenix, Arizona (Smith et al, 1988). Some observations are missing.\n\n\nSetup prior parameters and starting values.\n\n## prior parameters\nn = nrow(Y); p = ncol(Y)\n# prior on theta\nmu0 = c(120, 64, 26, 26); sd0 = (mu0 / 2)\nL0 = matrix(.1, p, p)\ndiag(L0) = 1 \nL0 = L0 * outer(sd0, sd0) # \\Lambda_0\n# prior on Sigma\nnu0 = p + 2; \nS0 = L0\n###\n\n### starting values\nSigma = S0\nY.full = Y\nO = 1 * (!is.na(Y)) # indices for observe values of Y\n\nfor(j in 1:p) {\n  Y.full[is.na(Y.full[,j]),j]<-mean(Y.full[,j],na.rm=TRUE)\n} \n\nExercise: what does the code above set the starting values of \\(\\theta, \\Sigma\\) and \\(\\textbf{Y}_{\\text{mis}}\\) to?\nThe Gibbs sampler.\n\n### Gibbs sampler\nTHETA <- SIGMA <- Y.MISS <- NULL\nset.seed(360)\n\nfor(s in 1:1000) {\n\n  ###update theta\n  ybar <- apply(Y.full, 2 , mean)\n  Ln <- solve(solve(L0) + n * solve(Sigma))\n  mun <- Ln %*% (solve(L0) %*% mu0 + n * solve(Sigma) %*% ybar)\n  theta <- rmvnorm(1, mun, Ln)\n  ###\n  \n  ###update Sigma\n  Sn <- S0 + (t(Y.full) - c(theta)) %*% t(t(Y.full) - c(theta))\n  Sigma <- rwish(nu0 + n, solve(Sn))\n  ###\n  \n  ###update missing data\n  for(i in 1:n) { \n    b <- (O[i, ] == 0)\n    a <- (O[i, ] == 1)\n    if( sum(b) != 0) {\n    iSa <- solve(Sigma[a, a])\n    beta.j <- Sigma[b, a] %*% iSa\n    s2.j   <- Sigma[b, b] - Sigma[b, a] %*% iSa %*% Sigma[a, b]\n    theta.j <- theta[b] + beta.j %*% (as.matrix(Y.full[i, a]) - theta[a])\n    Y.full[i, b] <- rmvnorm(1, theta.j, s2.j)\n    }\n  }\n  \n  ### save results\n  THETA<-rbind(THETA,theta) ; SIGMA<-rbind(SIGMA,c(Sigma))\n  Y.MISS<-rbind(Y.MISS, Y.full[O==0] )\n  ###\n\n  if(s %% 250 == 0 | s == 1) {\n  cat(s,theta,\"\\n\")\n  }\n}\n\n1 129.6031 71.66437 28.97947 31.385 \n250 123.6105 70.56862 29.10075 30.97481 \n500 123.6067 70.36211 29.20791 31.03185 \n750 123.6179 70.78825 29.29216 30.90945 \n1000 123.6017 70.15369 29.22685 31.08064 \n\n#### Posterior mean\napply(THETA,2,mean)\n\n[1] 123.60189  70.57085  29.16132  31.79352\n\n\n\n\n\nEffective sample size of THETA\n\n# effective sample size of THETA \napply(THETA, 2, effectiveSize)\n\n[1]  879.4569 3680.4806 4411.2164 7721.7664"
  },
  {
    "objectID": "notes/lec12-hierarchical-intro.html",
    "href": "notes/lec12-hierarchical-intro.html",
    "title": "Hierarchical modeling",
    "section": "",
    "text": "Example from Hoff Ch. 8\n\nEach year, students across North Carolina take an identical standardized test. In our sample, we observe scores from students at \\(m\\) different schools. At the \\(j\\)th school, \\(n_j\\) students take the exam and \\(j \\in \\{1, \\ldots m\\}\\). The exam is designed to give an average score of 50 on a 0 to 100 scale.\n\n\n\n# load packages\nlibrary(tidyverse)\nlibrary(coda)\n\n\n# load data\nmathScores = read_csv(\"https://sta360-fa23.github.io/data/mathScores.csv\")\n\n\nhead(mathScores, n = 3)\n\n# A tibble: 3 √ó 2\n  school mathscore\n   <dbl>     <dbl>\n1      1      52.1\n2      1      57.6\n3      1      66.4\n\n\nCodebook\n\nschool: which school the math score came from\nmathScore: score from 0 to 100 of an individual student\n\n\n\n\n\n\n\n\nConvert data to list for downstream processing\nY.school.mathscore <- as.matrix(mathScores)\n#### Put data into list form.\nY <- list()\nJ <- max(Y.school.mathscore[, 1])\nn <- ybar <- ymed <- s2 <- rep(0, J)\nfor (j in 1:J) {\n  Y[[j]] <- Y.school.mathscore[Y.school.mathscore[, 1] == j, 2]\n}"
  },
  {
    "objectID": "notes/lec12-hierarchical-intro.html#questions-about-the-data",
    "href": "notes/lec12-hierarchical-intro.html#questions-about-the-data",
    "title": "Hierarchical modeling",
    "section": "Questions about the data",
    "text": "Questions about the data\n\nHow are the schools ranked?\nDoes school 51 have a higher average score than school 41?\nWhat is the probability a single student randomly selected from school 51 performs better on the exam than a single student randomly selected from school 41?"
  },
  {
    "objectID": "notes/lec12-hierarchical-intro.html#model",
    "href": "notes/lec12-hierarchical-intro.html#model",
    "title": "Hierarchical modeling",
    "section": "Model",
    "text": "Model\nSuppose students scores at school \\(j\\) are exchangeable for all \\(n_j\\). By de Finetti‚Äôs theorem, this means\n\\[\n\\{y_{1,j}, \\ldots y_{n_j,j} | \\phi_j \\} \\sim \\text{ i.i.d. } p(y|\\phi_j).\n\\]\nThat is, the student‚Äôs scores at school \\(j\\) are conditionally i.i.d. given some school specific parameters \\(\\phi_j\\). This describes our within-group sampling variability.\nNow suppose that all the schools we sampled are similar in some way. Maybe they belong to some larger population of schools across the country i.e.¬†schools in North Carolina are somewhat distinct from schools in South Carolina. We might imagine that the school-specific parameters themselves are exchangeable for all \\(m\\). By de Finetti‚Äôs theorem, this means\n\\[\n\\{\\phi_1, \\ldots \\phi_m\\} \\sim \\text{ i.i.d. } p(\\phi|\\psi).\n\\]\nIn words, school-specific parameters are conditionally i.i.d. given some population specific parameters \\(\\psi\\). This describes our between-group sampling variability.\nFinally, if our hierarchy stops there, then to complete model specification, we may describe our prior beliefs about \\(\\psi\\) according to some prior density \\(p(\\psi)\\).\nExercise: Imagine variability among scores is the same across all schools, but there does exist heterogeneity in the mean scores of the schools. Write down the mathemtical form of a model that describes this using the normal distribution. What are some priors you could pick on relevant parameters to make sure full conditionals are easy to compute for Gibbs sampling? What are the full conditionals?\n\nSolution:\n\nsampling distributions:\n\n\\[\n\\begin{aligned}\ny_j | \\theta_j, \\sigma^2 &\\sim N(\\theta_j, \\sigma^2)\\\\\n\\theta_j | \\mu, \\tau^2 &\\sim N(\\mu, \\tau^2)\n\\end{aligned}\n\\]\n\npriors distributions:\n\n\\[\n\\begin{aligned}\n1/\\sigma^2 &\\sim \\text{ gamma}(\\nu_0/2, \\nu_0 \\sigma_0^2/2)\\\\\n1/\\tau^2 &\\sim \\text{ gamma}(\\eta_0/2, \\eta_0 \\tau_0^2/2)\\\\\n\\mu &\\sim N(\\mu_0, \\gamma_0^2)\n\\end{aligned}\n\\]\n\nTo facilitate Gibbs sampling, notice\n\\[\np(\\theta_1, \\ldots \\theta_m, \\mu, \\tau^2, \\sigma^2 | \\mathbf{y}_1, \\ldots \\mathbf{y}_m) \\propto\np(\\mu, \\tau^2, \\sigma^2) p(\\theta_1, \\ldots \\theta_m | \\mu, \\tau^2, \\sigma^2) \\times p(\\mathbf{y}_1, \\ldots \\mathbf{y}_m| \\theta_1, \\ldots \\theta_m, \\mu, \\tau^2, \\sigma^2)\n\\]\nIt follows that the full conditionals are:\n\\[\n\\begin{aligned}\np(\\mu | \\cdot) &\\propto p(\\mu) \\prod_{j = 1}^m p(\\theta_j| \\mu, \\tau^2)\\\\\np(\\tau^2 | \\cdot) &\\propto p(\\tau^2) \\prod_{j = 1}^m p(\\theta_j| \\mu, \\tau^2)\\\\\np(\\sigma^2|\\cdot) &\\propto p(\\sigma^2)\\prod_{j =1}^m \\prod_{i = 1}^{n_j} p(y_{i,j}|\\theta_j, \\sigma^2)\\\\\np(\\theta_j | \\cdot) &\\propto p(\\theta_j | \\mu, \\tau^2) \\prod_{i = 1}^{n_j} p(y_{i,j}|\\theta_j, \\sigma^2)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/lec12-hierarchical-intro.html#gibbs-sampling",
    "href": "notes/lec12-hierarchical-intro.html#gibbs-sampling",
    "title": "Hierarchical modeling",
    "section": "Gibbs sampling",
    "text": "Gibbs sampling\n\n#### MCMC approximation to posterior for the hierarchical normal model\n\n## weakly informative priors\nnu0 <- 1; s20 <- 100\neta0 <- 1; t20 <- 100\nmu0 <- 50; g20 <- 25\n\n## starting values\nm <- length(Y)\nn <- sv <- ybar <- rep(NA, m)\nfor (j in 1:m)\n{\n  ybar[j] <- mean(Y[[j]])\n  sv[j] <- var(Y[[j]])\n  n[j] <- length(Y[[j]])\n}\ntheta <- ybar\nsigma2 <- mean(sv)\nmu <- mean(theta)\ntau2 <- var(theta)\n\n## setup MCMC\nset.seed(1)\nS <- 5000\nTHETA <- matrix(nrow = S, ncol = m)\nMST <- matrix(nrow = S, ncol = 3)\npredictiveY = NULL\n\n## MCMC algorithm\nfor (s in 1:S)\n{\n  # sample new values of the thetas\n  for (j in 1:m)\n  {\n    vtheta <- 1 / (n[j] / sigma2 + 1 / tau2)\n    etheta <- vtheta * (ybar[j] * n[j] / sigma2 + mu / tau2)\n    theta[j] <- rnorm(1, etheta, sqrt(vtheta))\n  }\n  \n  #sample new value of sigma2\n  nun <- nu0 + sum(n)\n  ss <- nu0 * s20\n  for (j in 1:m) {\n    ss <- ss + sum((Y[[j]] - theta[j]) ^ 2)\n  }\n  sigma2 <- 1 / rgamma(1, nun / 2, ss / 2)\n  \n  #sample a new value of mu\n  vmu <- 1 / (m / tau2 + 1 / g20)\n  emu <- vmu * (m * mean(theta) / tau2 + mu0 / g20)\n  mu <- rnorm(1, emu, sqrt(vmu))\n  \n  # sample a new value of tau2\n  etam <- eta0 + m\n  ss <- eta0 * t20 + sum((theta - mu) ^ 2)\n  tau2 <- 1 / rgamma(1, etam / 2, ss / 2)\n  \n  #store results\n  THETA[s, ] <- theta\n  MST[s, ] <- c(mu, sigma2, tau2)\n  \n  # predictive sampling\n  y51 = rnorm(1, mean = theta[51], sd = sqrt(sigma2))\n  y41 = rnorm(1, mean = theta[41], sd = sqrt(sigma2))\n  predictiveY = rbind(predictiveY, c(y51, y41))\n  \n}\n\nmcmc1 <- list(THETA = THETA, MST = MST)"
  },
  {
    "objectID": "notes/lec12-hierarchical-intro.html#mcmc-diagnostics",
    "href": "notes/lec12-hierarchical-intro.html#mcmc-diagnostics",
    "title": "Hierarchical modeling",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\ntrace plots\n\nplotscode\n\n\n\n\n\n\n\n\n\n\ncolnames(MST) = c(\"mu\", \"sigma2\", \"tau2\")\nMST2 = MST %>%\n  as.data.frame() %>% \n  pivot_longer(cols = 1:3)\n\nMST2 %>%\n  ggplot(aes(x = seq(1, nrow(MST2)), y = value)) +\n  geom_line() +\n  theme_bw() +\n  facet_wrap(~ name, scales = \"free_y\") +\n  labs(y = \"mu\",\n       x = \"iteration\",\n       title = \"Traceplot of 5000 Gibbs samples\")\n\n\n\n\n\n\neffective sample size and autocorrelation\n\neffectiveSize(MST)\n\n      mu    sigma      tau \n3925.336 4461.112 2905.517 \n\npar(mfrow=c(1,3))\nacf(MST[,1])\nacf(MST[,2]) \nacf(MST[,3]) \n\n\n\n\n\n\nposterior means and standard error\n\n# MC error of mu, sigma2, tau2\nMCERR <- apply(MST,2,sd)/sqrt( effectiveSize(MST) )\napply(MST,2,mean)\n\n      mu    sigma      tau \n48.12530 84.82892 24.79410 \n\nMCERR\n\n         mu       sigma         tau \n0.008528321 0.041664073 0.082344432 \n\n\nWe can do the exact same for the thetas, but the output will be 100 lines, so I suppress output below.\n\n# MC error of thetas\neffectiveSize(THETA) -> esTHETA\nTMCERR <- apply(THETA,2,sd)/sqrt( effectiveSize(THETA) )\nTMCERR"
  },
  {
    "objectID": "notes/lec12-hierarchical-intro.html#answers",
    "href": "notes/lec12-hierarchical-intro.html#answers",
    "title": "Hierarchical modeling",
    "section": "Answers",
    "text": "Answers\n\nHow are the schools ranked? How does the ordering compare to just ranking the schools by the sample means?\n\n\n# Ordering E[theta | data] and comparing to ybar\n\nposteriorMean = THETA %>%\n  apply(2, mean)\n\norderedTable = mathScores %>%\n  group_by(school) %>%\n  summarize(ybar = mean(mathscore),\n            n = n()) %>%\n  cbind(posteriorMean) %>%\n  arrange(posteriorMean) %>%\n  relocate(school, n, ybar, posteriorMean) %>%\n  mutate_if(is.numeric, round, digits = 2)\n\nDT::datatable(\n  orderedTable,\n  fillContainer = FALSE, options = list(pageLength = 10)\n)\n\n\n\n\n\n\nHow many of the schools are ranked in the same position in the posterior ordering as the sample mean ordering?\n\noutputcode\n\n\n\n\n[1] 46\n\n\n\n\n\npostOrdering = posteriorMean %>%\n  order()\n\nybarOrdering = mathScores %>%\n  group_by(school) %>%\n  summarize(ybar = mean(mathscore), \n            n = n()) %>%\n  arrange(ybar) %>%\n  pull(school)\n\nsum(postOrdering == ybarOrdering)\n\n\n\n\n\nDoes school 51 have a higher average score than school 41? Re-cast as a Bayesian question: what‚Äôs \\(p(\\theta_{51} > \\theta_{41} | \\text{data})\\)?\n\n\nmean(THETA[,51] > THETA[,41])\n\n[1] 0.9892\n\n\n\nWhat‚Äôs the probability a student randomly selected from school 51 performs better than a student selected randomly from school 41?\n\nBefore looking at the solution below, how would you answer this problem?\n\n\nSolution\nmean(predictiveY[,1] > predictiveY[,2])\n\n# output:\n# [1] 0.685"
  },
  {
    "objectID": "notes/lec13-regression-intro.html",
    "href": "notes/lec13-regression-intro.html",
    "title": "Intro to regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(scatterplot3d)"
  },
  {
    "objectID": "notes/lec13-regression-intro.html#background-review",
    "href": "notes/lec13-regression-intro.html#background-review",
    "title": "Intro to regression",
    "section": "Background review",
    "text": "Background review\n\nLinear modeling and linear regression\n\n\n\n\n\n\nA generalized linear model states \\(E[Y|X] = g(X\\beta)\\), for some invertible ‚Äúlink‚Äù function \\(g\\).\nLinear regression is but a special case, \\(E[Y|X] = X \\beta\\). This is what we will focus on today.\nLeast squares regression (otherwise termed ‚Äúordinary least squares‚Äù or ‚ÄúOLS‚Äù) refers to a particular method of estimating \\(\\beta\\): minimize the sum of squared residuals.\n\n\n\nNotation\n\n\\(\\mathbf{y} = \\{y_1, \\ldots y_n\\}\\) is an \\(n \\times 1\\) vector outcomes. Also called the ‚Äúresponse‚Äù or ‚Äúdependent variable‚Äù. \\(y_i\\) is an individual observed outcome.\n\\(\\mathbf{x}_i\\) is a \\(p \\times 1\\) vector of predictors also called ‚Äúregressors‚Äù, ‚Äúindependent variables‚Äù, ‚Äúcovariates‚Äù, or ‚Äúfeatures‚Äù.\n\\(X\\) is a \\(n \\times p\\) matrix of all covariates. This is often referred to as ‚Äúthe data matrix‚Äù.\n\\(\\beta\\) is a \\(p \\times 1\\) vector of constants. These are referred to as parameters. These are fixed, but unknown numbers. Being Bayesian, we will describe our uncertainty about this population parameter vector using probability statements.\n\n\n\nCommon convention\nThe linear model\n\\[\nE[Y | X\\beta] = X \\beta\n\\]\noften has the hidden convention that the first column of \\(X\\) is all 1s and \\(\\beta_1\\) is understood to be the intercept term. E.g.\n\\[\nE [ Y | X ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_1\\\\\n    \\beta_2\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix}\n\\]\n\n\nIllustration of a linear model\n\nExample\nImagine we‚Äôve collected 3 measurements on a number of penguins:\n\nbody mass (g)\nbill length (mm)\nflipper length (mm)\n\nThe first five entries of our data set are given below:\n\n\n# A tibble: 5 √ó 3\n  body_mass_g bill_length_mm flipper_length_mm\n        <int>          <dbl>             <int>\n1        3750           39.1               181\n2        3800           39.5               186\n3        3250           40.3               195\n4        3450           36.7               193\n5        3650           39.3               190\n\n\nIn all, our data set contains the measurements of 342 penguins. Because we‚Äôve collected three measurements, each individual penguin can be represented as a point in three dimensional space:\n\n\n\n\n\nNow, imagine it‚Äôs hard to measure a penguin‚Äôs bodymass because it‚Äôs difficult to get them onto a scale. We wish to develop a linear model that uses bill length and flipper length to predict body mass,\n\\[\nE[Y|X] = X \\beta,\n\\]\nwhere\n\n\\(Y\\) is the body mass of the penguins and\n\\(X\\) contains covariates bill length and flipper length.\n\nWhat does our linear model look like?\n\n\n\n\n\nIn general, for \\(D\\) measurements, a linear model is a \\(D-1\\) dimensional hyperplane!\n\n\n\nTraditional way to find the hyperplane\nTo ‚Äúfit‚Äù a linear regression model means to estimate \\(\\beta\\). One way to do this is to minimize some objective function. A really common function to minimize is the sum of square residuals SSR.\nA residual is defined as the distance our mean is from the true value:\n\\[\n\\begin{aligned}\nr_i &= y_i - E[y_i | \\mathbf{x}\\beta]\\\\\n&= y_i - \\beta^T\\mathbf{x}_i\n\\end{aligned}\n\\]\nThus the sum of square residuals is:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n r_i^2 &= \\sum_{i=1}^n (y_i - \\beta^T \\mathbf{x}_i)^2\\\\\n&= (\\mathbf{y} -X\\beta)^T(\\mathbf{y} -X\\beta)\\\\\n&= \\mathbf{y}^T \\mathbf{y} - 2\\beta^T X^T\\mathbf{y} + \\beta^TX^TX\\beta)\n\\end{aligned}\n\\]\nExercise 1: what other objective functions could we optimize?\nThe ordinary least squares (OLS) estimate is\n\\[\n\\hat{\\beta}_{OLS} = (X^TX)^{-1}X^T \\mathbf{y}\n\\]\n\nSee the matrix cookbook by Petersen and Petersen for all your matrix algebra needs.\n\nExercise 2: is \\(\\hat{\\beta}_{OLS}\\) biased?\n\n\nNormal linear regression model\nSo far, we had not made any distributional assumptions, we only made an assumption about the expectation. Now for the normal linear regression model,\n\\[\nY = X \\beta + \\epsilon\n\\]\nand \\(\\epsilon \\sim N(0, \\sigma^2 I)\\) where \\(I\\) is a \\(n \\times n\\) identity matrix. This is a way of saying \\(\\epsilon_i \\sim_{iid} N(0, \\sigma^2)\\).\nTherefore,\n\\[\n\\mathbf{y} | X, \\beta, \\sigma^2 \\sim MVN(X\\beta, \\sigma^2 I)\n\\]\nExercise 3: What is \\(Var(\\hat{\\beta}_{OLS})\\) under the normal model?\n\n\n\n\n\n\nHint\n\n\n\nLet \\(z\\) be a random vector. \\(Var[Az] = A Var(z) A^T\\).\n\n\nExercise 4: What is \\(\\hat{\\beta}_{MLE}\\)?\n\n\nAssumptions\nA brief reminder about the flexibility and limitations of classical linear regression.\n\nLimitations so far:\n\nthe mean may not be a good summary of the conditional relationship, e.g.¬†if \\(p(y|x)\\) is skewed, multimodal, or has heavy tails.\nerror may not be iid. In other words, the conditional variance of \\(Y\\) may change with the \\(\\mathbf{x}\\)s.\n\n\n\nFlexibility\nIs this an example of linear regression?\n\\[\ny_i = \\beta_1 + \\beta_2x_1 + \\beta_3 x_1^2 + \\beta_4 \\log x_1 + \\beta_5 x_2 + \\beta_6 x_1 x_2 + \\epsilon_i\n\\]\nWhat‚Äôs linear about linear regression? The parameters!\nThis is powerful, because nonlinear relationships between \\(X\\) and \\(\\mathbf{y}\\) can often be corrected by a power transformation of \\(X\\), \\(\\mathbf{y}\\) or of both variables."
  },
  {
    "objectID": "notes/lec13-regression-intro.html#bayesian-regression",
    "href": "notes/lec13-regression-intro.html#bayesian-regression",
    "title": "Intro to regression",
    "section": "Bayesian regression",
    "text": "Bayesian regression\nLet‚Äôs assume the normal sampling model (i.e.¬†normal data generative process, aka normal likelihood),\n\\[\n\\mathbf{y} | X, \\beta, \\sigma^2 \\sim MVN(X\\beta, \\sigma^2I).\n\\]\nTo make inference about our model parameters, we will construct a posterior distribution,\n\\[\np(\\beta, \\sigma^2 | \\mathbf{y}, X) \\propto \\underbrace{ p(\\mathbf{y}|X, \\beta, \\sigma^2)}_{likelihood} \\underbrace{p(\\beta, \\sigma^2)}_{prior}\n\\]\n\nsemi-conjugate prior specification\nTo setup Gibbs sampling, let‚Äôs consider independent semi-conjugate priors, i.e.¬†assume \\(p(\\beta, \\sigma^2) = p(\\beta) p(\\sigma^2)\\)\nBefore reading ahead, what do you think semi-conjugate priors will be for the parameters under the normal linear regression model?\n\nsemi-conjuate prior on \\(\\beta\\)\nIf\n\\[\n\\beta \\sim MVN(\\beta_0, \\Sigma_0)\n\\]\nthen\n\\[\n\\begin{aligned}\np(\\beta|\\mathbf{y}, X, \\sigma^2) &\\propto\np( \\mathbf{y} | X, \\beta, \\sigma^2) p(\\beta)\\\\\n&\\propto MVN(\\mathbf{m}, V)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\nV = Var[\\beta | \\mathbf{y}, X, \\sigma^2] &=\n(\\Sigma_0^{-1} + X^TX / \\sigma^2)^{-1}\\\\\n\\mathbf{m} = E[\\beta | \\mathbf{y}, X, \\sigma^2] &= (\\Sigma_0^{-1} + X^T X/ \\sigma^2)^{-1}(\\Sigma_0^{-1} \\beta_0 + X^T\\mathbf{y} / \\sigma^2)\n\\end{aligned}\n\\]\n\n\nsemi-conjugate prior on \\(\\sigma^2\\)\nLet‚Äôs re-parameterize. Let \\(\\gamma = 1/\\sigma^2\\).\nIf\n\\[\n\\gamma \\sim \\text{gamma}(\\nu_0 /2, \\nu_0 \\sigma_0^2 / 2)\n\\]\nthen\n\\[\n\\begin{aligned}\np(\\gamma | \\mathbf{y}, X, \\beta) &\\propto p( \\mathbf{y} | X, \\beta, \\sigma^2) p(\\gamma)\\\\\n&\\propto\n\\text{gamma}([\\nu_0 + n]/2, [\\nu_0 \\sigma_0^2 + SSR(\\beta)]/2)\n\\end{aligned}\n\\]\nExercise 5: write out the pseudo-code of a Gibbs sampler that samples from \\(p(\\beta, \\sigma^2| \\mathbf{y}, X)\\)."
  },
  {
    "objectID": "notes/lec14-BayesianRegression2.html",
    "href": "notes/lec14-BayesianRegression2.html",
    "title": "Bayesian regression II",
    "section": "",
    "text": "See libraries used in these notes\nlibrary(tidyverse)\nlibrary(latex2exp)\nlibrary(patchwork)\nlibrary(tidymodels)\nlibrary(scatterplot3d)\nlibrary(palmerpenguins)\nlibrary(mvtnorm)\nlibrary(coda)\nlibrary(animation)"
  },
  {
    "objectID": "notes/lec14-BayesianRegression2.html#gibbs-sampler",
    "href": "notes/lec14-BayesianRegression2.html#gibbs-sampler",
    "title": "Bayesian regression II",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nLast time\nWe set up the model,\n\\[\n\\begin{aligned}\n\\mathbf{y} | X, \\beta, \\sigma^2 &\\sim MVN(X\\beta, \\sigma^2 I)\\\\\n\\beta &\\sim MVN(\\beta_0, \\Sigma_0)\\\\\n1/\\sigma^2 &\\sim \\text{gamma}(\\nu_0/2, \\nu_0 \\sigma_0^2/2)\n\\end{aligned}\n\\]\nand derived the full conditionals\n\\[\n\\begin{aligned}\n\\beta | \\mathbf{y}, X, \\sigma^2 &\\sim MVN(\\mathbf{m}, V),\\\\\n1/\\sigma^2 | \\mathbf{y}, X, \\beta & \\sim\n\\text{gamma}([\\nu_0 + n]/2, [\\nu_0 \\sigma_0^2 + SSR(\\beta)]/2),\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\nV = Var[\\beta | \\mathbf{y}, X, \\sigma^2] &=\n(\\Sigma_0^{-1} + X^TX / \\sigma^2)^{-1},\\\\\n\\mathbf{m} = E[\\beta | \\mathbf{y}, X, \\sigma^2] &= (\\Sigma_0^{-1} + X^T X/ \\sigma^2)^{-1}(\\Sigma_0^{-1} \\beta_0 + X^T\\mathbf{y} / \\sigma^2).\n\\end{aligned}\n\\]\n\n\nDiffuse prior\nTo complete model specification, we must choose \\(\\beta_0\\), \\(\\Sigma_0\\), \\(\\sigma_0^2\\) and \\(\\nu_0\\).\nIf we know very little about the relationships between \\(X\\) and \\(\\mathbf{y}\\), we might wish to consider a ‚Äúdiffuse‚Äù prior that prescribes a large mass of uncertainty around each parameter.\n\nmath of priorpicture of prior\n\n\n\\[\n\\begin{aligned}\n\\beta & \\sim MVN(0, 1000 I)\\\\\n1/\\sigma^2 &\\sim \\text{gamma}(1, 10)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSampler pseudo-code\n\n\n\n\n\n\npseudo-code\n\n\n\n\npick a starting \\(\\sigma^{2(0)}\\), set \\(s = 0\\). Now for \\(s\\) in 1:S perform 1-3:\nupdate \\(\\beta\\):\n\n\ncompute \\(V\\) and \\(\\mathbf{m}\\)\nsample \\(\\beta^{(s+1)} \\sim MVN(\\mathbf{m}, V)\\)\n\n\nupdate \\(\\sigma^{2}\\):\n\n\ncompute \\(SSR(\\beta^{(s+1)})\\)\nsample \\(1/\\sigma^{2(s+1)} \\sim \\text{gamma}([\\nu_0 + n]/2, [\\nu_0 \\sigma_0^2 + SSR(\\beta^{s+1})]/2)\\)\n\n\nsave the states of \\(\\beta\\) and \\(\\sigma^2\\).\n\n\n\n\n\nCode\nThe Gibbs sampler for our penguin example:\n\n\nCode to reproduce penguins_subset\n# our example examines just a subset of the penguin data\npenguins_subset = penguins %>%\n  select(body_mass_g, flipper_length_mm, bill_length_mm) %>%\n  drop_na() %>%\n  mutate(body_mass_kg = body_mass_g / 1000) %>%\n  select(-body_mass_g)\n\nX = penguins_subset %>%\n  select(-body_mass_kg) %>%\n  mutate(one = rep(1, nrow(penguins_subset))) %>%\n  relocate(one) %>%\n  as.matrix() \n\ny = select(penguins_subset, \n           body_mass_kg) %>%\n  as.matrix()\n\n\n\nset.seed(360)\n# prior hyperparameters \np = 2 # number of covariates\nSigma0 = 1000 * diag(rep(1, p+1)) # p + 1 for intercept term\nb0 = rep(0, p + 1)\nnu0 = 2\nsigma02 = 10\nn = nrow(y)\n\n# starting values\n## note: gamma = 1 / sigma^2\ngamma = 1 / var(penguins_subset$body_mass_kg)\n\n# values we should compute just once\nSigmaInv = solve(Sigma0)\nX2 = t(X) %*% X\nXy = t(X) %*% y\nSIB0 = SigmaInv %*% b0\na = (nu0 + n) / 2\nnu0s02 = nu0 * sigma02\n\n## empty objects to fill\nBETA = NULL\nGAMMA = NULL\n\nS = 2000\nfor (s in 1:S) {\n  ### UPDATE BETA\n  V = solve(SigmaInv + (gamma * X2))\n  m = V %*% (Xy * gamma) # simplified since b0 = 0\n  beta = rmvnorm(1, mean = m, sigma = V)\n  \n  ### UPDATE SIGMA\n  SSR1 = (y - (X %*% t(beta)))\n  SSRB = t(SSR1) %*% SSR1\n  gamma = rgamma(1, a, ((nu0s02 + SSRB) / 2))\n  \n  ### SAVE STATES\n  GAMMA = c(GAMMA, gamma)\n  BETA = rbind(BETA, beta)\n}\n\n\n\nESS?\neffectiveSize(BETA)\n\n\nvar1 var2 var3 \n2000 2000 2000 \n\n\nESS?\neffectiveSize(GAMMA)\n\n\nvar1 \n2000 \n\n\nHow do posterior mean estimates compare to the OLS estimates?\n\nposteriorMean = apply(BETA, 2, mean)\nOLS = lm(body_mass_kg ~ flipper_length_mm + bill_length_mm, data = penguins_subset) \nOLS = OLS$coefficients\nrbind(OLS, posteriorMean)\n\n              (Intercept) flipper_length_mm bill_length_mm\nOLS             -5.736897        0.04814486    0.006047488\nposteriorMean   -5.723741        0.04801443    0.006336029\n\n\nWe might have figured out this is what we were going to see already based on the the fact that the expressions for \\(E[\\beta | \\mathbf{y}, X]\\) and \\(Var[\\beta | \\mathbf{y}, X]\\) look just like \\(E[\\hat{\\beta}_{OLS} | \\beta]\\) and \\(Var[\\hat{\\beta}_{OLS} | \\beta]\\) when the prior information is diffuse.\nWhat was the point of all that extra work? Well, we don‚Äôt just have a point estimate and a confidence interval, we have a whole posterior! We can quantify uncertainty about \\(\\beta\\) in an easy and intuitive way.\nUsing the posterior, we may find 95% posterior CI, compute \\(p(\\beta_i > 0 | \\mathbf{y}, X)\\), compute \\(p(\\beta_i > \\beta_j | \\mathbf{y}, X)\\), compute the posterior median, and a whole host of additional queries quickly and intuitively.\nLet‚Äôs take a look at the marginal posteriors.\n\n\n\n\n\n\nExercise\n\n\nIs flipper length or bill length a ‚Äúmore important‚Äù predictor of penguin body mass? Why?\n\n\n\n\n\nVisually\nFor each iteration of our Gibbs sampler, we‚Äôre sampling a hyperplane, i.e.¬†a set of \\(\\beta\\)s.\n\n\n\n\n\nExercise\n\n\nDiscuss how autocorrelation of \\(\\beta\\)s would affect our sampler based on the animation above."
  },
  {
    "objectID": "notes/lec14-BayesianRegression2.html#ridge-regression-and-the-normal-prior",
    "href": "notes/lec14-BayesianRegression2.html#ridge-regression-and-the-normal-prior",
    "title": "Bayesian regression II",
    "section": "Ridge regression and the normal prior",
    "text": "Ridge regression and the normal prior\nWhat if \\(p > n\\)? In words: what if we have more predictors than observations? \\(X\\) will be wide and therefore have linearly dependent columns.\nIn this case, \\(X^T X\\) is \\(p \\times p\\) but is of rank \\(n < p\\), i.e.¬†\\(X^TX\\) is not full rank and thus not invertible. Therefore, \\(\\hat{\\beta}_{OLS}\\) satisfying \\((X^T X)\\hat{\\beta}_{OLS} = X^T \\mathbf{y}\\) does not exist uniquely.\nSeparately, in the case of multicollinearity, where the columns of \\(X\\) are highly correlated, some eigenvalues of \\(X^TX\\) will be very small, which means \\((X^TX)^{-1}\\) will have very large eigenvalues, i.e.¬†\\(Var(\\hat{\\beta}_{OLS})\\) will be very large.\n\nIntuitively: we can fix this by shrinking some of the \\(\\beta_i\\) towards zero (reducing \\(p\\)).\nAlgebraically: one way we can fix this is by adding some positive quantity on the diagonals.\n\nFrequentists call this sort of algebraic fix ‚Äúridge regression‚Äù and define the problem thus:\n\\[\n\\hat{\\beta}_{ridge} = \\underset{\\beta}{\\mathrm{argmin}} \\underbrace{(\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X \\beta)}_{\\text{SSR}(\\beta)} + \\underbrace{\\lambda \\beta^T \\beta}_{L_2^2 ~\\text{penalty}}\n\\]\nwhere \\(\\lambda\\) is a tuning parameter called the ‚Äúridge coefficient‚Äù.\nBayesians obtain the same objective via the following prior on \\(\\beta\\),\n\\[\n\\beta \\sim MVN(0, \\sigma^2 I /\\lambda)\n\\]\n\nExercise\n\n\nShow that \\(\\hat{\\beta}_{ridge} = E[\\beta | \\mathbf{y}, X, \\sigma^2] = ((X^TX) + \\lambda I)^{-1} X^T \\mathbf{y}\\)."
  },
  {
    "objectID": "notes/lec15-MetropolisHastings.html",
    "href": "notes/lec15-MetropolisHastings.html",
    "title": "Metropolis-Hastings",
    "section": "",
    "text": "See libraries used in these notes\nlibrary(tidyverse)\nlibrary(latex2exp)\nlibrary(patchwork)\nlibrary(tidymodels)\nlibrary(mvtnorm)\nlibrary(coda)\nlibrary(animation)"
  },
  {
    "objectID": "notes/lec15-MetropolisHastings.html#notation",
    "href": "notes/lec15-MetropolisHastings.html#notation",
    "title": "Metropolis-Hastings",
    "section": "Notation",
    "text": "Notation\n\n\\(\\theta\\) is some parameter of interest.\n\\(\\pi(\\theta)\\) represents the target distribution of the parameter.\n\nQuestion: what does the phrase ‚Äúexplore parameter space‚Äù mean?"
  },
  {
    "objectID": "notes/lec15-MetropolisHastings.html#metropolis-algorithm",
    "href": "notes/lec15-MetropolisHastings.html#metropolis-algorithm",
    "title": "Metropolis-Hastings",
    "section": "Metropolis algorithm",
    "text": "Metropolis algorithm\n\nSample \\(\\theta^* | \\theta^{(s)} \\sim J(\\theta | \\theta^{(s)})\\)\nCompute the acceptance ratio \\(r = \\frac{\\pi(\\theta^*)}{\\pi(\\theta^{(s)})}\\)\nLet\n\n\\[\n\\theta^{(s+1)} =\n\\begin{cases}\n\\theta^* \\text{ with probability } \\min(r, 1)\\\\\n\\theta^{(s)} \\text{ with probability } 1 - \\min(r, 1)\n\\end{cases}\n\\]\n\nExample 1\nLet \\(\\pi(\\theta) = \\text{dnorm}(\\theta, 10, 1)\\) and let \\(J(\\theta | \\theta^{(s)}) = \\text{normal}(\\theta^{(s)},\\delta^2)\\).\nWe have to choose \\(\\delta\\). How should we choose it? Let‚Äôs gain some intuition by trying out three different values of \\(\\delta\\).\n\nset.seed(360)\ntheta_s = 0 # starting point\nTHETA = NULL # empty object to save iterations in\nS = 10000 # number of iteations\ndelta = 1 # proposal variance\naccept = 0 # keep track of acceptance rate\n\nfor (s in 1:S) {\n  # log everything for numerical stability #\n  \n  ### generate proposal and compute ratio r ###\n  theta_proposal = rnorm(1, mean = theta_s, sd = delta) \n  log.r = dnorm(theta_proposal, mean = 10, sd = 1, log = TRUE) - \n    dnorm(theta_s, mean = 10, sd = 1, log = TRUE)\n  \n  ### accept or reject proposal and add to chain ###\n  if(log(runif(1)) < log.r)  {\n    theta_s = theta_proposal\n    accept = accept + 1 \n  }\n  THETA = c(THETA, theta_s)\n}\n\n\n\n\nLet‚Äôs look at a trace plot\n\ntrace plotcode\n\n\n\n\n\n\n\n\n\n\ndf = data.frame(theta = THETA)\ndf %>%\n  ggplot(aes(x = 1:nrow(df), y = theta)) + \n  geom_line() +\n  theme_bw() +\n  labs(x = \"iteration\", y = TeX(\"\\\\theta\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs look at how various \\(\\delta\\) let us sample the target:\n\n\\(\\delta = 1\\)\\(\\delta = 4\\)\\(\\delta = 0.1\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\nThe fledglings of female song sparrows. To begin, let‚Äôs load the data.\n\n\nLoad the data\nyX = structure(c(3, 1, 1, 2, 0, 0, 6, 3, 4, 2, 1, 6, 2, 3, 3, 4, 7, \n2, 2, 1, 1, 3, 5, 5, 0, 2, 1, 2, 6, 6, 2, 2, 0, 2, 4, 1, 2, 5, \n1, 2, 1, 0, 0, 2, 4, 2, 2, 2, 2, 0, 3, 2, 1, 1, 1, 1, 1, 1, 1, \n1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, \n2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, \n5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 6, 1, 1, 9, 9, 1, 1, 1, 1, 1, 1, \n1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 25, 25, 16, 16, 16, 16, 16, \n16, 16, 16, 16, 16, 16, 16, 25, 16, 16, 16, 16, 25, 25, 25, 25, \n9, 9, 9, 9, 9, 9, 9, 36, 1, 1), .Dim = c(52L, 4L), .Dimnames = list(\n    NULL, c(\"fledged\", \"intercept\", \"age\", \"age2\")))\n\n\n\nyX %>%\n  head(n = 5)\n\n     fledged intercept age age2\n[1,]       3         1   3    9\n[2,]       1         1   3    9\n[3,]       1         1   1    1\n[4,]       2         1   1    1\n[5,]       0         1   1    1\n\ny = yX[,1]\nX = yX[,-1]\n\nThe model:\n\\[\n\\begin{aligned}\nY | X &\\sim \\text{Poisson}(\\exp[ \\beta^T \\boldsymbol{x}])\\\\\n\\beta &\\sim MVN(0, \\sqrt{10})\n\\end{aligned}\n\\]\nThe Metropolis algorithm with\n\\[\nJ(\\beta | \\beta^{(s)}) = MVN(\\beta^{(s)}, \\hat{\\sigma}^2(X^TX)^{-1})\n\\]\nwhere \\(\\hat{\\sigma}^2\\) is the sample variance of \\(\\{\\log(y_1 + 1/2), \\ldots, \\log(y_n + 1/2)\\}\\).\n\n\n\n\n\n\nNote\n\n\n\n\nThis variance is intuitively useful choice for \\(\\delta\\) since the posterior variance would be \\(\\sigma^2 (X^TX)^{-1}\\) in a normal regression problem.\nWe use \\(\\log(y + 1/2)\\) instead of \\(\\log y\\) because if \\(y=0\\), \\(\\log y\\) would be \\(-\\infty\\).\n\n\n\n\nset.seed(360)\nn = length(y)\np = ncol(X)\n\npmn.beta = rep(0, p) # prior mean beta\npsd.beta = rep(10, p) # prior sd beta\n\nvar.prop = var(log(y + 1/2)) * solve(t(X) %*% X) # proposal variance\n\nS = 10000\nbeta = rep(0, p); accepts = 0\nBETA = matrix(0, nrow = S, ncol = p)\nset.seed(1)\n\nfor (s in 1:S) {\n  # multivariate proposal of beta\n  beta.p = t(rmvnorm(1, beta, var.prop))\n  \n  # log ratio\n  lhr = sum(dpois(y, exp(X %*%beta.p), log = TRUE)) -\n    sum(dpois(y, exp(X %*% beta), log = TRUE)) + \n    sum(dnorm(beta.p, pmn.beta, psd.beta, log = TRUE)) -\n    sum(dnorm(beta, pmn.beta, psd.beta, log = TRUE)) \n  \n  if (log(runif(1)) < lhr) {\n    beta = beta.p ; accepts = accepts + 1\n  }\n  \n  BETA[s,] = beta\n}\n\nThe acceptance ratio is 0.428\nLet‚Äôs examine convergence.\n\ntrace plotsplot codeESSacf\n\n\n\n\n\n\n\n\n\n\nvalue = c(BETA[,1], BETA[,2], BETA[,3])\nn = length(value)\nbeta = c(rep(\"beta1\", n/3), rep(\"beta2\", n/3), rep(\"beta3\", n/3))\ndf = data.frame(value = value,\n                beta = beta) \n\ndf %>%\n  ggplot(aes(x = 1:nrow(df), y = value)) + \n  geom_line() + \n  facet_wrap(~ beta, scales = \"free_x\") +\n  theme_bw() +\n  labs(x = \"iteration\")\n\n\n\n\n# effective sample size\nBETA %>%\n  apply(2, effectiveSize)\n\n[1] 867.4750 825.6214 692.0495\n\n\n\n\n\npar(mfrow=c(1,3))\nacf(BETA[,1])\nacf(BETA[,2])\nacf(BETA[,3])"
  },
  {
    "objectID": "notes/lec15-MetropolisHastings.html#metropolis-hastings",
    "href": "notes/lec15-MetropolisHastings.html#metropolis-hastings",
    "title": "Metropolis-Hastings",
    "section": "Metropolis-Hastings",
    "text": "Metropolis-Hastings\nIf we have a vector of parameters \\(\\theta = \\theta_1, \\ldots \\theta_p\\) then we can choose a proposal \\(J(\\theta | \\theta^{(s)})\\) that updates all \\(\\theta\\) elements simultaneously (as seen above with \\(J(\\beta | \\beta^{(s)})\\).\nAlternatively, we could update blocks of \\(\\theta\\), e.g.¬†propose an update for the first \\(j\\) elements of \\(\\theta\\), \\(J_1(\\theta_1,\\ldots \\theta_j | \\theta_1^{(s)}, \\ldots \\theta_j^{(s)})\\) and then an update for the \\(p-j\\) remaining elements, \\(J(\\theta_{j+1}, \\ldots, \\theta_{p} | \\theta_{j+1}^{(s)}, \\ldots, \\theta_{p}^{(s)})\\).\nSeparately, we could even update each element of \\(\\theta\\) individually, e.g.¬†have an individual, different proposal on each \\(\\theta_i\\), \\(i \\in \\{1, \\ldots p\\}\\).\nWe might even combine block updates with individual updates.\nQuestion: Where have we seen block updates and individual updates within MCMC before?\nThe Metropolis-Hastings algorithm is a generalization of both the Metropolis algorithm and the Gibbs sampler.\n\nThe Metropolis-Hastings algorithm\nLet \\(\\pi(\\theta_1, \\theta_2)\\) be the target distribution. The Metropolis-Hastings algorithm proceeds:\n\nUpdate \\(\\theta_1\\):\n\n\nsample \\(\\theta_1^* \\sim J_1(\\theta_1 | \\theta_1^{(s)}, \\theta_2^{(s)})\\);\ncompute the acceptance ratio\n\n\\[\nr = \\frac{\\pi(\\theta_1^*, \\theta_2^{(s)})}{\\pi(\\theta_1^{(s)}, \\theta_2^{(s)})} \\times \\frac{J_1(\\theta_1^{(s)}| \\theta_1^*, \\theta_2^{(s)})}{\nJ_1(\\theta_1^{*}| \\theta_1^{(s)}, \\theta_2^{(s)})\n}\n\\]\n\nset \\(\\theta_1^{(s+1)}\\) to \\(\\theta_1^*\\) with probability \\(\\min(1, r)\\), otherwise set \\(\\theta_1^{(s+1)}\\) to \\(\\theta_1^{(s)}\\).\n\n\nRepeat the above to update \\(\\theta_2\\) given \\(\\theta_1^{(s+1)}\\).\n\n\n\n\n\n\n\nImportant\n\n\n\nHere, the proposal distribution \\(J\\) need not be symmetric!"
  },
  {
    "objectID": "notes/lec16-MCMC-and-HMC.html",
    "href": "notes/lec16-MCMC-and-HMC.html",
    "title": "MCMC and HMC",
    "section": "",
    "text": "See libraries used in these notes\nlibrary(tidyverse)\nlibrary(latex2exp)\nlibrary(mvtnorm)\nlibrary(coda)"
  },
  {
    "objectID": "notes/lec16-MCMC-and-HMC.html#ergodic-theorem",
    "href": "notes/lec16-MCMC-and-HMC.html#ergodic-theorem",
    "title": "MCMC and HMC",
    "section": "Ergodic theorem",
    "text": "Ergodic theorem\nUnder what conditions does Metropolis-Hastings MCMC work?\nErgodic theorem: If \\(\\{\\theta^{(1)}, \\theta^{(2)}, \\ldots \\}\\) is an irreducible, aperiodic and recurrent Markov chain, then there is a unique probability distribution \\(\\pi\\) such that as \\(s \\rightarrow \\infty\\),\n\n\\(Pr(\\theta^{(s)} \\in \\mathcal{A}) \\rightarrow \\pi(\\mathcal{A})\\) for any set \\(\\mathcal{A}\\);\n\\(\\frac{1}{S} \\sum g(\\theta^{(s)}) \\rightarrow \\int g(x) \\pi(x) dx\\)."
  },
  {
    "objectID": "notes/lec16-MCMC-and-HMC.html#definitions",
    "href": "notes/lec16-MCMC-and-HMC.html#definitions",
    "title": "MCMC and HMC",
    "section": "Definitions",
    "text": "Definitions\n\nstationary distribution\n\\(\\pi\\) is called the stationary distribution of the Markov chain because if \\(\\theta^{(s)} \\sim \\pi\\) and \\(\\theta^{(s+1)}\\) is generated from the Markov chain starting at \\(\\theta^{(s)}\\), then \\(Pr(\\theta^{(s+1)} \\in \\mathcal{A}) = \\pi(\\mathcal{A})\\).\n\n\nirreducible\nA chain is reducible if the state-space can be divided into non-overlapping sets (due to some \\(J\\)). In practice, the proposal \\(J(\\theta^* | \\theta^{(s)})\\) needs to let us go from any value of \\(\\theta\\) to any other, eventually.\n\n\naperiodic\nWe want our Markov chain to be aperiodic. A value \\(\\theta\\) is said to be periodic with period \\(k>1\\) if it can only be visited every \\(k\\)th iteration. A Markov chain without periodic states is aperiodic.\n\n\nrecurrent\nA value \\(\\theta\\) is recurrent if we are guaranteed to return to it eventually.\n\noffline\nProof that the stationary distribution \\(\\pi(\\theta)\\) is the same as our target distribution \\(p_0(\\theta)\\)."
  },
  {
    "objectID": "notes/lec16-MCMC-and-HMC.html#hamiltonian-monte-carlo",
    "href": "notes/lec16-MCMC-and-HMC.html#hamiltonian-monte-carlo",
    "title": "MCMC and HMC",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\nHamiltonian Monte Carlo (HMC) is a proposal mechanism \\(J(\\theta | \\theta^{(s)})\\), that uses Hamiltonian dynamics to generate proposals that are far away from the current state of the chain with high acceptance probability. These proposals are subsequently accepted or rejected according to the Metropolis-Hastings acceptance ratio.\n\nMotivation: the banana target\n\ntarget distributioncodeplot\n\n\nposterior:\n\\[\np(\\theta | y_1, \\ldots y_n) \\propto \\underbrace{\\prod_{i=1}^n \\text{dnorm}(y_i; \\theta_1 + \\theta_2^2, 1)}_{\\text{likelihood}} \\cdot \\underbrace{\\text{dnorm}(\\theta_1; 0, 1) \\text{dnorm}(\\theta_2; 0, 1)}_{\\text{priors}}\n\\]\n\n\n\nlogPosterior = function(theta) {\n  c = theta[1] + (theta[2] ^ 2)\n  logLikelihood = sum(dnorm(y, mean = c, sd = 1, log = TRUE))\n  logPrior = dnorm(theta[1], 0, 1, log = TRUE) +\n    dnorm(theta[2], 0, 1, log = TRUE)\n    return(logLikelihood + logPrior)\n}\n\n\n# simulated data y\nset.seed(360)\nn = 30\ntheta1 = .75\ntheta2 = .5\ny = rnorm(n, (theta1 + (theta2^2)), 1)\ny\n\n [1]  2.4374945977  1.3225732383  0.7957033706  0.0009050433  0.9624998552\n [6]  0.2485689217  0.3494050797  0.8481528753  0.1619672883  1.5373043843\n[11]  1.9319327323  2.1723549678  0.5916180759  1.5788760946 -0.2521989302\n[16] -0.0956751145  2.1896602700  2.7428271328 -0.8507334992 -0.3434228915\n[21]  0.7158629051  2.9076884521 -0.0258688807  2.7880781640  1.3319085255\n[26]  1.0734242350  1.3910936322  1.8806039555  1.6171004720  1.4077704842\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What about this target distribution could challenge a Metropolis sampler with \\(J(\\theta_i | \\theta_i^{(s)}) = \\text{normal}(\\theta_i, \\delta)\\)?\nLet‚Äôs try it out.\n\nMetropolis samplertrajectoryESSautocorrelationtrace plots\n\n\n\n# sample from posterior\nset.seed(360)\ntheta1 = 0 # starting point\ntheta2 = 0\nTHETA1 = NULL # empty object to save iterations in\nTHETA2 = NULL\nS = 10000 # number of iterations\ndelta = .5 # proposal variance\naccept1 = 0 # keep track of acceptance rate\naccept2 = 0\n\nfor (s in 1:S) {\n  # log everything for numerical stability #\n  \n  ### generate proposal 1 and compute ratio r ###\n  theta1star = rnorm(1, mean = theta1, sd = delta) \n  log.r = logPosterior(c(theta1star, theta2)) - \n    logPosterior(c(theta1, theta2))\n  \n  ### accept or reject proposal and add to chain ###\n  if(log(runif(1)) < log.r)  {\n    theta1 = theta1star\n    accept1 = accept1 + 1 \n  }\n  THETA1 = c(THETA1, theta1)\n  THETA2 = c(THETA2, theta2)\n  \n   ### generate proposal 2 and compute ratio r ###\n  theta2star = rnorm(1, mean = theta2, sd = delta) \n  log.r = logPosterior(c(theta1, theta2star)) - \n    logPosterior(c(theta1, theta2))\n  \n  ### accept or reject proposal and add to chain ###\n  if(log(runif(1)) < log.r)  {\n    theta2 = theta2star\n    accept2 = accept2 + 1 \n  }\n  THETA1 = c(THETA1, theta1)\n  THETA2 = c(THETA2, theta2)\n}\n\n\n\n\n\n\n\n\n\n\n\neffectiveSize(THETA1)\n\n    var1 \n38.28904 \n\neffectiveSize(THETA2)\n\n    var1 \n40.44706 \n\n\n\n\n\npar(mfrow=c(1,2))\nacf(THETA1)\nacf(THETA2)\n\n\n\n\n\n\n\nN = length(THETA1)\ndf = data.frame(theta = c(THETA1, THETA2), \n                theta_id = c(rep(\"theta1\", N), rep(\"theta2\", N)),\n                step = rep(1:N, 2))  \ndf %>%\n  ggplot(aes(x = step, y = theta, col = theta_id)) + \n  geom_line() +\n  theme_bw() +\n  facet_wrap(~ theta_id) +\n  labs(x = \"iteration\", y = \"value\")\n\n\n\n\n\n\n\n\n\nHamiltonian dynamics\nIf we view the state of the Markov chain as the physical location of a particle in parameter space, then what happens if we pretend the laws of physics apply to this physical space? More specifically, let‚Äôs suppose the steps of the Markov chain are akin to a particle moving through Euclidean space and obeying Hamiltonian dynamics. The Hamiltonian of a system specifies its total energy.\nTo be a Hamiltonian system, the particle will have:\n\na location (the position in parameter space)\npotential energy\nkinetic energy\n\nQuestion: we are going to match up the negative log-posterior to either the kinetic energy or the potential energy. Which one do you think makes more sense? Why does the negative sign make sense when we think of what we are trying to do in the context of this as a physical system?\nMathematically, let \\(q\\) be the position of the particle (in parameter space) and let \\(p\\) be the momentum of the particle. So \\(q\\) and \\(p\\) are both vectors of the same dimension (the dimension of parameter space). Then the Hamiltonian, \\(H = U(q) + K(p)\\) where \\(U(q)\\) and \\(K(p)\\) are the potential and kinetic energy respectively. We will let \\(U(q) = - \\log \\pi(q)\\) where \\(\\pi(q)\\) is our target distribution.\nHamilton‚Äôs equations of motion state\n\\[\n\\begin{aligned}\n\\frac{dq_i}{dt} &= \\frac{\\partial{H}}{\\partial p_i}\\\\\n\\frac{dp_i}{dt} &= -\\frac{\\partial{H}}{\\partial q_i}\\\\\n\\end{aligned}\n\\]\nThese equations govern the motion of the particle. They let us map from the state at time \\(t\\) to the state of the system at any future state \\(t + s\\). And it can be shown that \\(\\frac{d}{dt} H = 0\\). In words, energy is conserved.\n\n\n\n\n\n\nImportant\n\n\n\nThe above equations elicit a need to compute \\(-\\frac{\\partial H}{\\partial q_i} = \\frac{\\partial}{\\partial q_i} \\log \\pi(q)\\), i.e.¬†the gradient of the log-posterior.\n\n\nA simple choice of kinetic energy is:\n\\[\nK(p) = \\frac{1}{2} p^T M^{-1}p\n\\]\nwhere \\(M\\) is called the ‚Äúmass matrix‚Äù.\nQuestion: this looks like the log of a kernel you know‚Ä¶ which one?\n\n\nAlgorithm\nFundamentally, HMC is just the Metropolis algorithm with proposals generated via Hamiltonian dynamics. The equations of motion above describe a vector field, and if we integrate them numerically, we can follow the flow through joint space of parameters and momentum.\nLet‚Äôs tackle the banana target from before in an example.\n\ngradientHMC codetrajectoryESSautocorrelationtrace plots\n\n\nWe need the gradient of the log-posterior\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial\\theta_1} \\log \\pi(\\theta_1, \\theta_2 | y_1,\\ldots y_n) &=\n\\frac{\\partial}{\\partial\\theta_1} \\log\\prod_{i=1}^n \\text{dnorm}(y_i; \\theta_1 + \\theta_2^2, 1) \\cdot \\text{dnorm}(\\theta_1; 0, 1) \\text{dnorm}(\\theta_2; 0, 1)\\\\\n&= n \\bar{y} - n \\theta_1 - n\\theta_2^2 - \\theta_1\\\\\n&= n\\bar{y} - n\\theta_2^2 - \\theta_1(n + 1)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial\\theta_2} \\log \\pi(\\theta_1, \\theta_2 | y_1,\\ldots y_n) &=\n\\frac{\\partial}{\\partial\\theta_2} \\log\\prod_{i=1}^n \\text{dnorm}(y_i; \\theta_1 + \\theta_2^2, 1) \\cdot \\text{dnorm}(\\theta_1; 0, 1) \\text{dnorm}(\\theta_2; 0, 1)\\\\\n&= 2n \\bar{y} \\theta_2 - 2 n \\theta_1 \\theta_2 - 2n\\theta_2^3\n\\end{aligned}\n\\]\n\nn = length(y)\nybar = mean(y)\ngradLogPosterior = function(theta) {\n  gradTheta1 = (n*ybar) - (n * (theta[2]^2)) - ((theta[1])*(n+1))\n  gradTheta2 = (2 * n * ybar * theta[2]) - (2 * n * theta[1] * theta[2]) - \n    (2 * n * (theta[2]^3))\n  return(c(gradTheta1, gradTheta2))\n}\n\n\n\nHMC code block below from Neal (2011), see the references\n\nHMC = function (U, grad_U, epsilon, L, current_q) { \n  q = current_q\n  p = rnorm(length(q), 0, 1) # independent standard normal variates\n  current_p = p\n  # Make a half step for momentum at the beginning\n  p = p - epsilon * grad_U(q) / 2\n  # Alternate full steps for position and momentum\n  for (i in 1:L) {\n    # Make a full step for the position\n    q = q + epsilon * p\n    # Make a full step for the momentum, except at end of trajectory\n    if (i != L)\n      p = p - epsilon * grad_U(q)\n  }\n  # Make a half step for momentum at the end.\n  p = p - epsilon * grad_U(q) / 2\n  # Negate momentum at end of trajectory to make the proposal symmetric\n  p = -p\n  # Evaluate potential and kinetic energies at start and end of trajectory\n  current_U = U(current_q)\n  current_K = sum(current_p ^ 2) / 2\n  proposed_U = U(q)\n  proposed_K = sum(p ^ 2) / 2\n  # Accept or reject the state at end of trajectory, returning either\n  # the position at the end of the trajectory or the initial position\n  if (runif(1) < exp(current_U - proposed_U + current_K - proposed_K))\n  {\n    return (q) # accept\n  }\n  else\n  {\n    return (current_q) # reject\n  }\n}\n\n\nset.seed(360)\nS = 10000\nTHETA1 = NULL\nTHETA2 = NULL\ncurrent_q = c(1, 0)\n\nU = function(theta) {\n  return(-1 * logPosterior(theta))\n}\n\ngradU = function(theta) {\n  return(-1 * gradLogPosterior(theta))\n}\n\n\nfor(s in 1:S) {\n  current_q = HMC(U, gradU, epsilon = .05, L = 10, current_q)\n  theta1 = current_q[1]\n  theta2 = current_q[2]\n  THETA1 = c(THETA1, theta1)\n  THETA2 = c(THETA2, theta2)\n}\n\n\n\n\ntrajectoryDF = data.frame(theta1 = THETA1, theta2 = THETA2) %>%\n  head(n = 300)\n\nTHETA %>%\n  ggplot(aes(x = theta1, y = theta2)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  theme_bw() +\n  labs(x = TeX(\"$\\\\theta_1$\"), y = TeX(\"$\\\\theta_2$\"), fill = \"density\",\n       title = \"Trajectory of first 300 steps of HMC\") +\n  geom_path(data = trajectoryDF, color = \"orange\", alpha = 0.6, size=0.5)\n\n\n\n\n\n\n\neffectiveSize(THETA1)\n\n    var1 \n765.6712 \n\neffectiveSize(THETA2)\n\n    var1 \n301.3347 \n\n\n\n\n\npar(mfrow=c(1,2))\nacf(THETA1)\nacf(THETA2)\n\n\n\n\n\n\n\nN = length(THETA1)\ndf = data.frame(theta = c(THETA1, THETA2), \n                theta_id = c(rep(\"theta1\", N), rep(\"theta2\", N)),\n                step = rep(1:N, 2))  \ndf %>%\n  ggplot(aes(x = step, y = theta, col = theta_id)) + \n  geom_line() +\n  theme_bw() +\n  facet_wrap(~ theta_id) +\n  labs(x = \"iteration\", y = \"value\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFurther reading\nGreat it works‚Ä¶ but how do I know this is producing an ergodic Markov chain?\n\nMichael Betancourt‚Äôs conceptual intro\nRadford Neal‚Äôs comprehensive book chapter\nHow is it implemented in stan?"
  },
  {
    "objectID": "quizzes/quiz01.html",
    "href": "quizzes/quiz01.html",
    "title": "Quiz 1",
    "section": "",
    "text": "Exercise 1\nTRUE or FALSE: The beta prior is conjugate to a binomial likelihood.\n\n\nExercise 2\nIn Bayes‚Äô theorem (written below), which term is the ‚Äúnormalizing constant‚Äù?\n\\[\np(\\theta | y) = \\frac{p(y |\\theta) p(\\theta)}{\\int p(y, \\theta) d\\theta}\n\\]\n\n\nExercise 3\n\\[\n\\begin{aligned}\nX &\\sim gamma(k, \\theta)\\\\\np(x) &= \\frac{1}{\\Gamma(k) \\theta^k}x^{k-1}e^{-x/\\theta}\n\\end{aligned}\n\\]\nWrite down the kernel of the density (as a function of \\(x\\)).\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "quizzes/quiz02.html",
    "href": "quizzes/quiz02.html",
    "title": "Quiz 2",
    "section": "",
    "text": "Exercise 1\nTRUE or FALSE: this is a 95% Bayesian confidence interval:\n\\[\np(l(y) < \\theta < u(y) | y) = 0.95\n\\]\n\n\nExercise 2\nWrite ‚Äúequals‚Äù or ‚Äúdoes not equal‚Äù in the blank below:\nIf\n\\[\nY | \\theta \\sim \\text{binomial}(n, \\theta),\n\\]\nthen \\(\\hat{\\theta}_{MLE}\\) ___ \\(\\hat{\\theta}_{MAP}\\) when \\(\\theta \\sim \\text{beta}(1, 1)\\).\n\n\nExercise 3\nTRUE or FALSE: high posterior density regions are always intervals.\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "quizzes/quiz03.html",
    "href": "quizzes/quiz03.html",
    "title": "Quiz 3",
    "section": "",
    "text": "Exercise 1\nWrite ‚Äúposterior‚Äù or ‚Äúprior‚Äù in the blank below:\n\\(\\int p(\\tilde{y}|\\theta) p(\\theta | y_1,\\ldots y_n)d\\theta\\) is a ___ predictive distribution.\n\n\nExercise 2\nExpand \\(p(\\theta | y)\\) using Bayes‚Äô rule (include the normalization constant).\n\n\nExercise 3\nThe vertical line on the following plot is best described as the posterior ____.\n\n\n\n\n\n\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "quizzes/quiz04.html",
    "href": "quizzes/quiz04.html",
    "title": "Quiz 4",
    "section": "",
    "text": "Exercise 1\nTRUE or FALSE: for any valid proposal distribution J, \\(J(\\theta^* | \\theta^{(1)}, \\ldots, \\theta^{(s)}) = J(\\theta^* | \\theta^{(s)})\\).\n\n\nExercise 2\nTRUE or FALSE: the Metropolis algorithm requires symmetric proposal \\(J(\\theta^* | \\theta^{(s)})\\).\n\n\nExercise 3\nSuppose you are sampling \\(\\theta\\) from some target distribution \\(\\pi(\\theta)\\) using the Metropolis algorithm. Write the probability of accepting a new state \\(\\theta^{*}\\) given that the current state of the chain is \\(\\theta^{(s)}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "quizzes/quiz05.html",
    "href": "quizzes/quiz05.html",
    "title": "Quiz 5",
    "section": "",
    "text": "Exercise 1\nLet \\(\\theta\\) be a positive random variable. In the first 100 states of a Markov chain we sample \\(\\theta \\in (0, 5)\\), then for states 100 through S we sample \\(\\theta\\) values \\(\\in\\) \\((5, \\infty)\\) for all S. This Markov chain is not ____. (Choose one of the following to fill in the blank: aperiodic, irreducible, recurrent).\n\n\nExercise 2\nWrite down the two distributions you need to be able to sample from to write a Gibbs sampler for the following target: \\(p(\\beta_1, \\beta_2 | y_1, \\ldots, y_n)\\).\n\n\nExercise 3\nWhich traceplot reveals a Markov chain that hast not reached stationarity?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "quizzes/quiz06.html",
    "href": "quizzes/quiz06.html",
    "title": "Quiz 6",
    "section": "",
    "text": "Exercise 1\nLet \\(\\mathbf{y} = \\left[ {\\begin{array}{cc}  y_1 \\\\  y_2  \\end{array} } \\right]\\), \\(\\boldsymbol{\\theta} = \\left[ {\\begin{array}{cc}  4 \\\\  8  \\end{array} } \\right]\\), \\(\\Sigma = \\left[ {\\begin{array}{cc}  1 & .2 \\\\  .2 & 1.3\\\\  \\end{array} } \\right]\\).\nIf \\(\\mathbf{y} | \\boldsymbol{\\theta}, \\Sigma \\sim MVN(\\boldsymbol{\\theta}, \\Sigma)\\), then \\(y_1 \\sim N(a, b)\\). What is \\(a\\) and \\(b\\)?\n\n\nExercise 2\n\\(\\mathbf{x}\\) is a \\(p \\times 1\\) vector.\nWhat is the dimension of \\(\\mathbf{x}^T \\Sigma \\mathbf{x}\\)?\n\n\nExercise 3\n\\(\\mathbf{x}\\) is a \\(p \\times 1\\) vector.\nWhat is the dimension of \\(Var[\\mathbf{x}]\\)?\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "quizzes/quiz07.html",
    "href": "quizzes/quiz07.html",
    "title": "Quiz 7",
    "section": "",
    "text": "Exercise 1\nTRUE or FALSE: The Metropolis-Hastings algorithm is a generalization of both the Metropolis algorithm and the Gibbs sampler.\n\n\nExercise 2\nTRUE or FALSE: the Metropolis-Hastings algorithm requires symmetric proposal \\(J(\\theta^* | \\theta^{(s)})\\).\n\n\nExercise 3\nTRUE or FALSE: for any valid proposal distribution J, \\(J(\\theta^* | \\theta^{(1)}, \\ldots, \\theta^{(s)}) = J(\\theta^* | \\theta^{(s)})\\).\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "slides/lab-5.html#full-conditionals-are-proportional-to-the-joint",
    "href": "slides/lab-5.html#full-conditionals-are-proportional-to-the-joint",
    "title": "Full conditionals",
    "section": "Full conditionals are proportional to the joint",
    "text": "Full conditionals are proportional to the joint\nSuppose \\(X, Y, Z, \\theta, \\phi\\) are random variables,\n\\[\n\\begin{aligned}\np(x| y, z, \\theta, \\phi) &=\n\\frac{p(x, y, z, \\theta, \\phi)}{\\int p(x,y,z,\\theta, \\phi) dx}\\\\\n&\\propto_x p(x| y, z, \\theta, \\phi)\n\\end{aligned}\n\\]\nSimilarly, each full conditional is proportional to the joint distribution.\n\n\nüîó sta360-fa23.github.io"
  },
  {
    "objectID": "slides/lab0-welcome.html#introductions",
    "href": "slides/lab0-welcome.html#introductions",
    "title": "Welcome to Lab",
    "section": "Introductions",
    "text": "Introductions\n\n\n\n\nMeet the TA!\nIntroduce yourself (icebreaker)\nFollow along these slides on the course website (under slides): sta360-fa24.github.io\nBookmark this! It‚Äôs the course website."
  },
  {
    "objectID": "slides/lab0-welcome.html#what-to-expect-in-labs",
    "href": "slides/lab0-welcome.html#what-to-expect-in-labs",
    "title": "Welcome to Lab",
    "section": "What to expect in labs",
    "text": "What to expect in labs\n\nDiscussion\nPractice problems\nAssistance on computing portion of homeworks"
  },
  {
    "objectID": "slides/lab0-welcome.html#tips",
    "href": "slides/lab0-welcome.html#tips",
    "title": "Welcome to Lab",
    "section": "Tips",
    "text": "Tips\n\nShow up.\nMake use of office hours. Before you need help!"
  },
  {
    "objectID": "slides/lab0-welcome.html#beginnings",
    "href": "slides/lab0-welcome.html#beginnings",
    "title": "Welcome to Lab",
    "section": "Beginnings",
    "text": "Beginnings\nWhile this is not a computing class, computers are the workhorse of Bayesian statistics and we will use R to both enhance understanding of fundamental course material as well as to implement models to learn about real data sets."
  },
  {
    "objectID": "slides/lab0-welcome.html#set-up-rstudio",
    "href": "slides/lab0-welcome.html#set-up-rstudio",
    "title": "Welcome to Lab",
    "section": "Set up RStudio",
    "text": "Set up RStudio\nOption 1 (easiest): RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and login with your Duke NetID and Password.\nClick RStudio to log into the Docker container. You should now see the RStudio environment.\n\nIf you haven‚Äôt previously done so, you will need to reserve a container for RStudio first."
  },
  {
    "objectID": "slides/lab0-welcome.html#set-up-rstudio-1",
    "href": "slides/lab0-welcome.html#set-up-rstudio-1",
    "title": "Welcome to Lab",
    "section": "Set up RStudio",
    "text": "Set up RStudio\nOption 2: RStudio on your computer\n\nDownload R from http://www.r-project.org/.\nDownload RStudio, the popular IDE for R, from https://posit.co/downloads/.\n(optionally) download Quarto from https://quarto.org/docs/get-started/"
  },
  {
    "objectID": "slides/lab0-welcome.html#demo",
    "href": "slides/lab0-welcome.html#demo",
    "title": "Welcome to Lab",
    "section": "Demo",
    "text": "Demo\nNext, check your familarity with R/RStudio fundamentals here. You can also find a link to this from the course schedule under ‚ÄúAssignment‚Äù.\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "slides/lab1.html#example-normal-likelihood",
    "href": "slides/lab1.html#example-normal-likelihood",
    "title": "MLEs and MAPs",
    "section": "Example: normal likelihood",
    "text": "Example: normal likelihood\nLet \\(X\\) be the resting heart rate (RHR) in beats per minute of a student in this class.\nAssume RHR is normally distributed with some mean \\(\\mu\\) and standard deviation \\(8\\).\n\n\\[\n\\textbf{Data-generative model: } X_i \\overset{\\mathrm{iid}}{\\sim} N(\\mu, 64)\n\\]\n\n\nIf we observe three student heart rates, {75, 58, 68} then our likelihood\n\\[L(\\mu) = f_x(75 |\\mu) \\cdot f_x(58|\\mu) \\cdot f_x(68|\\mu).\\]\nThat is, the joint density function of the observed data, viewed as a function of the parameter.\n\n\n\n\n\n\n\n\nImportant\n\n\nThe likelihood itself is not a density function. The integral with respect to the parameter does not need to equal 1."
  },
  {
    "objectID": "slides/lab1.html#visualizing-the-likelihood",
    "href": "slides/lab1.html#visualizing-the-likelihood",
    "title": "MLEs and MAPs",
    "section": "Visualizing the likelihood",
    "text": "Visualizing the likelihood\n\\[L(\\mu) = f_x(75 |\\mu) \\cdot f_x(58|\\mu) \\cdot f_x(68|\\mu).\\]\n\ndatalikelihood functionplotplot code\n\n\n\nx = c(75, 58, 68)\n\n\n\n\nL = function(mu, x) {\n  stopifnot(is.numeric(x))\n  n = length(x)\n  likelihood = 1\n  for(i in 1:n){\n    likelihood = likelihood * dnorm(x[i], mean = mu, sd = 8)\n  }\n  return(likelihood)\n}\n\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  xlim(c(50, 83)) +\n  geom_function(fun = L, args = list(x = x)) +\n  theme_bw() +\n  labs(x = expression(mu), y = \"likelihood\") + \n  geom_vline(xintercept = 67, color = 'red')\n\n\n\n\n\nThe maximum likelihood estimate \\(\\hat{\\mu} = \\frac{75 + 58 + 68}{3} = 67\\).\nThe maximum likelihood estimate is the parameter value that maximizes the likelihood function."
  },
  {
    "objectID": "slides/lab1.html#the-log-likelihood",
    "href": "slides/lab1.html#the-log-likelihood",
    "title": "MLEs and MAPs",
    "section": "The log-likelihood",
    "text": "The log-likelihood\nNotice how small the y-axis is on the previous slide. What happens to the scale of the likelihood as we add additional data points?\n\\[\nL(\\mu) = \\prod_{i = 1}^{n} f_x(x_i |\\mu)\n\\]\n\nSince densities often evaluate between 0 and 1, multiplying many together (as we usually do in likelihoods) can quickly result in floating point underflow. That is, numbers smaller than the computer can actually represent in memory.\n\nNote: sometimes densities evaluate to greater than 1 (e.g.¬†dnorm(0, 0, 0.001)) and multiplying several together can result in overflow.\n\n\n\nlog to the rescue!\n\nlog is a monotonic function, i.e.¬†\\(x > y\\) implies \\(\\log(x) > \\log(y)\\), because of this the maximum of \\(f\\) is the same as the maximum of \\(\\log f\\).\nadditionally, log turns products into sums\n\nin practice, we always work with the log-likelihood,\n\\[\n\\log L(\\mu) = \\sum_{i = 1}^n \\log f_x(x_i | \\mu).\n\\]"
  },
  {
    "objectID": "slides/lab1.html#maximum-likelihood-estimation-mle",
    "href": "slides/lab1.html#maximum-likelihood-estimation-mle",
    "title": "MLEs and MAPs",
    "section": "Maximum likelihood estimation (MLE)",
    "text": "Maximum likelihood estimation (MLE)\nHow did we know to take the average of the values to find the maximum likelihood estimator \\(\\hat{\\mu}\\)?\n\nFrom calculus, we know that to maximize a function, we need to find where the slope equals zero (technically, to ensure we find some maxima and not a minima we need to also check that the second derivative is negative).\nExample: normal likelihood\nFor the normal likelihood example on the previous slide, we can see visually that the function is concave.\nTo find the maximum,\n\\[\n\\begin{aligned}\n\\frac{d}{d\\mu} \\log L(\\mu) &= \\sum_{i}\\frac{d}{d\\mu} \\log f_x(x_i |\\mu)\\\\\n&= \\sum_{i}\\frac{d}{d\\mu} \\left[ -\\frac{1}{2} \\log (2 \\pi \\sigma^2) - \\frac{1}{2\\sigma^2} (x_i - \\mu)^2 \\right]\\\\\n&= \\sum_i \\frac{1}{\\sigma^2} (x_i - \\mu)\n\\end{aligned}\n\\]\nSetting the derivative equal to zero,\n\\[\n\\begin{aligned}\n\\sum_i \\left[ x_i - \\hat{\\mu} \\right] &= 0\\\\\nn \\hat{\\mu} &= \\sum_i x_i\\\\\n\\hat{\\mu} &= \\bar{x}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lab1.html#maximum-a-posteriori-probability-map",
    "href": "slides/lab1.html#maximum-a-posteriori-probability-map",
    "title": "MLEs and MAPs",
    "section": "Maximum a posteriori probability (MAP)",
    "text": "Maximum a posteriori probability (MAP)\nIn Bayesian inference, we wish to find the mode of the posterior, not the likelihood.\nTo find the posterior mode, \\(\\hat{\\theta}\\), we instead take the derivative of the log-posterior,\n\\[\n\\frac{d}{d\\theta} \\log p(\\theta | y) = 0\n\\]\nPractice exercise\nAs in class, let\n\\[\nY | \\theta \\sim \\text{binomial}(n, \\theta)\\\\\n\\theta \\sim \\text{beta}(a, b)\n\\]\n\nFind the closed-form solution for the posterior mode \\(\\hat{\\theta}\\).\nRecreate Figure 1 from class using the same data flips provided below but change the prior to \\(\\theta \\sim \\text{beta}(2, 2)\\).\n\n\nset.seed(3)\nflips = rbinom(5000, size = 1, prob = 0.25)\n\n\nAdd a red vertical line to each subplot that shows the MAP estimate under the prior \\(\\theta \\sim \\text{beta}(2, 2)\\).\n\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "slides/lab2.html#practice-exercise",
    "href": "slides/lab2.html#practice-exercise",
    "title": "Sensitivity to the prior and change of variables",
    "section": "Practice exercise",
    "text": "Practice exercise\nA cancer laboratory is estimating the rate of tumorigenesis in two strains of mice, \\(A\\) and \\(B\\). They have tumor count data for 10 mice in strain \\(A\\) and 13 mice in strain \\(B\\),\n\nyA = c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)\nyB = c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)\n\nAssume\n\\[\n\\begin{aligned}\nY_A &\\sim \\text{Poisson}(\\theta_A)\\\\\nY_B &\\sim \\text{Poisson}(\\theta_B).\n\\end{aligned}\n\\]\n\nExercise 1Exercise 2\n\n\nLet\n\\[\n\\begin{aligned}\n\\theta_A &\\sim \\text{gamma}(120, 10)\\\\\n\\theta_B &\\sim \\text{gamma}(12, 1).\n\\end{aligned}\n\\]\n\nCompute \\(p(\\theta_B < \\theta_A ~|~ \\vec{y}_A, \\vec{y}_B)\\) via Monte Carlo sampling.\n\n\n\nLet\n\\[\n\\begin{aligned}\n\\theta_A &\\sim \\text{gamma}(120, 10)\\\\\n\\theta_B &\\sim \\text{gamma}(12\\cdot n_0, n_0).\n\\end{aligned}\n\\]\n\nFor a range of values of \\(n_0\\), obtain \\(p(\\theta_B < \\theta_A ~|~ \\vec{y}_A, \\vec{y}_B)\\).\nDescribe how sensitive conclusions about the event \\(\\{ \\theta_B < \\theta_A\\}\\) are to the prior distribution on \\(\\theta_B\\)."
  },
  {
    "objectID": "slides/lab2.html#practice-exercise-1",
    "href": "slides/lab2.html#practice-exercise-1",
    "title": "Sensitivity to the prior and change of variables",
    "section": "Practice exercise",
    "text": "Practice exercise\n\nLet \\(X \\sim \\text{Unif}(5, 10)\\)\nLet \\(Y = X^2\\)\n\nNotice that even though \\(X^2\\) is not a monotonic function everywhere, it is a monotonic function over the support of X.\nExercise: use the change of variables formula to derive \\(p(y)\\). Confirm with Monte Carlo simulation.\n\n\nShow solution\nlibrary(tidyverse)\n\nx = runif(100000, 5, 10)\ny = x^2\n\ndf = data.frame(y)\n\nf = function(y) {\n  return(.1/sqrt(y))\n}\n\ndf %>%\n  ggplot(aes(x = y)) + \n  stat_function(fun = f) +\n  geom_histogram(aes(x = y, y = ..density..),\n                 fill = 'steelblue', alpha = 0.5)\n\n\n\n\nüîó sta360-fa23.github.io"
  },
  {
    "objectID": "slides/lab3-review.html#exercise",
    "href": "slides/lab3-review.html#exercise",
    "title": "Exam practice",
    "section": "Exercise",
    "text": "Exercise\nPhysicists studying a radioactive substance measure the times at which the substance emits a particle. They will record \\(n+1\\) emissions and set \\(Y_1\\) to be the time elapsed between the first and second emission, \\(Y_2\\) to be the time elapsed between the second and third emission and so on. They will model the data as \\(Y_1, \\ldots Y_n | \\theta \\sim \\text{exponential}(\\theta)\\). The pdf of the exponential(\\(\\theta\\)) distribution is\n\\[\np(y |\\theta) = \\theta e^{-\\theta y} \\ \\text{ for } \\ y>0, \\ \\theta>0.\n\\]\nFor this distribution, \\(E[Y|\\theta] = \\frac{1}{\\theta}\\).\n(a). Write out the corresponding joint density \\(p(y_1, \\ldots, y_n | \\theta)\\) and simplify as much as possible. Justify each step of your calculation.\n(b). Compute the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\), i.e.¬†the value \\(\\hat{\\theta}_{MLE}\\) that maximizes \\(p(y_1,\\ldots y_n | \\theta)\\). Hint: it‚Äôs easier to work with the log-likelihood.\n(c). Choose a prior \\(p(\\theta)\\) that is conjugate to the likelihood. Hint: look at kernels of densities on the distribution sheet. Write out the formula for \\(p(\\theta | y_1, \\ldots y_n)\\), up to a proportionality in \\(\\theta\\), and simplify as much as possible. From this, identify explicitly the posterior distribution of \\(\\theta\\) (i.e., write ‚Äúthe posterior is a blank distribution with parameter(s) blank)‚Äù.\n(d). Obtain the formula for \\(E[\\theta, y_1, \\ldots y_n]\\) as a function of \\(a, b, n\\) and \\(y_1, \\ldots y_n\\), and try to write this as a function of the estimator \\(\\hat{\\theta}\\) you found in part (b). What does \\(E[\\theta | y_1,\\ldots,y_n]\\) get close to as \\(n\\) increases?\n(e). Report a 95% posterior confidence interval for \\(\\theta\\).\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "slides/lab4.html#exercise",
    "href": "slides/lab4.html#exercise",
    "title": "Extra practice",
    "section": "Exercise",
    "text": "Exercise\nA data scientist at a small subscriber-based tech company models the number of new subscribers in a day as \\(Y|\\theta \\sim \\text{Poisson}(\\theta)\\) with prior \\(\\theta \\sim \\text{gamma}(a,b)\\). A priori, the data scientist believes that there are on average 20 signups per day and 90% of the time there are between approximately 3 and 50 signups on a given day.\na\nFind suitable \\(a\\) and \\(b\\) that satisfy the data scientist‚Äôs prior beliefs.\nVerify how well your prior aligns with this belief using Monte Carlo sampling to generate the prior predictive distribution, \\(p(\\tilde{y}) = \\int p(\\tilde{y}, \\theta)d\\theta\\).\n\n\n\nb\nAfter one month the data scientist observes the following daily subscriber counts:\n\ny = c(10, 21, 19, 16, 20, 18, 35, 16, 23, 26, 20, 21, 23, 19, 18, 20, 23, 18, 21, 16, 15, 15, 20, 22, 19, 25)\n\nThe data scientist is fundamentally interested in the variance of subscriber counts per day. Is the Poisson model appropriate for this data?\nReport \\(p(\\tilde{S}^2 > s^2_{obs} | y_1,\\ldots y_n)\\) where \\(\\tilde{S}^2\\) is the posterior predictive sample variance and \\(s^2_{obs}\\) is the observed sample variance (\\(s^2_{obs} = 21.3\\)). To generate samples under the posterior predictive distribution, use the prior from part (a)."
  },
  {
    "objectID": "slides/lab4.html#exercise-1",
    "href": "slides/lab4.html#exercise-1",
    "title": "Extra practice",
    "section": "Exercise",
    "text": "Exercise\nLet \\(Y_1,\\ldots Y_n\\) be iid random variables with expectation \\(\\theta\\) and variance \\(\\sigma^2\\).\nShow that \\(\\frac{1}{n} \\sum_{i = 1}^n (Y_i -\\bar{Y})^2\\) is a biased estimator of \\(\\sigma^2\\).\n\n\nüîó sta360-fa23.github.io"
  },
  {
    "objectID": "slides/lab6-mcmc-d-practice.html#exercise-1",
    "href": "slides/lab6-mcmc-d-practice.html#exercise-1",
    "title": "MCMC diagnostics practice",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet \\(p(\\theta_1, \\theta_2 | \\mathbf{y})\\) be our target distribution, i.e.¬†the distribution we are interested in sampling.\nWe construct a Gibbs sampler and look at the trace plots of \\(\\theta_1\\) and \\(\\theta_2\\), produced below. Chat with your neighbor, describe what you observe. Has the chain reached stationarity for each parameter? How well is the sampler mixing? Do you think the parameters are correlated or uncorrelated? Why or why not?"
  },
  {
    "objectID": "slides/lab6-mcmc-d-practice.html#exercise-2",
    "href": "slides/lab6-mcmc-d-practice.html#exercise-2",
    "title": "MCMC diagnostics practice",
    "section": "Exercise 2",
    "text": "Exercise 2\nBased on the first 1000 iterations of your Gibbs sampler shown on the previous slide, which of the following joint densities is the most plausible for \\(\\theta_1, \\theta_2 | \\mathbf{y}\\)? Why? Hint: it may help to think about where your sampler starts and imagine a particle moving through space according to conditional updates."
  },
  {
    "objectID": "slides/lab7-exam2-prep.html#exercise-1",
    "href": "slides/lab7-exam2-prep.html#exercise-1",
    "title": "Practice",
    "section": "Exercise 1",
    "text": "Exercise 1\n\\[\n\\begin{aligned}\nY_1, \\ldots, Y_n &\\sim \\text{ i.i.d. binary}(\\theta)\\\\\n\\theta &\\sim \\text{beta}(a, b)\n\\end{aligned}\n\\]\n\nCompute \\(\\hat{\\theta}_{MLE}\\)\nCompute \\(\\hat{\\theta}_{B} = E[\\theta | y_1,\\ldots y_n]\\).\nCompare \\(MSE(\\hat{\\theta}_{MLE})\\) to \\(MSE(\\hat{\\theta}_{B})\\)). Under what conditions is the MSE of \\(\\hat{\\theta}_B\\) smaller?"
  },
  {
    "objectID": "slides/lab7-exam2-prep.html#exercise-2",
    "href": "slides/lab7-exam2-prep.html#exercise-2",
    "title": "Practice",
    "section": "Exercise 2",
    "text": "Exercise 2\nConsider a single observation \\((y_1, y_2)\\) drawn from a bivariate normal distribution with mean \\((\\theta_1, \\theta_2)\\) and fixed, known \\(2 \\times 2\\) covariance matrix \\(\\Sigma = \\left[ {\\begin{array}{cc}  1 & .5 \\\\  .5 & 1  \\end{array} } \\right]\\). Consider a uniform prior on \\(\\theta = (\\theta_1, \\theta_2)\\) : \\(p(\\theta_1, \\theta_2) \\propto 1\\).\n(a.) Derive the joint posterior for \\(\\theta_1, \\theta_2 | y_1, y_2, \\Sigma\\). Describe a direct sampler for this distribution.\n(b.) Write down full conditionals \\(p(\\theta_1 | \\theta_2, y_1, y_2, \\Sigma)\\) and \\(p(\\theta_2 | \\theta_1, y_1, y_2, \\Sigma)\\). Write pseudo-code to describe a Gibbs sampling procedure. Hint: you can use the result from HW6 Ex 3.\n(c.) Will the direct sampler from part (a) or the Gibbs sampler in part (b) have higher ESS? Why?\n\n\nüîó sta360-fa23.github.io"
  },
  {
    "objectID": "slides/lab8-rstan.html#download",
    "href": "slides/lab8-rstan.html#download",
    "title": "Easy Bayesian linear modeling",
    "section": "Download",
    "text": "Download\nTo download rstanarm and bayesplot run the code below\n\ninstall.packages(\"rstanarm\", \"bayesplot\")\n\nTo load the packages, run\n\nlibrary(rstanarm)\nlibrary(bayesplot)"
  },
  {
    "objectID": "slides/lab8-rstan.html#overview",
    "href": "slides/lab8-rstan.html#overview",
    "title": "Easy Bayesian linear modeling",
    "section": "Overview",
    "text": "Overview\n\nrstanarm contains a host of functions to make Bayesian linear modeling in R easy. See https://mc-stan.org/rstanarm/articles/ for a variety of tutorials.\n\npros: easy to test Bayesian linear models, can be fast (uses Hamiltonian Monte Carlo proposals)\ncons: limited in scope, e.g.¬†requires differentiable objective and small model adjustments can be cumbersome to implement, e.g.¬†placing a prior on variance versus standard deviation of normal model.\n\nbayesplot contains many useful plotting wrappers that work out of the box with objects created by rstanarm in an intuitive way."
  },
  {
    "objectID": "slides/lab8-rstan.html#example",
    "href": "slides/lab8-rstan.html#example",
    "title": "Easy Bayesian linear modeling",
    "section": "Example",
    "text": "Example\n\nLoad dataGlimpse data\n\n\n\nlibrary(tidyverse)\nspam = read_csv(\n  \"https://sta360-fa24.github.io/data/spam.csv\")\n\n\n\n\nglimpse(spam)\n\nRows: 4,601\nColumns: 58\n$ make              <dbl> 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15‚Ä¶\n$ address           <dbl> 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ all               <dbl> 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0.00, 0.46‚Ä¶\n$ num3d             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ our               <dbl> 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1.88, 0.61‚Ä¶\n$ over              <dbl> 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ remove            <dbl> 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0.00, 0.30‚Ä¶\n$ internet          <dbl> 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1.88, 0.00‚Ä¶\n$ order             <dbl> 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0.00, 0.92‚Ä¶\n$ mail              <dbl> 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0.00, 0.76‚Ä¶\n$ receive           <dbl> 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0.00, 0.76‚Ä¶\n$ will              <dbl> 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0.00, 0.92‚Ä¶\n$ people            <dbl> 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ report            <dbl> 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ addresses         <dbl> 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ free              <dbl> 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0.00, 0.00‚Ä¶\n$ business          <dbl> 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ email             <dbl> 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0.00, 0.15‚Ä¶\n$ you               <dbl> 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0.00, 1.23‚Ä¶\n$ credit            <dbl> 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0.00, 3.53‚Ä¶\n$ your              <dbl> 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0.00, 2.00‚Ä¶\n$ font              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ num000            <dbl> 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ money             <dbl> 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15‚Ä¶\n$ hp                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ hpl               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ george            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ num650            <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ lab               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ labs              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ telnet            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ num857            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ data              <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15‚Ä¶\n$ num415            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ num85             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ technology        <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ num1999           <dbl> 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ parts             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ pm                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ direct            <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ cs                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ meeting           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ original          <dbl> 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0.00, 0.30‚Ä¶\n$ project           <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ re                <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ edu               <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00‚Ä¶\n$ table             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ conference        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ charSemicolon     <dbl> 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0‚Ä¶\n$ charRoundbracket  <dbl> 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0.054, 0.2‚Ä¶\n$ charSquarebracket <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.0‚Ä¶\n$ charExclamation   <dbl> 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0.164, 0.0‚Ä¶\n$ charDollar        <dbl> 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0.054, 0.0‚Ä¶\n$ charHash          <dbl> 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0‚Ä¶\n$ capitalAve        <dbl> 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1.671, 2.4‚Ä¶\n$ capitalLong       <dbl> 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6, 11, 61,‚Ä¶\n$ capitalTotal      <dbl> 278, 1028, 2259, 191, 191, 54, 112, 49, 1257, 749, 2‚Ä¶\n$ type              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n\n\n\n\n\nDescription\n4601 emails sent to the inbox of someone named ‚ÄúGeorge‚Äù that are classified as type = 1 (spam) or 0 (non-spam). The data was collected at Hewlett-Packard labs and contains 58 variables. The first 48 variables are specific keywords and each observation is the percentage of appearance (frequency) of that word in the message. Click here to read more."
  },
  {
    "objectID": "slides/lab8-rstan.html#fitting-a-normal-linear-model",
    "href": "slides/lab8-rstan.html#fitting-a-normal-linear-model",
    "title": "Easy Bayesian linear modeling",
    "section": "Fitting a normal linear model",
    "text": "Fitting a normal linear model\nLet‚Äôs consider the model below:\n\\[\ny_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon_i\n\\]\nwhere \\(\\epsilon_i \\sim \\text{ i.i.d.} N(0, \\sigma^2)\\). We complete model specification with the priors:\n\\[\n\\begin{aligned}\n\\beta_0 &\\sim N(0, 100)\\\\\n\\beta_i &\\sim \\text{ i.i.d } N(0, 1) \\text{ for all } i > 0\\\\\n\\sigma &\\sim \\text{uniform}(0, \\infty)\n\\end{aligned}\n\\]\nLet‚Äôs look at building this model using the stan_glm function of rstanarm.\n\n\n\n\n\n\nNote\n\n\nWe‚Äôll always want to access the object we create, so you should save the result, e.g.¬†model1 below.\n\n\n\n\n# save the result as \"model1\"\nmodel1 = stan_glm(mercury ~ ., data = bass, # remove the intercept\n                 family = gaussian(link = \"identity\"),\n                 seed = 360, # sets a random starting seed\n                 prior_intercept = normal(0, 100), # sets the intercept prior\n                 prior = normal(0, 1), # sets the beta prior\n                 prior_aux = NULL, # set a flat prior on sigma\n                 )"
  },
  {
    "objectID": "slides/lab8-rstan.html#examining-the-output",
    "href": "slides/lab8-rstan.html#examining-the-output",
    "title": "Easy Bayesian linear modeling",
    "section": "Examining the output",
    "text": "Examining the output\n\nDid stan_glm do what we think it did? Did the Markov chain converge? Which parameters, if any, have a posterior mean rounded to 0?\n\n\nquick lookcheck priorstrace plotsmarginal posteriorsplotting tipsget chainsummarize\n\n\nNotice sample: 1000 since half get thrown away by stan and is called ‚Äúburn-in‚Äù i.e.¬†a period that the chain spends reaching the target distribution gets discarded.\n\nsummary(fit1)\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      type ~ .\n algorithm:    sampling\n sample:       1000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 3680\n predictors:   58\n\nEstimates:\n                    mean   sd    10%   50%   90%\n(Intercept)        -3.9    0.5  -4.5  -3.9  -3.4\nmake               -0.1    0.1  -0.2  -0.1   0.0\naddress            -0.2    0.1  -0.4  -0.2  -0.1\nall                 0.1    0.1   0.0   0.1   0.1\nnum3d               0.9    0.7   0.2   0.7   1.7\nour                 0.4    0.1   0.3   0.4   0.5\nover                0.2    0.1   0.1   0.2   0.3\nremove              0.9    0.1   0.7   0.9   1.1\ninternet            0.2    0.1   0.1   0.2   0.3\norder               0.1    0.1   0.0   0.1   0.2\nmail                0.1    0.0   0.0   0.1   0.1\nreceive             0.0    0.1  -0.1   0.0   0.0\nwill               -0.2    0.1  -0.3  -0.2  -0.1\npeople             -0.1    0.1  -0.1   0.0   0.0\nreport              0.0    0.1  -0.1   0.0   0.1\naddresses           0.3    0.2   0.1   0.3   0.5\nfree                1.0    0.1   0.8   1.0   1.1\nbusiness            0.5    0.1   0.3   0.5   0.6\nemail               0.1    0.1   0.0   0.1   0.2\nyou                 0.1    0.1   0.0   0.1   0.2\ncredit              0.6    0.2   0.3   0.6   0.9\nyour                0.3    0.1   0.2   0.3   0.4\nfont                0.2    0.2   0.1   0.2   0.4\nnum000              0.7    0.2   0.5   0.7   0.9\nmoney               0.2    0.1   0.1   0.2   0.3\nhp                 -3.1    0.5  -3.8  -3.1  -2.5\nhpl                -1.0    0.4  -1.5  -1.0  -0.5\ngeorge             -8.5    1.8 -10.9  -8.5  -6.3\nnum650              0.2    0.1   0.1   0.2   0.4\nlab                -1.1    0.5  -1.8  -1.0  -0.5\nlabs               -0.1    0.1  -0.3  -0.1   0.0\ntelnet             -0.3    0.3  -0.8  -0.3   0.0\nnum857             -0.2    0.6  -1.0  -0.2   0.4\ndata               -0.7    0.2  -1.0  -0.7  -0.5\nnum415             -0.6    0.8  -1.7  -0.5   0.2\nnum85              -0.8    0.4  -1.3  -0.8  -0.3\ntechnology          0.4    0.1   0.2   0.4   0.6\nnum1999             0.0    0.1  -0.1   0.0   0.1\nparts              -0.2    0.1  -0.4  -0.2  -0.1\npm                 -0.4    0.2  -0.6  -0.4  -0.2\ndirect             -0.2    0.1  -0.3  -0.1   0.0\ncs                 -1.4    0.8  -2.4  -1.2  -0.5\nmeeting            -1.5    0.5  -2.2  -1.5  -1.0\noriginal           -0.4    0.2  -0.8  -0.4  -0.1\nproject            -1.3    0.4  -1.8  -1.3  -0.8\nre                 -0.7    0.2  -0.9  -0.7  -0.5\nedu                -1.5    0.3  -1.9  -1.5  -1.1\ntable              -0.3    0.1  -0.5  -0.3  -0.1\nconference         -1.1    0.4  -1.7  -1.1  -0.6\ncharSemicolon      -0.3    0.1  -0.5  -0.3  -0.2\ncharRoundbracket   -0.1    0.1  -0.2  -0.1   0.0\ncharSquarebracket  -0.1    0.1  -0.3  -0.1   0.0\ncharExclamation     0.2    0.1   0.2   0.2   0.3\ncharDollar          1.2    0.2   1.0   1.3   1.5\ncharHash            0.7    0.4   0.2   0.7   1.2\ncapitalAve          0.0    0.3  -0.3   0.0   0.4\ncapitalLong         0.9    0.4   0.4   0.8   1.3\ncapitalTotal        0.7    0.1   0.5   0.7   0.8\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.4   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                  mcse Rhat n_eff\n(Intercept)       0.0  1.0   740 \nmake              0.0  1.0  1161 \naddress           0.0  1.0  1156 \nall               0.0  1.0   907 \nnum3d             0.0  1.0   958 \nour               0.0  1.0   973 \nover              0.0  1.0  1022 \nremove            0.0  1.0   909 \ninternet          0.0  1.0  1188 \norder             0.0  1.0  1064 \nmail              0.0  1.0  1022 \nreceive           0.0  1.0  1171 \nwill              0.0  1.0  1022 \npeople            0.0  1.0  1307 \nreport            0.0  1.0   935 \naddresses         0.0  1.0  1115 \nfree              0.0  1.0   830 \nbusiness          0.0  1.0  1059 \nemail             0.0  1.0   895 \nyou               0.0  1.0  1026 \ncredit            0.0  1.0  1328 \nyour              0.0  1.0  1077 \nfont              0.0  1.0  1226 \nnum000            0.0  1.0  1124 \nmoney             0.0  1.0   910 \nhp                0.0  1.0  1161 \nhpl               0.0  1.0  1422 \ngeorge            0.1  1.0   779 \nnum650            0.0  1.0  1017 \nlab               0.0  1.0  1477 \nlabs              0.0  1.0  1317 \ntelnet            0.0  1.0  1002 \nnum857            0.0  1.0  1388 \ndata              0.0  1.0  1265 \nnum415            0.0  1.0  1427 \nnum85             0.0  1.0  1247 \ntechnology        0.0  1.0  1021 \nnum1999           0.0  1.0   834 \nparts             0.0  1.0  1227 \npm                0.0  1.0  1135 \ndirect            0.0  1.0   793 \ncs                0.0  1.0  1293 \nmeeting           0.0  1.0  1547 \noriginal          0.0  1.0  1805 \nproject           0.0  1.0  1432 \nre                0.0  1.0  1027 \nedu               0.0  1.0  1048 \ntable             0.0  1.0  1082 \nconference        0.0  1.0  1492 \ncharSemicolon     0.0  1.0  1308 \ncharRoundbracket  0.0  1.0   906 \ncharSquarebracket 0.0  1.0  1286 \ncharExclamation   0.0  1.0  1070 \ncharDollar        0.0  1.0  1147 \ncharHash          0.0  1.0  1111 \ncapitalAve        0.0  1.0  1165 \ncapitalLong       0.0  1.0  1421 \ncapitalTotal      0.0  1.0  1312 \nmean_PPD          0.0  1.0   909 \nlog-posterior     0.5  1.0   338 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\n\n\nprior_summary(fit1)\n\nPriors for model 'fit1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 10)\n\nCoefficients\n ~ laplace(location = [0,0,0,...], scale = [0.5,0.5,0.5,...])\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\n\n\nbetaNames = names(spam_train)[2:7]\nbetaNames\n\n[1] \"make\"    \"address\" \"all\"     \"num3d\"   \"our\"     \"over\"   \n\nmcmc_trace(fit1, pars = betaNames)\n\n\n\n\n\n\n\nbetaNames = names(spam_train)[2:7]\nbetaNames\n\n[1] \"make\"    \"address\" \"all\"     \"num3d\"   \"our\"     \"over\"   \n\nmcmc_hist(fit1, pars = c(betaNames))\n\n\n\n\n\n\nTo plot specific parameters, use the arguemnt pars, e.g.\n\nmcmc_trace(fit1, pars = c(\"internet\", \"george\")\nmcmc_hist(fit1, pars = \"make\")\n\nTo read more about bayesplot functionality, see https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchain_draws = as_draws(fit1)\nchain_draws$george[1:5] # first 5 samples of the first chain run by stan\n\n[1]  -8.657706  -8.058416  -7.542682  -5.875524 -10.805427\n\n\n\ntry the following command: View(chain_draws)\n\n\n\nReport posterior mean, posterior median and 90% posterior CI.\n\nposteriorMean = apply(chain_draws, 2, mean)\nposteriorMedian = fit1$coefficients\nposteriorCI = posterior_interval(fit1, prob = 0.9)\ncbind(posteriorMean, posteriorMedian, posteriorCI)\n\n                  posteriorMean posteriorMedian            5%          95%\n(Intercept)         -3.92819676     -3.89842329  -4.688486640 -3.214996079\nmake                -0.10511093     -0.10291579  -0.234925630  0.007473911\naddress             -0.23994990     -0.22496984  -0.459838123 -0.079085239\nall                  0.06181680      0.06204777  -0.032348802  0.162371165\nnum3d                0.86239227      0.69068831   0.094477008  2.262920466\nour                  0.36326097      0.36517847   0.252660614  0.482222431\nover                 0.22388788      0.22526813   0.104903413  0.345376589\nremove               0.89046939      0.88519560   0.660560624  1.124715173\ninternet             0.19219955      0.18873972   0.084482068  0.313373050\norder                0.14088547      0.14175852   0.022540540  0.262790934\nmail                 0.07091792      0.07049483  -0.004812217  0.148380741\nreceive             -0.04641076     -0.04704090  -0.155648064  0.056349226\nwill                -0.19669579     -0.19506749  -0.323302246 -0.079179801\npeople              -0.05080340     -0.04939546  -0.170276558  0.067469609\nreport              -0.01152332     -0.01105396  -0.102076144  0.078218538\naddresses            0.27381609      0.26410052   0.016663808  0.554382803\nfree                 0.95909363      0.95496752   0.738679361  1.184988932\nbusiness             0.46684266      0.46497563   0.276894271  0.663670058\nemail                0.10488455      0.10024984   0.001909845  0.222419640\nyou                  0.10146793      0.10210904  -0.005066755  0.212057793\ncredit               0.60260737      0.58770051   0.245466733  1.011651069\nyour                 0.29510195      0.29496631   0.185160138  0.408134909\nfont                 0.23712284      0.22816521   0.014405002  0.514370136\nnum000               0.71052738      0.69647322   0.470113449  0.973663603\nmoney                0.20621767      0.19864778   0.101091758  0.340661983\nhp                  -3.12311740     -3.10880324  -4.016671505 -2.304686191\nhpl                 -1.03485238     -1.03298227  -1.697441864 -0.401214421\ngeorge              -8.54020532     -8.47338329 -11.651960267 -5.719147532\nnum650               0.23079106      0.22722517   0.066179720  0.417323354\nlab                 -1.06217022     -0.98480464  -2.007243002 -0.334531315\nlabs                -0.12552567     -0.11162875  -0.398538494  0.091786582\ntelnet              -0.34336793     -0.26439840  -0.971412117  0.026123689\nnum857              -0.24559769     -0.15437261  -1.410789083  0.608907729\ndata                -0.73133589     -0.71699338  -1.108487928 -0.382954624\nnum415              -0.64101304     -0.47534569  -2.093090996  0.285482781\nnum85               -0.79569146     -0.76847847  -1.439452875 -0.215331103\ntechnology           0.39271940      0.38676108   0.173703662  0.625821033\nnum1999             -0.01626319     -0.01501346  -0.146351106  0.113311375\nparts               -0.19387536     -0.17594695  -0.420298806 -0.034525634\npm                  -0.36307678     -0.35286882  -0.654982493 -0.099008434\ndirect              -0.16079193     -0.14842066  -0.405108058  0.023707001\ncs                  -1.35347826     -1.21771081  -2.748096259 -0.377142268\nmeeting             -1.54394154     -1.48634297  -2.451147222 -0.853955898\noriginal            -0.44492327     -0.43068267  -0.889558420 -0.068304526\nproject             -1.27091449     -1.25700373  -1.901896255 -0.691331691\nre                  -0.68897480     -0.68667471  -0.941480034 -0.439694059\nedu                 -1.52342346     -1.51146322  -2.056088445 -1.041884151\ntable               -0.27888780     -0.25736694  -0.549891396 -0.077155657\nconference          -1.10733935     -1.06265166  -1.864536810 -0.460671052\ncharSemicolon       -0.32588657     -0.31416332  -0.514101364 -0.166688438\ncharRoundbracket    -0.08698799     -0.08292353  -0.212643020  0.023441788\ncharSquarebracket   -0.14980893     -0.13280261  -0.342135625 -0.004178006\ncharExclamation      0.23238206      0.22752584   0.137507390  0.351658871\ncharDollar           1.24928427      1.25351495   0.944702156  1.558125382\ncharHash             0.71795213      0.70213467   0.122922001  1.368832748\ncapitalAve          -0.01033156     -0.03262611  -0.401092859  0.470713740\ncapitalLong          0.85197631      0.82214223   0.303300262  1.493571408\ncapitalTotal         0.67337732      0.67245181   0.471182694  0.893198052"
  },
  {
    "objectID": "slides/lab8-rstan.html#exercise-bayesian-logistic-regression",
    "href": "slides/lab8-rstan.html#exercise-bayesian-logistic-regression",
    "title": "Easy Bayesian linear modeling",
    "section": "Exercise: Bayesian logistic regression",
    "text": "Exercise: Bayesian logistic regression\nUsing the bass data set, see how well you can predict which river a bass came from, given only its length and mercury level. In other words,\n\\[\np(y_i = 1) = \\frac{1}{1 + \\exp [- (\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2)]}\n\\]\nUse the priors\n\\[\n\\beta_i \\sim N(0, 4) \\text{ for } i \\in (0, 1, 2)\n\\]\nA template for fitting logistic regression using rstanarm can be found at https://mc-stan.org/rstanarm/articles/binomial.html.\n\nDoes your chain converge?\nReport \\(E[\\beta_i | y]\\) and 80% posterior CI for each \\(\\beta\\), i.e.¬†\\(i \\in (0, 1, 2)\\)\nIs length or mercury a more important predictor of which river the bass came from? Why?\n\n\n\n\n\n\n\n\n\n\n\n\nüîó sta360-fa23.github.io"
  },
  {
    "objectID": "slides/lab9-MH-MCMC.html#exercise",
    "href": "slides/lab9-MH-MCMC.html#exercise",
    "title": "MCMC Practice",
    "section": "Exercise",
    "text": "Exercise\nSuppose the target distribution we wish to sample from is given by probability mass function\n\\[\n\\pi(\\theta) = \\theta / w \\text{ for } \\theta \\in \\{1, 2, \\ldots 6\\}\n\\]\nin words, we wish to roll a die with probability \\(1/w\\) of landing on face 1, \\(2/w\\) of landing on face 2, etc.\n\nWrite a Metropolis algorithm to approximate the target distribution using a proposal \\(J(\\theta = j | \\theta^{(s)} = i) = 1/6\\) for all \\(j\\), i.e.¬†propose a new state \\(j\\) uniformly. Run your Markov chain for \\(S=10000\\) states.\nThe Metropolis algorithm requires a symmetric proposal \\(J\\). Explain why this proposal is symmetric.\nPlot a histogram of the Markov chain samples. Does the plot match your intuition?\nCompare the estimated probabilities of each outcome to the truth (compute \\(w\\))."
  },
  {
    "objectID": "slides/lab9-MH-MCMC.html#exercise-1",
    "href": "slides/lab9-MH-MCMC.html#exercise-1",
    "title": "MCMC Practice",
    "section": "Exercise",
    "text": "Exercise\nMetropolis-Hastings lets us work with non-symmetric proposals. Re-write the algorithm of the previous exercise using the non-symmetric proposal \\(J(\\theta = j | \\theta^{(s)} = i)\\) such that\n\\[\n\\theta  = \\begin{cases}\n1 & \\text{ with prob } & 0.05\\\\\n2 & \\text{ with prob } & 0.15\\\\\n3 & \\text{ with prob } & 0.2\\\\\n4 & \\text{ with prob } & 0.15\\\\\n5 & \\text{ with prob } & 0.15\\\\\n6 & \\text{ with prob } & 0.3\\\\\n\\end{cases}\n\\]\n\ncompare your results to that those of the previous exercise. In particular, compare the ESS of \\(\\theta\\) in each chain. Which do you prefer? How might you explain this difference in ESS?\n\n\n\n\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "solutions/hw01s.html",
    "href": "solutions/hw01s.html",
    "title": "Homework 1 solutions",
    "section": "",
    "text": "By definition of covariance,\n\\[\nCov(X_i, X_j) = \\mathbb{E}~X_i X_j - \\mathbb{E}~X_i ~\\mathbb{E}~X_j\n\\]\nBy the law of total expectation,\n\\[\n= \\mathbb{E}~[\\mathbb{E}~X_i X_j| \\theta] -\n\\mathbb{E}~[\\mathbb{E}~X_i | \\theta]\n\\mathbb{E}~[\\mathbb{E}~X_j | \\theta]\n\\]\nBy de Finetti‚Äôs theorem, exchangeability of \\(X_i\\) and \\(X_j\\) implies the two variables are conditionally iid relative to \\(\\theta\\). Therefore,\n\\[\n\\begin{aligned}\n\\mathbb{E}~[\\mathbb{E} ~ X_i X_j| \\theta] &= \\mathbb{E}~\n\\left[\n\\int \\int x_i~x_j~p(x_i, x_j |\\theta) dx_i~dx_j\n\\right]\\\\\n&=\\mathbb{E}~\n\\left[\n\\int x_i~p(x_i |\\theta) dx_i \\cdot \\int x_j~p(x_j |\\theta) dx_j\n\\right]\\\\\n&=\n\\mathbb{E}\n\\left[\n\\mathbb{E}~ X_i | \\theta \\cdot\n\\mathbb{E}~ X_j | \\theta\n\\right]\n\\end{aligned}\n\\]\nSimilarly,\n\\[\n\\mathbb{E}~\nX_i | \\theta\n=\n\\mathbb{E}\nX_j | \\theta\n\\]\nso that in total,\n\\[\nCov(X_i, X_j) =\n\\mathbb{E}~\n\\left[\n\\left(\n\\mathbb{E}~X_i | \\theta\n\\right)^2\n\\right]\n-\n\\left(\n\\mathbb{E}~\n\\left[\n\\mathbb{E}~\nX_i | \\theta\n\\right]\n\\right)^2\n\\]\nNote that \\(\\mathbb{E}~X_i | \\theta\\) is some function of \\(\\theta\\), say \\(g(\\theta)\\). It is easy to see\n\\[\nCov(X_i, X_j) = Var(g(\\theta)) \\geq 0\n\\]\n\nExplicit detail:\nBy de Finetti‚Äôs theorem exchangeability of \\(X_i\\) and \\(X_j\\) implies\n\\[\np(x_i, x_j) = \\int p(x_i | \\theta) p(x_j | \\theta) p(\\theta) d\\theta\n\\]\nRecall \\(p(x_i, x_j) = \\int p(x_i, x_j, \\theta) d\\theta\\) and therefore \\(p(x_i, x_j) = \\int p(x_i, x_j | \\theta) p(\\theta) d\\theta\\). By comparison to the above, notice\n\\[\np(x_i, x_j | \\theta) = p(x_i | \\theta) p(x_j|\\theta).\n\\]\nIn words, \\(x_i\\) and \\(x_j\\) are conditionally independent given \\(\\theta\\)."
  },
  {
    "objectID": "solutions/workshop.html",
    "href": "solutions/workshop.html",
    "title": "Workshopping",
    "section": "",
    "text": "Proof of concept\nWe have some data:\n\n# generating 10 samples from the population\ntrue.theta = 4\ntrue.sigma = 1\ny = rnorm(10, true.theta, true.sigma)\n\nybar = mean(y) # sample mean\nn = length(y) # sample size\ns2 = var(y) # sample variance\n\nWe make inference about \\(\\theta\\) and \\(\\sigma^2\\):\n\n# priors\n# theta prior\nmu_0 = 2; k_0 = 1\n# sigma2 prior\nnu_0 = 1; s2_0 = 0.010\n\n# posterior parameters\nkn = k_0 + n\nnun = nu_0 + n\nmun = (k_0 * mu_0 + n * ybar) /kn\ns2n = (nu_0 * s2_0 + (n - 1) * s2 + k_0 * n * (ybar - mu_0)^2 / (kn)) / (nun)\n\ns2.postsample = 1 / rgamma(10000, nun / 2, s2n * nun / 2)\ntheta.postsample = rnorm(10000, mun, sqrt(s2.postsample / kn))\n\ndf = data.frame(theta.postsample, s2.postsample)\n\ndf %>%\n  ggplot(aes(x = theta.postsample, y = s2.postsample)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  labs(x = TeX(\"$\\\\theta$\"),\n       y = TeX(\"$\\\\sigma^2$\"),\n       fill = TeX(\"$p(\\\\theta, \\\\sigma^2 | y_1, \\\\ldots y_n)$\")) +\n  theme_bw()"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA 360: Bayesian methods and modern statistics",
    "section": "",
    "text": "This course introduces Bayesian modeling and inference, motivated by real world examples. Course topics include Bayes‚Äô theorem, exchangeability, conjugate priors, Markov chain Monte Carlo (MCMC), Gibbs sampling, Metropolis-Hastings, hierarchical modeling, Bayesian regression and generalized linear models. We compare and contrast Bayesian methods to the frequentist paradigm. By the end of this course students should feel comfortable (1) writing Bayesian models and, when appropriate, (2) sampling from the posterior using MCMC to make inference.\n\n\n\n\n\n\n\n\n\nContact\nOffice hours\nLocation\n\n\n\n\nDr.¬†Alexander Fisher\naaf29@duke.edu\nMo/We: 10:30-11:30am\nOld Chem 223B\n\n\nMatt O‚ÄôDonnell\nmatthew.l.odonnell@duke.edu\nMo: 5:00-7:00pm\nOld Chem 203B\n\n\nShuo Wang\nshuo.wang717@duke.edu\nTu: 3:00-5:00pm\nOld Chem 203B\n\n\nBo Liu\nbo.liu1997@duke.edu\nWe: 12:45-2:45pm\nOld Chem 203B\n\n\nMinh-Anh To\nminhanh.to@duke.edu\nFr: 3:00-5:00pm\nZoom (link on Canvas)\n\n\n\n\n\n\n\n\n\nLecture\nMo/We 3:05 - 4:20pm\nReuben-Cooke Building 130\n\n\nLab 01\nTh 1:25pm - 2:40pm\nLSRC A155\n\n\nLab 02\nTh 3:05pm - 4:20pm\nOld Chemistry 003\n\n\n\nCourse website: sta360-fa24.github.io\n\n\n\n\n\n\n\n\n\n\n\nA First Course in Bayesian Statistical Methods. As a Duke student, an electronic version of the book is freely available to you on Springer link. Check the errata at the link above.\nChapter summaries. I compile major take-away points from each section. Review these to help prepare for exams.\nWe will use the statistical software package R on homework asignments in this course. R is freely available at http://www.r-project.org/. RStudio, the popular IDE for R, is freely available at https://posit.co/downloads/.\n\n\n\n\nPart I: The Bayesian modeling toolkit\n\nReview of probability\nConjugate statistical models\nSemi-conjugate models and Gibbs sampling\n\nPart II: Statistical model building and analysis\n\nMultilevel models\nLinear regression\nGeneralized linear models\nDensity estimation and classification\n\n\n\n\n\n\n\n\n\n\n\nAssignment\nDescription\n\n\n\n\nHomework (40%)\nIndividual take-home assignments, submitted to Gradescope.\n\n\nMidterms (30%)\nTwo in-class exams.\n\n\nFinal exam (25%)\nCumulative final during final‚Äôs week.\n\n\nQuizzes (5%)\nIn-class pop quizzes.\n\n\n\nA \\(>= 93\\), A- \\(< 93\\), B+ \\(< 90\\), B \\(< 87\\), B- \\(< 83\\), C+ \\(<80\\), C \\(< 77\\), C- \\(< 73\\), D+ \\(< 70\\), D \\(< 67\\), D- \\(< 63\\), F \\(< 60\\)\n\n\n\n\n\n\nA note on quizzes\n\n\n\nOn random class days, there will be a brief quiz on the previous lectures. If you score \\(>60\\%\\) cumulatively on your final quiz grade, you will receive full participation credit. Your lowest two quizzes will also be dropped.\n\n\n\n\n\n\n\n\nA note on exams\n\n\n\nIf you miss either midterm 1 or midterm 2, and have an excused absence, your missing midterm grade will be replaced by your final exam grade. You must take at least 1 midterm and the final exam to pass the course.\n\n\n\n\n\n\n\n\n\nAcademic integrity\nBy enrolling in this course, you commit to upholding Duke‚Äôs community standard reproduced as follows:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations of academic integrity will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action. For the Exams and Quizzes, students are required to work alone. For the Homework assignments, students may work with a study group but each student must write up and submit their own answers.\nLate work\nLate homework may be submitted within 48 hours of the assignment deadline. Late homework submitted within 24 hours (even 1 minute late) will receive a 5% late penalty. Late work submitted between 24 to 48 hours of the deadline will receive a 10% late penalty. Work submitted after 48 hours will not be accepted. Exams cannot be turned in late and can only be excused under exceptional circumstances. The Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\nErrors in grading\nErrors in grading must be brought to the attention of the TA or instructor during office hours within 1 week of receiving the grade."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "RStudio containers\n\nResources\n\nCanvas website\n\nFind announcements and grades here.\nSolutions uploaded here under ‚ÄúFiles‚Äù tab on the left hand side.\nRecorded zoom lectures uploaded here under ‚ÄúPages‚Äù tab on the left hand side.\n\nGradescope\n\nTextbook\n\nA First Course in Bayesian Statistical Methods by Peter Hoff\nErrata to the textbook"
  },
  {
    "objectID": "labs/lab0.html",
    "href": "labs/lab0.html",
    "title": "Hello R.",
    "section": "",
    "text": "This ‚Äòlab 0‚Äô will introduce you to the course computing workflow. The main goal of today is to get you setup in RStudio and play around with a few fundamental skills."
  },
  {
    "objectID": "labs/lab0.html#r-and-r-studio",
    "href": "labs/lab0.html#r-and-r-studio",
    "title": "Hello R.",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of a Quarto (.qmd) file. Note: this is essentially the same as an Rmarkdown (.Rmd) file, with a couple built-in quality of life additions."
  },
  {
    "objectID": "labs/lab0.html#yaml",
    "href": "labs/lab0.html#yaml",
    "title": "Hello R.",
    "section": "YAML",
    "text": "YAML\nThe top portion of your Quarto or R markdown file (between the three dashed lines) is called YAML. It stands for ‚ÄúYAML Ain‚Äôt Markup Language‚Äù. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nGo to file > new file, Quarto document. Input title ‚ÄúLab 0‚Äù, and change the author name to your name. Select pdf output and press Create. Render the document. Examine the rendered document."
  },
  {
    "objectID": "labs/lab0.html#latex",
    "href": "labs/lab0.html#latex",
    "title": "Hello R.",
    "section": "LaTeX",
    "text": "LaTeX\nAssignments in this course are not required to be written in LaTeX. You may write equations by hand and scan them as a pdf to submit to Gradescope. However, LaTeX is the typesetting system to communicate statistics and mathematics professionally. It‚Äôs worthwhile to use. Moreover, it‚Äôs fully supported within .Rmd and .qmd files.\nIf you‚Äôre using R on your local machine, you may need to install\n\nMiKTeX (if you‚Äôre using windows): https://miktex.org/\nMacTeX (if you‚Äôre using macOS): https://www.tug.org/mactex/\nTeXLive (if you‚Äôre using linux): https://tug.org/texlive/\n\nTo write a LaTeX equation within your markdown document, simply use $$ to surround blocks of math and $ to surround in-line math.\nExample: copy and paste the following and then render.\nWe can see that $\\beta_0 = 2 and \\beta_1 = 3$ is the OLS solution under our model\n\n$$\ny = \\beta_0 + \\beta_1 x\n$$\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is no space between $ and math. Whitespace may cause the document to fail to render.\n\n\nCheck out this LaTeX cheatsheet to typeset a variety of math."
  },
  {
    "objectID": "labs/lab0.html#exercises",
    "href": "labs/lab0.html#exercises",
    "title": "Hello R.",
    "section": "Exercises",
    "text": "Exercises\nThe following exercises are designed to help you gain basic familiarity with R as well as the quirks of floating point arithmetic.\n\nFloating point algebra.\n\nDo floating point numbers obey the rules of algebra? For example, one of the rules of algebra is additive association. (x + y) + z == x + (y + z). Check if this is true in R using \\(x = 0.1\\), \\(y = 0.1\\) and \\(z = 1\\). Explain what you find.\n\nAdditional examples of floating point pecularity are provided below.\n\n# example 1\n0.2 == 0.6 / 3\n# example 2\npoint3 <- c(0.3, 0.4 - 0.1, 0.5 - 0.2, 0.6 - 0.3, 0.7 - 0.4)\npoint3\npoint3 == 0.3\n\nTo work around these issues, you could use all.equal() for checking the equality of two double quantities in R. What does all.equal() do?\n\n# example 1, all.equal()\nall.equal(0.2, 0.6 / 3)\n# example 2, all.equal()\npoint3 <- c(0.3, 0.4 - 0.1, 0.5 - 0.2, 0.6 - 0.3, 0.7 - 0.4)\npoint3\nall.equal(point3, rep(.3, length(point3)))\n\n\nWhat do these functions do?\n\nUse ?rnorm to read the documentation and explain the output of each of the following:\n\nrnorm(10, mean = 1, sd = 2)\npnorm(0)\ndnorm(0.5)\nqnorm(0.5)\n\nHow is dnorm(0.5) computed? Can you compute it manually?\n\nShow it numerically\n\n\\(X \\sim N(\\mu, \\sigma^2)\\) means that \\(X\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Show, using rnorm that if \\(X \\sim N(0, 1)\\) and \\(Y \\sim N(1, 2)\\) that \\(\\mathbb{E}(X + Y) = 1\\) and \\(\\mathbb{V}(X + Y) = 3\\)\n\n\n\n\nControl flow\n\n\n# for loop example\nfor (i in 1:5) {\n  cat(\"Hello\", i, \"\\n\")\n}\n\n# if else example\nx = 1\nif(x > 0) {\n  print(\"I'm positive x is greater than 0.\")\n} else {\n  print(\"I'm not so positive about x being positive\")\n}\n\nAssume there are 50 days of class. Suppose that, on any given day, there is a \\(X_i\\) probability student \\(i\\) will come to class. Every day you come to class, you obtain Y points towards your final grade. Every day that you don‚Äôt come to class, you obtain Z points towards your final grade.\nAssume \\(Y \\sim Uniform(1.9, 2)\\) and \\(Z \\sim Uniform(1, 2)\\).\nAssume student A has a 95% chance of coming to class any given day (X = 0.95) and student B has a 70% of coming to class any given day (X = 0.7). While there are more efficient ways to do this, practice using a for loop, a conditional if statement, rbinom and runif to simulate one possible final grade for each student.\n\n\n\n\nAn example of plotting pdfs and pmfs with ggplot (loaded via the tidyverse package.)\n\n\npdf (normal)pmf (Poisson)\n\n\n\nlibrary(tidyverse)\nx = c(-3, 3) # values over which to plot\ndf = data.frame(x) # create a data frame for ggplot purposes\ndf %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = dnorm, args = c(mean = 0, sd = 3)) +\n  labs(title = \"Plot of a normal(0, 9) density\", y = \"f(x)\")\n\n\n\n\n\n\n\nlibrary(tidyverse)\ndf2 = data.frame(x = 0:6)\ndf2 %>%\n  mutate(y2 = dpois(x, 2)) %>%\n  ggplot(aes(x = x, y = y2)) + \n  geom_bar(stat = \"identity\") +\n  labs(title = \"Poisson(2) mass function\", y = \"f(x)\")"
  },
  {
    "objectID": "labs/lab0.html#style-guidelines",
    "href": "labs/lab0.html#style-guidelines",
    "title": "Hello R.",
    "section": "Style guidelines",
    "text": "Style guidelines\nAlthough coding is not the primary focus of this course, there are a short list below of fundamental principles we will follow. Note: some of these stylistic principles may not be followed in the text!\nFirst, it‚Äôs easy to write code that runs off the page when you render to pdf. This happens when you write more than 80 characters in a single line of code. To ensure this doesn‚Äôt happen, make sure your code doesn‚Äôt have 80 characters in a single line. To enable a vertical line in the RStudio IDE that helps you visually see the limit, go to Tools > Global Options > Code > Display > Show margin > 80. This will enable a vertical line in your .qmd files that shows you where the 80 character cutoff is for code chunks. Instructions may vary slightly for local installs of RStudio.\n\nAll binary operators should be surrounded by space. For example x + y is appropriate. x+y is not.\nAny and all pipes %>% or |> as well as ggplot layers + should be followed by a new line.\nYou should be consistent with stylistic choices, e.g.¬†only use 1 of = vs <- and %>% vs |>\nYour name should be at the top (in the YAML) of each document under ‚Äúauthor:‚Äù\n\nIf you have any questions about style, please ask a member of the teaching team."
  },
  {
    "objectID": "hw/hw00.html",
    "href": "hw/hw00.html",
    "title": "Homework 0",
    "section": "",
    "text": "This math assessment is meant to help both you and the instructor identify gaps in background knowledge both at the class and individual level."
  },
  {
    "objectID": "hw/hw00.html#exercise-1",
    "href": "hw/hw00.html#exercise-1",
    "title": "Homework 0",
    "section": "Exercise 1",
    "text": "Exercise 1\nSimplify\n\\[\n\\log(e^{a_1} e^{a_2} e^{a_3} \\cdots e^{a_n})\n\\]"
  },
  {
    "objectID": "hw/hw00.html#exercise-2",
    "href": "hw/hw00.html#exercise-2",
    "title": "Homework 0",
    "section": "Exercise 2",
    "text": "Exercise 2\nFind the derivative.\n\\[\n\\frac{d}{dx} \\left( \\frac{x}{\\log x} \\right)\n\\]"
  },
  {
    "objectID": "hw/hw00.html#exercise-3",
    "href": "hw/hw00.html#exercise-3",
    "title": "Homework 0",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat is the ordinary least squares estimator of \\(\\beta\\) (1-dimensional) in the linear regression \\(y = x \\beta + \\epsilon\\) with iid errors?"
  },
  {
    "objectID": "hw/hw00.html#exercise-4",
    "href": "hw/hw00.html#exercise-4",
    "title": "Homework 0",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat is the ordinary least squares estimator of \\(\\beta\\) (p-dimensional) in the linear regression \\(y = X \\beta + \\epsilon\\) with iid errors?"
  },
  {
    "objectID": "hw/hw00.html#exercise-5",
    "href": "hw/hw00.html#exercise-5",
    "title": "Homework 0",
    "section": "Exercise 5",
    "text": "Exercise 5\nIn linear regression with p-dimensional Œ≤, what is the interpretation of the estimate for the jth coefficient?"
  },
  {
    "objectID": "hw/hw00.html#exercise-6",
    "href": "hw/hw00.html#exercise-6",
    "title": "Homework 0",
    "section": "Exercise 6",
    "text": "Exercise 6\nCompute the integral,\n\\[\n\\int_{-\\infty}^{\\infty} e^{-x^2} dx\n\\]"
  },
  {
    "objectID": "hw/hw00.html#exercise-7",
    "href": "hw/hw00.html#exercise-7",
    "title": "Homework 0",
    "section": "Exercise 7",
    "text": "Exercise 7\n\\(X \\sim N(\\mu, \\sigma^2)\\) reads ‚ÄúX is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nLet\n\\[\n\\begin{aligned}\nX &\\sim N(0, 1),\\\\\nY &\\sim N(3, 2),\\\\\nZ &= X + Y\n\\end{aligned}\n\\]\nWhat is the distribution of \\(Z\\)? What is \\(\\mathbb{E}[Z]\\) and \\(Var(Z)\\)?"
  },
  {
    "objectID": "hw/hw00.html#exercise-8",
    "href": "hw/hw00.html#exercise-8",
    "title": "Homework 0",
    "section": "Exercise 8",
    "text": "Exercise 8\nIn your own words, the ‚Äúsupport‚Äù of random variable is‚Ä¶"
  },
  {
    "objectID": "hw/hw00.html#exercise-9",
    "href": "hw/hw00.html#exercise-9",
    "title": "Homework 0",
    "section": "Exercise 9",
    "text": "Exercise 9\nTRUE/FALSE: The product of two uniform[0, 1] random variables is uniform[0, 1]."
  },
  {
    "objectID": "hw/hw00.html#exercise-10",
    "href": "hw/hw00.html#exercise-10",
    "title": "Homework 0",
    "section": "Exercise 10",
    "text": "Exercise 10\n\\(X_1, \\ldots X_n\\) i.i.d. with pdf \\(p(x)\\). For all \\(i\\), \\(E(X_i) = \\mu\\) and \\(Var(X_i) = \\sigma^2 < \\infty\\).\nCompare \\(\\text{Var}(\\frac{1}{n}\\sum X_i)\\) to \\(\\text{Var}(X_1)\\). Which is smaller?"
  },
  {
    "objectID": "notes/dynamicalSystems.html",
    "href": "notes/dynamicalSystems.html",
    "title": "Parameter Estimation in Dynamical Systems",
    "section": "",
    "text": "library(tidyverse)\nToy example: logistic growth of a population.\n\\[\n\\frac{dN}{dt} = rN ( 1 - \\frac{N}{K})\n\\]\nwhere \\(N(t)\\) is the number of individuals at time \\(t\\), \\(r\\) is the per capita growth rate, and \\(K\\) is the carrying capacity.\nThis toy example can be solved analytically,\n\\[\nN(t) = N_0 f(t)\n\\]\nLet true parameter values \\(r = 1\\), \\(N_0 = 10\\), \\(K = 50\\)."
  },
  {
    "objectID": "data/Data.html",
    "href": "data/Data.html",
    "title": "STA 360: Data sets",
    "section": "",
    "text": "ALPHA_SAMPLES.RData\n\n\n12/6/23, 8:47:38 PM\n\n\n\n\n\n\n \n\n\n\n\nBETA_SAMPLES.RData\n\n\n12/6/23, 8:50:22 PM\n\n\n\n\n\n\n \n\n\n\n\nPima.csv\n\n\n10/23/23, 6:46:52 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-test.csv\n\n\n11/27/23, 12:16:56 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-train.csv\n\n\n11/27/23, 12:17:04 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes.csv\n\n\n11/26/23, 7:21:17 PM\n\n\n\n\n\n\n \n\n\n\n\nbach30.csv\n\n\n10/13/23, 12:12:28 PM\n\n\n\n\n\n\n \n\n\n\n\nbanana_distribution.csv\n\n\n12/5/23, 12:30:02 AM\n\n\n\n\n\n\n \n\n\n\n\nbluecrab.csv\n\n\n10/31/23, 2:48:53 PM\n\n\n\n\n\n\n \n\n\n\n\nciphertext.txt\n\n\n4/25/23, 10:37:00 PM\n\n\n\n\n\n\n \n\n\n\n\ndivorce.csv\n\n\n10/20/23, 3:15:56 PM\n\n\n\n\n\n\n \n\n\n\n\nglucose.csv\n\n\n10/12/23, 9:09:49 AM\n\n\n\n\n\n\n \n\n\n\n\ngss.csv\n\n\n9/19/23, 12:24:10 AM\n\n\n\n\n\n\n \n\n\n\n\nmathScores.csv\n\n\n10/25/23, 8:06:12 PM\n\n\n\n\n\n\n \n\n\n\n\nnobach30.csv\n\n\n10/13/23, 12:14:52 PM\n\n\n\n\n\n\n \n\n\n\n\norangecrab.csv\n\n\n10/31/23, 2:49:56 PM\n\n\n\n\n\n\n \n\n\n\n\nschool1.csv\n\n\n9/29/23, 5:24:59 PM\n\n\n\n\n\n\n \n\n\n\n\nschool2.csv\n\n\n9/29/23, 5:24:57 PM\n\n\n\n\n\n\n \n\n\n\n\nschool3.csv\n\n\n9/29/23, 5:24:53 PM\n\n\n\n\n\n\n \n\n\n\n\nschool4.csv\n\n\n10/31/23, 2:54:58 PM\n\n\n\n\n\n\n \n\n\n\n\nschool5.csv\n\n\n10/31/23, 2:55:04 PM\n\n\n\n\n\n\n \n\n\n\n\nschool6.csv\n\n\n10/31/23, 2:55:07 PM\n\n\n\n\n\n\n \n\n\n\n\nschool7.csv\n\n\n10/31/23, 2:55:10 PM\n\n\n\n\n\n\n \n\n\n\n\nschool8.csv\n\n\n10/31/23, 2:55:14 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.RData\n\n\n11/27/23, 3:38:32 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.rds\n\n\n11/28/23, 12:00:32 AM\n\n\n\n\n\n\n \n\n\n\n\nyXsparrow.csv\n\n\n11/27/23, 12:08:36 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "STA 360: Data sets",
    "section": "",
    "text": "ALPHA_SAMPLES.RData\n\n\n12/6/23, 8:47:38 PM\n\n\n\n\n\n\n \n\n\n\n\nBETA_SAMPLES.RData\n\n\n12/6/23, 8:50:22 PM\n\n\n\n\n\n\n \n\n\n\n\nPima.csv\n\n\n10/23/23, 6:46:52 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-test.csv\n\n\n11/27/23, 12:16:56 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-train.csv\n\n\n11/27/23, 12:17:04 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes.csv\n\n\n11/26/23, 7:21:17 PM\n\n\n\n\n\n\n \n\n\n\n\nbach30.csv\n\n\n10/13/23, 12:12:28 PM\n\n\n\n\n\n\n \n\n\n\n\nbanana_distribution.csv\n\n\n12/5/23, 12:30:02 AM\n\n\n\n\n\n\n \n\n\n\n\nbluecrab.csv\n\n\n10/31/23, 2:48:53 PM\n\n\n\n\n\n\n \n\n\n\n\nciphertext.txt\n\n\n4/25/23, 10:37:00 PM\n\n\n\n\n\n\n \n\n\n\n\ndivorce.csv\n\n\n10/20/23, 3:15:56 PM\n\n\n\n\n\n\n \n\n\n\n\nglucose.csv\n\n\n10/12/23, 9:09:49 AM\n\n\n\n\n\n\n \n\n\n\n\ngss.csv\n\n\n9/19/23, 12:24:10 AM\n\n\n\n\n\n\n \n\n\n\n\nmathScores.csv\n\n\n10/25/23, 8:06:12 PM\n\n\n\n\n\n\n \n\n\n\n\nnobach30.csv\n\n\n10/13/23, 12:14:52 PM\n\n\n\n\n\n\n \n\n\n\n\norangecrab.csv\n\n\n10/31/23, 2:49:56 PM\n\n\n\n\n\n\n \n\n\n\n\nschool1.csv\n\n\n9/29/23, 5:24:59 PM\n\n\n\n\n\n\n \n\n\n\n\nschool2.csv\n\n\n9/29/23, 5:24:57 PM\n\n\n\n\n\n\n \n\n\n\n\nschool3.csv\n\n\n9/29/23, 5:24:53 PM\n\n\n\n\n\n\n \n\n\n\n\nschool4.csv\n\n\n10/31/23, 2:54:58 PM\n\n\n\n\n\n\n \n\n\n\n\nschool5.csv\n\n\n10/31/23, 2:55:04 PM\n\n\n\n\n\n\n \n\n\n\n\nschool6.csv\n\n\n10/31/23, 2:55:07 PM\n\n\n\n\n\n\n \n\n\n\n\nschool7.csv\n\n\n10/31/23, 2:55:10 PM\n\n\n\n\n\n\n \n\n\n\n\nschool8.csv\n\n\n10/31/23, 2:55:14 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.RData\n\n\n11/27/23, 3:38:32 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.rds\n\n\n11/28/23, 12:00:32 AM\n\n\n\n\n\n\n \n\n\n\n\nyXsparrow.csv\n\n\n11/27/23, 12:08:36 PM\n\n\n\n\n\n\n \n\n\n\n\nALPHA_SAMPLES.RData\n\n\n12/6/23, 8:47:38 PM\n\n\n\n\n\n\n \n\n\n\n\nBETA_SAMPLES.RData\n\n\n12/6/23, 8:50:22 PM\n\n\n\n\n\n\n \n\n\n\n\nData.html\n\n\n8/24/24, 8:54:39 AM\n\n\n\n\n\n\n \n\n\n\n\nData.qmd\n\n\n8/24/24, 8:54:28 AM\n\n\n\n\n\n\n \n\n\n\n\nPima.csv\n\n\n10/23/23, 6:46:52 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-test.csv\n\n\n11/27/23, 12:16:56 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-train.csv\n\n\n11/27/23, 12:17:04 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes.csv\n\n\n11/26/23, 7:21:17 PM\n\n\n\n\n\n\n \n\n\n\n\nbach30.csv\n\n\n10/13/23, 12:12:28 PM\n\n\n\n\n\n\n \n\n\n\n\nbanana_distribution.csv\n\n\n12/5/23, 12:30:02 AM\n\n\n\n\n\n\n \n\n\n\n\nbluecrab.csv\n\n\n10/31/23, 2:48:53 PM\n\n\n\n\n\n\n \n\n\n\n\nciphertext.txt\n\n\n4/25/23, 10:37:00 PM\n\n\n\n\n\n\n \n\n\n\n\ndivorce.csv\n\n\n10/20/23, 3:15:56 PM\n\n\n\n\n\n\n \n\n\n\n\nglucose.csv\n\n\n10/12/23, 9:09:49 AM\n\n\n\n\n\n\n \n\n\n\n\ngss.csv\n\n\n9/19/23, 12:24:10 AM\n\n\n\n\n\n\n \n\n\n\n\nmathScores.csv\n\n\n10/25/23, 8:06:12 PM\n\n\n\n\n\n\n \n\n\n\n\nnobach30.csv\n\n\n10/13/23, 12:14:52 PM\n\n\n\n\n\n\n \n\n\n\n\norangecrab.csv\n\n\n10/31/23, 2:49:56 PM\n\n\n\n\n\n\n \n\n\n\n\nschool1.csv\n\n\n9/29/23, 5:24:59 PM\n\n\n\n\n\n\n \n\n\n\n\nschool2.csv\n\n\n9/29/23, 5:24:57 PM\n\n\n\n\n\n\n \n\n\n\n\nschool3.csv\n\n\n9/29/23, 5:24:53 PM\n\n\n\n\n\n\n \n\n\n\n\nschool4.csv\n\n\n10/31/23, 2:54:58 PM\n\n\n\n\n\n\n \n\n\n\n\nschool5.csv\n\n\n10/31/23, 2:55:04 PM\n\n\n\n\n\n\n \n\n\n\n\nschool6.csv\n\n\n10/31/23, 2:55:07 PM\n\n\n\n\n\n\n \n\n\n\n\nschool7.csv\n\n\n10/31/23, 2:55:10 PM\n\n\n\n\n\n\n \n\n\n\n\nschool8.csv\n\n\n10/31/23, 2:55:14 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.RData\n\n\n11/27/23, 3:38:32 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.rds\n\n\n11/28/23, 12:00:32 AM\n\n\n\n\n\n\n \n\n\n\n\nyXsparrow.csv\n\n\n11/27/23, 12:08:36 PM\n\n\n\n\n\n\n \n\n\n\n\nALPHA_SAMPLES.RData\n\n\n12/6/23, 8:47:38 PM\n\n\n\n\n\n\n \n\n\n\n\nBETA_SAMPLES.RData\n\n\n12/6/23, 8:50:22 PM\n\n\n\n\n\n\n \n\n\n\n\nData.html\n\n\n8/24/24, 8:54:39 AM\n\n\n\n\n\n\n \n\n\n\n\nData.qmd\n\n\n8/24/24, 8:54:28 AM\n\n\n\n\n\n\n \n\n\n\n\nPima.csv\n\n\n10/23/23, 6:46:52 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-test.csv\n\n\n11/27/23, 12:16:56 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-train.csv\n\n\n11/27/23, 12:17:04 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes.csv\n\n\n11/26/23, 7:21:17 PM\n\n\n\n\n\n\n \n\n\n\n\nbach30.csv\n\n\n10/13/23, 12:12:28 PM\n\n\n\n\n\n\n \n\n\n\n\nbanana_distribution.csv\n\n\n12/5/23, 12:30:02 AM\n\n\n\n\n\n\n \n\n\n\n\nbluecrab.csv\n\n\n10/31/23, 2:48:53 PM\n\n\n\n\n\n\n \n\n\n\n\nciphertext.txt\n\n\n4/25/23, 10:37:00 PM\n\n\n\n\n\n\n \n\n\n\n\ndivorce.csv\n\n\n10/20/23, 3:15:56 PM\n\n\n\n\n\n\n \n\n\n\n\nglucose.csv\n\n\n10/12/23, 9:09:49 AM\n\n\n\n\n\n\n \n\n\n\n\ngss.csv\n\n\n9/19/23, 12:24:10 AM\n\n\n\n\n\n\n \n\n\n\n\nmathScores.csv\n\n\n10/25/23, 8:06:12 PM\n\n\n\n\n\n\n \n\n\n\n\nnobach30.csv\n\n\n10/13/23, 12:14:52 PM\n\n\n\n\n\n\n \n\n\n\n\norangecrab.csv\n\n\n10/31/23, 2:49:56 PM\n\n\n\n\n\n\n \n\n\n\n\nschool1.csv\n\n\n9/29/23, 5:24:59 PM\n\n\n\n\n\n\n \n\n\n\n\nschool2.csv\n\n\n9/29/23, 5:24:57 PM\n\n\n\n\n\n\n \n\n\n\n\nschool3.csv\n\n\n9/29/23, 5:24:53 PM\n\n\n\n\n\n\n \n\n\n\n\nschool4.csv\n\n\n10/31/23, 2:54:58 PM\n\n\n\n\n\n\n \n\n\n\n\nschool5.csv\n\n\n10/31/23, 2:55:04 PM\n\n\n\n\n\n\n \n\n\n\n\nschool6.csv\n\n\n10/31/23, 2:55:07 PM\n\n\n\n\n\n\n \n\n\n\n\nschool7.csv\n\n\n10/31/23, 2:55:10 PM\n\n\n\n\n\n\n \n\n\n\n\nschool8.csv\n\n\n10/31/23, 2:55:14 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.RData\n\n\n11/27/23, 3:38:32 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.rds\n\n\n11/28/23, 12:00:32 AM\n\n\n\n\n\n\n \n\n\n\n\nyXsparrow.csv\n\n\n11/27/23, 12:08:36 PM\n\n\n\n\n\n\n \n\n\n\n\nALPHA_SAMPLES.RData\n\n\n12/6/23, 8:47:38 PM\n\n\n\n\n\n\n \n\n\n\n\nBETA_SAMPLES.RData\n\n\n12/6/23, 8:50:22 PM\n\n\n\n\n\n\n \n\n\n\n\nData.html\n\n\n8/24/24, 8:54:39 AM\n\n\n\n\n\n\n \n\n\n\n\nData.qmd\n\n\n8/24/24, 8:54:28 AM\n\n\n\n\n\n\n \n\n\n\n\nPima.csv\n\n\n10/23/23, 6:46:52 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-test.csv\n\n\n11/27/23, 12:16:56 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-train.csv\n\n\n11/27/23, 12:17:04 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes.csv\n\n\n11/26/23, 7:21:17 PM\n\n\n\n\n\n\n \n\n\n\n\nbach30.csv\n\n\n10/13/23, 12:12:28 PM\n\n\n\n\n\n\n \n\n\n\n\nbanana_distribution.csv\n\n\n12/5/23, 12:30:02 AM\n\n\n\n\n\n\n \n\n\n\n\nbluecrab.csv\n\n\n10/31/23, 2:48:53 PM\n\n\n\n\n\n\n \n\n\n\n\nciphertext.txt\n\n\n4/25/23, 10:37:00 PM\n\n\n\n\n\n\n \n\n\n\n\ndivorce.csv\n\n\n10/20/23, 3:15:56 PM\n\n\n\n\n\n\n \n\n\n\n\nglucose.csv\n\n\n10/12/23, 9:09:49 AM\n\n\n\n\n\n\n \n\n\n\n\ngss.csv\n\n\n9/19/23, 12:24:10 AM\n\n\n\n\n\n\n \n\n\n\n\nmathScores.csv\n\n\n10/25/23, 8:06:12 PM\n\n\n\n\n\n\n \n\n\n\n\nnobach30.csv\n\n\n10/13/23, 12:14:52 PM\n\n\n\n\n\n\n \n\n\n\n\norangecrab.csv\n\n\n10/31/23, 2:49:56 PM\n\n\n\n\n\n\n \n\n\n\n\nschool1.csv\n\n\n9/29/23, 5:24:59 PM\n\n\n\n\n\n\n \n\n\n\n\nschool2.csv\n\n\n9/29/23, 5:24:57 PM\n\n\n\n\n\n\n \n\n\n\n\nschool3.csv\n\n\n9/29/23, 5:24:53 PM\n\n\n\n\n\n\n \n\n\n\n\nschool4.csv\n\n\n10/31/23, 2:54:58 PM\n\n\n\n\n\n\n \n\n\n\n\nschool5.csv\n\n\n10/31/23, 2:55:04 PM\n\n\n\n\n\n\n \n\n\n\n\nschool6.csv\n\n\n10/31/23, 2:55:07 PM\n\n\n\n\n\n\n \n\n\n\n\nschool7.csv\n\n\n10/31/23, 2:55:10 PM\n\n\n\n\n\n\n \n\n\n\n\nschool8.csv\n\n\n10/31/23, 2:55:14 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.RData\n\n\n11/27/23, 3:38:32 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.rds\n\n\n11/28/23, 12:00:32 AM\n\n\n\n\n\n\n \n\n\n\n\nyXsparrow.csv\n\n\n11/27/23, 12:08:36 PM\n\n\n\n\n\n\n \n\n\n\n\nALPHA_SAMPLES.RData\n\n\n12/6/23, 8:47:38 PM\n\n\n\n\n\n\n \n\n\n\n\nBETA_SAMPLES.RData\n\n\n12/6/23, 8:50:22 PM\n\n\n\n\n\n\n \n\n\n\n\nData.html\n\n\n8/24/24, 8:54:39 AM\n\n\n\n\n\n\n \n\n\n\n\nData.qmd\n\n\n8/24/24, 8:54:28 AM\n\n\n\n\n\n\n \n\n\n\n\nPima.csv\n\n\n10/23/23, 6:46:52 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-test.csv\n\n\n11/27/23, 12:16:56 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-train.csv\n\n\n11/27/23, 12:17:04 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes.csv\n\n\n11/26/23, 7:21:17 PM\n\n\n\n\n\n\n \n\n\n\n\nbach30.csv\n\n\n10/13/23, 12:12:28 PM\n\n\n\n\n\n\n \n\n\n\n\nbanana_distribution.csv\n\n\n12/5/23, 12:30:02 AM\n\n\n\n\n\n\n \n\n\n\n\nbluecrab.csv\n\n\n10/31/23, 2:48:53 PM\n\n\n\n\n\n\n \n\n\n\n\nciphertext.txt\n\n\n4/25/23, 10:37:00 PM\n\n\n\n\n\n\n \n\n\n\n\ndivorce.csv\n\n\n10/20/23, 3:15:56 PM\n\n\n\n\n\n\n \n\n\n\n\nglucose.csv\n\n\n10/12/23, 9:09:49 AM\n\n\n\n\n\n\n \n\n\n\n\ngss.csv\n\n\n9/19/23, 12:24:10 AM\n\n\n\n\n\n\n \n\n\n\n\nmathScores.csv\n\n\n10/25/23, 8:06:12 PM\n\n\n\n\n\n\n \n\n\n\n\nnobach30.csv\n\n\n10/13/23, 12:14:52 PM\n\n\n\n\n\n\n \n\n\n\n\norangecrab.csv\n\n\n10/31/23, 2:49:56 PM\n\n\n\n\n\n\n \n\n\n\n\nschool1.csv\n\n\n9/29/23, 5:24:59 PM\n\n\n\n\n\n\n \n\n\n\n\nschool2.csv\n\n\n9/29/23, 5:24:57 PM\n\n\n\n\n\n\n \n\n\n\n\nschool3.csv\n\n\n9/29/23, 5:24:53 PM\n\n\n\n\n\n\n \n\n\n\n\nschool4.csv\n\n\n10/31/23, 2:54:58 PM\n\n\n\n\n\n\n \n\n\n\n\nschool5.csv\n\n\n10/31/23, 2:55:04 PM\n\n\n\n\n\n\n \n\n\n\n\nschool6.csv\n\n\n10/31/23, 2:55:07 PM\n\n\n\n\n\n\n \n\n\n\n\nschool7.csv\n\n\n10/31/23, 2:55:10 PM\n\n\n\n\n\n\n \n\n\n\n\nschool8.csv\n\n\n10/31/23, 2:55:14 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.RData\n\n\n11/27/23, 3:38:32 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.rds\n\n\n11/28/23, 12:00:32 AM\n\n\n\n\n\n\n \n\n\n\n\nyXsparrow.csv\n\n\n11/27/23, 12:08:36 PM\n\n\n\n\n\n\n \n\n\n\n\nALPHA_SAMPLES.RData\n\n\n12/6/23, 8:47:38 PM\n\n\n\n\n\n\n \n\n\n\n\nBETA_SAMPLES.RData\n\n\n12/6/23, 8:50:22 PM\n\n\n\n\n\n\n \n\n\n\n\nData.html\n\n\n8/24/24, 8:54:39 AM\n\n\n\n\n\n\n \n\n\n\n\nData.qmd\n\n\n8/24/24, 8:54:28 AM\n\n\n\n\n\n\n \n\n\n\n\nPima.csv\n\n\n10/23/23, 6:46:52 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-test.csv\n\n\n11/27/23, 12:16:56 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-train.csv\n\n\n11/27/23, 12:17:04 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes.csv\n\n\n11/26/23, 7:21:17 PM\n\n\n\n\n\n\n \n\n\n\n\nbach30.csv\n\n\n10/13/23, 12:12:28 PM\n\n\n\n\n\n\n \n\n\n\n\nbanana_distribution.csv\n\n\n12/5/23, 12:30:02 AM\n\n\n\n\n\n\n \n\n\n\n\nbluecrab.csv\n\n\n10/31/23, 2:48:53 PM\n\n\n\n\n\n\n \n\n\n\n\nciphertext.txt\n\n\n4/25/23, 10:37:00 PM\n\n\n\n\n\n\n \n\n\n\n\ndivorce.csv\n\n\n10/20/23, 3:15:56 PM\n\n\n\n\n\n\n \n\n\n\n\nglucose.csv\n\n\n10/12/23, 9:09:49 AM\n\n\n\n\n\n\n \n\n\n\n\ngss.csv\n\n\n9/19/23, 12:24:10 AM\n\n\n\n\n\n\n \n\n\n\n\nmathScores.csv\n\n\n10/25/23, 8:06:12 PM\n\n\n\n\n\n\n \n\n\n\n\nnobach30.csv\n\n\n10/13/23, 12:14:52 PM\n\n\n\n\n\n\n \n\n\n\n\norangecrab.csv\n\n\n10/31/23, 2:49:56 PM\n\n\n\n\n\n\n \n\n\n\n\nschool1.csv\n\n\n9/29/23, 5:24:59 PM\n\n\n\n\n\n\n \n\n\n\n\nschool2.csv\n\n\n9/29/23, 5:24:57 PM\n\n\n\n\n\n\n \n\n\n\n\nschool3.csv\n\n\n9/29/23, 5:24:53 PM\n\n\n\n\n\n\n \n\n\n\n\nschool4.csv\n\n\n10/31/23, 2:54:58 PM\n\n\n\n\n\n\n \n\n\n\n\nschool5.csv\n\n\n10/31/23, 2:55:04 PM\n\n\n\n\n\n\n \n\n\n\n\nschool6.csv\n\n\n10/31/23, 2:55:07 PM\n\n\n\n\n\n\n \n\n\n\n\nschool7.csv\n\n\n10/31/23, 2:55:10 PM\n\n\n\n\n\n\n \n\n\n\n\nschool8.csv\n\n\n10/31/23, 2:55:14 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.RData\n\n\n11/27/23, 3:38:32 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.rds\n\n\n11/28/23, 12:00:32 AM\n\n\n\n\n\n\n \n\n\n\n\nyXsparrow.csv\n\n\n11/27/23, 12:08:36 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "STA 360: Data sets",
    "section": "",
    "text": "ALPHA_SAMPLES.RData\n\n\n12/6/23, 8:47:38 PM\n\n\n\n\n\n\n \n\n\n\n\nBETA_SAMPLES.RData\n\n\n12/6/23, 8:50:22 PM\n\n\n\n\n\n\n \n\n\n\n\nPima.csv\n\n\n10/23/23, 6:46:52 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-test.csv\n\n\n11/27/23, 12:16:56 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes-train.csv\n\n\n11/27/23, 12:17:04 PM\n\n\n\n\n\n\n \n\n\n\n\nazdiabetes.csv\n\n\n11/26/23, 7:21:17 PM\n\n\n\n\n\n\n \n\n\n\n\nbach30.csv\n\n\n10/13/23, 12:12:28 PM\n\n\n\n\n\n\n \n\n\n\n\nbanana_distribution.csv\n\n\n12/5/23, 12:30:02 AM\n\n\n\n\n\n\n \n\n\n\n\nbluecrab.csv\n\n\n10/31/23, 2:48:53 PM\n\n\n\n\n\n\n \n\n\n\n\nciphertext.txt\n\n\n4/25/23, 10:37:00 PM\n\n\n\n\n\n\n \n\n\n\n\ndivorce.csv\n\n\n10/20/23, 3:15:56 PM\n\n\n\n\n\n\n \n\n\n\n\nglucose.csv\n\n\n10/12/23, 9:09:49 AM\n\n\n\n\n\n\n \n\n\n\n\ngss.csv\n\n\n9/19/23, 12:24:10 AM\n\n\n\n\n\n\n \n\n\n\n\nmathScores.csv\n\n\n10/25/23, 8:06:12 PM\n\n\n\n\n\n\n \n\n\n\n\nnobach30.csv\n\n\n10/13/23, 12:14:52 PM\n\n\n\n\n\n\n \n\n\n\n\norangecrab.csv\n\n\n10/31/23, 2:49:56 PM\n\n\n\n\n\n\n \n\n\n\n\nschool1.csv\n\n\n9/29/23, 5:24:59 PM\n\n\n\n\n\n\n \n\n\n\n\nschool2.csv\n\n\n9/29/23, 5:24:57 PM\n\n\n\n\n\n\n \n\n\n\n\nschool3.csv\n\n\n9/29/23, 5:24:53 PM\n\n\n\n\n\n\n \n\n\n\n\nschool4.csv\n\n\n10/31/23, 2:54:58 PM\n\n\n\n\n\n\n \n\n\n\n\nschool5.csv\n\n\n10/31/23, 2:55:04 PM\n\n\n\n\n\n\n \n\n\n\n\nschool6.csv\n\n\n10/31/23, 2:55:07 PM\n\n\n\n\n\n\n \n\n\n\n\nschool7.csv\n\n\n10/31/23, 2:55:10 PM\n\n\n\n\n\n\n \n\n\n\n\nschool8.csv\n\n\n10/31/23, 2:55:14 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.RData\n\n\n11/27/23, 3:38:32 PM\n\n\n\n\n\n\n \n\n\n\n\ntrans-prob-mat.rds\n\n\n11/28/23, 12:00:32 AM\n\n\n\n\n\n\n \n\n\n\n\nyXsparrow.csv\n\n\n11/27/23, 12:08:36 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/lec01-probability.html",
    "href": "notes/lec01-probability.html",
    "title": "Probability",
    "section": "",
    "text": "This is foundational material. Most of it is background you will have learned in STA230/240. While dry, we must soldier on to get to the exciting stuff."
  },
  {
    "objectID": "notes/lec01-probability.html#review-set-theory",
    "href": "notes/lec01-probability.html#review-set-theory",
    "title": "Probability",
    "section": "Review: set theory",
    "text": "Review: set theory\n\n\n\n\n\n\nDefinition\n\n\n\nset: a collection of elements, denoted by {}\nExamples\n\n\\(\\phi\\) = {} ‚Äúthe empty set‚Äù\nA = {1, 2, 3}\nB = {taken STA199, has not taken STA199}\nC = {{1,2,3}, {4, 5}}\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nsubset: denoted by \\(\\subset\\), \\(A \\subset B\\) iff \\(a \\in A \\implies a \\in B\\)\nExamples\nUsing the previously examples of A, B and C above,\n\n\\(A \\subset C\\)\n\\(A \\not\\subset B\\)\n\n\n\nRecall:\n\n\\(\\cup\\) means ‚Äúunion‚Äù, ‚Äúor‚Äù\n\\(\\cap\\) means ‚Äúintersection‚Äù, ‚Äúand‚Äù\n\n\n\n\n\n\n\nDefinition\n\n\n\npartition: {\\(H_1, H_2, ... H_n\\)} = \\(\\{H_i\\}_{i = 1}^n\\) is a partition of \\(\\mathcal{H}\\) if\n\nthe union of sets is \\(\\mathcal{H}\\) i.e. \\(\\cup_{i = 1}^n H_i = \\mathcal{H}\\)\nthe sets are disjoint i.e.¬†\\(H_i \\cap H_j = \\phi\\) for all \\(i \\neq j\\)\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nsample space: \\(\\mathcal{H}\\), the set of all possible data sets (outcomes)\nevent: a set of one or more outcomes\nNote: p(\\(\\mathcal{H}\\)) = 1\nExamples\n\nRoll a six-sided die once. The sample space \\(\\mathcal{H} = \\{1, 2, 3, 4, 5, 6\\}\\).\nLet \\(A\\) be the event that the die lands on an even number. \\(A = \\{2, 4, 6 \\}\\)"
  },
  {
    "objectID": "notes/lec01-probability.html#axioms-of-probability-in-words",
    "href": "notes/lec01-probability.html#axioms-of-probability-in-words",
    "title": "Probability",
    "section": "Axioms of probability (in words)",
    "text": "Axioms of probability (in words)\nP1. Probabilities are between 0 and 1, importantly p(\\(\\neg\\)H|H) = 0 and p(H|H) = 1.\nP2. If two events A and B are disjoint, then p(A or B) = p(A) + p(B)\nP3. The joint probability of two events may be broken down stepwise: p(A,B) = p(A|B)p(B)\n‚Äì\nIt follows that\n\nfor any partition \\(\\{H_i\\}_{i = 1}^n\\), \\(\\sum_{i=1}^n p(H_i) = 1\\) (rule of total probability)\n\nnote: simplest partition \\(p(A) + p(\\neg A) = 1\\)\n\n\\(p(A) = \\sum_{i=1}^n p(A, H_i)\\) (rule of marginal probability)\n\nnote: P3 implies that equivalently, \\(p(A) = \\sum_{i=1}^n p(A | H_i) p(H_i)\\)\n\np(A|B) = p(A,B) / p(B) when p(B) \\(\\neq 0\\)\n\nnote: these statements can also be made where each term is additionally conditioned on another event C\n\n\n\n\n\n\n\n\nExercise\n\n\n\nDerive Bayes‚Äô rule:\n\\(p(H_i|X) = \\frac{p(X|H_i)p(H_i)}{\\sum_k p(X|H_k)p(H_k)}\\)\nusing the axioms of probability.\n\n\nBayes‚Äô rule tells us how to update beliefs about \\(\\{H_i \\}_{i = 1}^n\\) given data \\(X\\)."
  },
  {
    "objectID": "notes/lec01-probability.html#independence",
    "href": "notes/lec01-probability.html#independence",
    "title": "Probability",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\nDefinition\n\n\n\nTwo events \\(F\\) and \\(G\\) are conditionally independent given \\(H\\) if \\(p(F, G | H) = p(F | H) p(G | H)\\)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nShow conditional independence implies\n\\(p(F | H, G) = p(F | H)\\)\n\n\nThis means that if we know H, then G does not supply any additional information about F."
  },
  {
    "objectID": "notes/lec01-probability.html#random-variables",
    "href": "notes/lec01-probability.html#random-variables",
    "title": "Probability",
    "section": "Random variables",
    "text": "Random variables\n\n\n\n\n\n\nDefinition\n\n\n\nIn Bayesian inference, a random variable is an unknown numerical quantity about which we make probability statements.\nThe support of a random variable is the set of values a random variable can take.\nExamples\n\nData. E.g. the amount of a wheat a field will yield later this year. Since this data has not yet been generated, the quantity is unknown.\nA population parameter. E.g. the true mean resting heart rate of Duke students. Note: this is a fixed (non-random) quantity, but it is also unknown. We use probability to describe our uncertainty in this quantity.\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\ndiscrete random variable: a random variable that takes countably many values. Y is discrete if its possible outcomes can be enumerated \\(\\mathcal{Y} = \\{y_1, y_2, \\ldots \\}\\).\nNote: discrete does not mean finite. There may be infinitely many outcomes!\nExamples\n\nY = the number of children of a randomly sampled person\nY = the number of visible stars in the sky on a randomly sampled night of the year\n\nFor each \\(y \\in \\mathcal{Y}\\), let p(Y) = probability(Y = y). Then p is the probability mass function (pmf) of the random variable Y.\nExamples\n\nBinomial pmf: the probability of \\(y\\) successes in \\(n\\) trials, where each trial has an individual probability of success \\(\\theta\\).\n\\[p(y | \\theta) = {n \\choose y} \\theta ^y (1-\\theta)^{n-y} \\text{ for } y \\in \\{0, 1, \\ldots n \\}\\]\n\nsupport: \\(y \\in \\{0, 1, 2, \\ldots n\\}\\)\nsuccess probability \\(\\theta \\in [0, 1]\\)\ndbinom(y, n, theta) computes this pmf in R\n\nPoisson pmf: probability of \\(y\\) events occurring during a fixed interval at a mean rate \\(\\theta\\)\n\\[p(y | \\theta) = \\frac{\\theta^y e^{-\\theta}}{y!}\\]\n\nsupport: \\(y \\in \\{0, 1, 2, \\ldots \\}\\)\nrate \\(\\theta \\in \\mathbb{R}^+\\)\ndpois(y, theta) computes this pmf in R\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\ncontinuous random variable: a random variable that takes uncountably many values.\nThe probability density function (pdf) of a continuous random variable, X is defined\n\\(pdf(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{p(x < X < x + \\Delta x)}{\\Delta x}\\)\nand the probability X is in some interval,\n\\(p(x_1 < X < x_2) = \\int_{x_1}^{x_2} pdf(x) dx\\)\nExamples\n\nNormal pdf \\[\np(x | \\mu, \\sigma) = (2\\pi \\sigma^2)^{-\\frac{1}{2}}e^{-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\n\\]\nUniform pdf \\[p(x|a,b) =\n\\begin{cases}\n\\frac{1}{b - a} \\hspace{.6cm}\\text{ for } x \\in [a, b]\\\\\n0 \\hspace{1cm}\\text{ otherwise }\n\\end{cases}\\]\n\n\n\nNote: we will often abuse notation and use \\(p(x)\\) in place of \\(pmf(x)\\) and \\(pdf(x)\\) and prob(event), where only the context makes meaning clear.\nFor pmfs\n\\[\n\\begin{aligned}\n0 \\leq p(y) \\leq 1\\\\\n\\sum_{y \\in \\mathcal{Y}} p(y) = 1\n\\end{aligned}\n\\]\nSimilarly, for pdfs,\n\\[\n\\begin{aligned}\n0 \\leq p(y) \\ \\text{and} \\\\\n\\int_{y \\in \\mathcal{Y}} p(y) = 1\n\\end{aligned}\n\\]\nNote: For a continuous random variable Y, p(y) can be larger than 1 and p(y) is not p(Y = y), which equals 0.\n\n\n\n\n\n\nDefinition\n\n\n\nThe part of the density/mass function that depends on the variable is called the kernel.\nExample\n\nthe kernel of the normal pdf is \\(e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat‚Äôs the kernel of a gamma random variable X?\nRecall: the pdf of a gamma distribution:\n\\[\np(x | \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha - 1} e^{-\\beta x}\n\\]"
  },
  {
    "objectID": "notes/lec01-probability.html#moments",
    "href": "notes/lec01-probability.html#moments",
    "title": "Probability",
    "section": "Moments",
    "text": "Moments\nFor a random variable X, the \\(n\\)th moment is defined as E(\\(X^n\\)).\nRecall, the expected value is defined for discrete random variable X,\n\\[\nE(X) = \\sum_{x \\in \\mathcal{X}} x p(x)\n\\]\nand for continuous random variable Y,\n\\[\nE(Y) = \\int_{-\\infty}^{\\infty} y p(y) dy\n\\]\nThe variance of a random variable, is also known as the second central moment and is defined\n\\[\nE(X - E(X))^2\n\\] or equivalently,\n\\[\nE(X^2) - E(X)^2\n\\]\nMore generally, the covariance between two random variables X and Y is defined\n\\[\nE[(X - E[X])(Y - E[Y])]\n\\]"
  },
  {
    "objectID": "notes/lec01-probability.html#exchangeability",
    "href": "notes/lec01-probability.html#exchangeability",
    "title": "Probability",
    "section": "Exchangeability",
    "text": "Exchangeability\n\noffline notes"
  },
  {
    "objectID": "chapterSummaries.html",
    "href": "chapterSummaries.html",
    "title": "Chapter summaries",
    "section": "",
    "text": "Axioms of probability\nFor all sets \\(F\\), \\(G\\) and \\(H\\),\n\n\\(0 = Pr(\\neg H | H) \\leq Pr(F | H) \\leq Pr(H | H) = 1\\)\n\\(Pr(F \\cup G | H) = Pr(F|H) + Pr(G|H) \\text{ if } F \\cap G = \\emptyset\\)\n\\(Pr(F \\cap G | H) = Pr(G | H) Pr(F | G \\cap H)\\)\n\nPartitions and probability\nSuppose \\(\\{ H_1, \\ldots, H_K\\}\\) is a partition of \\(\\mathcal{H}\\), \\(Pr(\\mathcal{H}) = 1\\) and \\(E\\) is some specific event. From the axioms of probability one may prove:\n\nRule of total probability:\n\n\\[\\begin{equation}\n\\sum_{k = 1}^K Pr(H_k) = 1\n\\end{equation}\\]\n\nRule of marginal probability:\n\n\\[\\begin{equation}\n\\begin{aligned}\nPr(E) &= \\sum_{k = 1}^K Pr(E \\cap H_k)\\\\ &= \\sum_{k = 1}^K Pr(E | H_k) Pr(H_k)\n\\end{aligned}\n\\end{equation}\\]\n\nBayes‚Äô theorem:\n\n\\[\\begin{equation}\nPr(H_j | E) = \\frac{Pr(E|H_j) Pr(H_j)}{Pr(E)}\n\\end{equation}\\]\nNote it is often useful to replace the denominator, \\(Pr(E)\\), using the rule of marginal probability.\nIndependence\nTwo events \\(F\\) and \\(G\\) are conditionally independent given \\(H\\) if \\(Pr(F \\cap G |H) = Pr(F|H) Pr(G|H)\\).\n\n\n\nLaw of total expectation \\(E(X) = E(E(X|Y))\\)\nLaw of total variance \\(Var(X) = E(Var(X|Y)) + Var(E(X|Y))\\)\n\n\n\n\n\n\n\nNote\n\n\n\nRemember we can always add conditioning statements e.g.\n\nLaw of total expectation \\(E(X|Z) = E(E(X|Y)|Z)\\)\nLaw of total variance \\(Var(X|Z) = E(Var(X|Y)|Z) + Var(E(X|Y)|Z)\\)"
  },
  {
    "objectID": "chapterSummaries.html#chapter-3",
    "href": "chapterSummaries.html#chapter-3",
    "title": "Chapter summaries",
    "section": "Chapter 3",
    "text": "Chapter 3\n\nDefinitions and conjugacy\n\nBe able to define likelihood, prior, posterior, normalizing constant\n\n\n\n\n\n\n\nDefinition\n\n\n\nA prior \\(p(\\theta)\\) is said to be conjugate to the data generative model \\(p(y|\\theta)\\) if the family of the posterior is necessarily in the same family as the prior. In math, \\(p(\\theta)\\) is conjugate to \\(p(y|\\theta)\\) if\n\\[\np(\\theta) \\in \\mathcal{P} \\implies p(\\theta | y) \\in \\mathcal{P}\n\\]\n\n\n\nExamples of conjugate models: beta-binomial, gamma-Poisson.\n\n\n\nReliability\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(\\Phi\\) be the support of \\(\\theta\\). An interval \\((l(y), u(y)) \\subset \\Phi\\) has 95% posterior coverage if\n\\[\np(l(y) < \\theta < u(y) | y ) = 0.95\n\\]\nInterpretation: after observing \\(Y = y\\), our probability that \\(\\theta \\in (l(y), u(y))\\) is 95%.\nSuch an interval is called 95% posterior confidence interval (CI). It may also sometimes be referred to as a 95% ‚Äúcredible interval‚Äù to distinguish it from a frequentist CI.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nA \\(100 \\times (1-\\alpha)\\)% high posterior density (HPD) region is a set \\(s(y) \\subset \\Theta\\) such that\n\n\\(p(\\theta \\in s(y) | Y = y) = 1 - \\alpha\\)\nIf \\(\\theta_a \\in s(y)\\) and \\(\\theta_b \\not\\in s(y)\\), then \\(p(\\theta_a | Y = y) > p(\\theta_b | Y = y)\\)\n\n\n\n\n\nExponential families\nIf density \\(p(y|\\theta)\\) can be written \\(h(y) c(\\phi) e^{\\phi t(y)}\\) for some transform \\(\\phi = f(\\theta)\\) we can say \\(p(y|\\theta)\\) belongs in the exponential family, and the conjugate prior is \\(p(\\phi | n_0, t_0) =c(\\phi)^{n_0} e^{n_0 t_0 \\phi}\\). Note: the conjugate prior is given over \\(\\phi\\) and we‚Äôd have to transform back if we care about \\(p(\\theta)\\)."
  },
  {
    "objectID": "chapterSummaries.html#chapter-4",
    "href": "chapterSummaries.html#chapter-4",
    "title": "Chapter summaries",
    "section": "Chapter 4",
    "text": "Chapter 4\n\nPredictive distributions\nThe posterior predictive distribution,\n\\[\np(\\tilde{y} | y_1, \\ldots y_n) = \\int p(\\tilde{y}|\\theta) p(\\theta|y_1, \\ldots, y_n)d\\theta\n\\]\nwhen \\(Y | \\theta\\) conditionally iid.\nThe prior predictive distribution,\n\\[\np(\\tilde{y}) = \\int p(\\tilde{y}|\\theta) p(\\theta)d\\theta.\n\\]\nNotice both the posterior and prior predictive distributions are represented as integrals. Integrals are expectations. This means we can use Monte Carlo integration to approximate.\nTo approximate the posterior predictive distribution:\n\nsample from the posterior of theta, \\(p(\\theta|y_1,\\ldots y_n)\\)\nsample from data generative model \\(p(\\tilde{y}|\\theta)\\) for the values of theta sampled in (1).\n\nTo approximate the prior predictive distribution:\n\nsample from the prior of theta, \\(p(\\theta)\\)\nsample from the data generative model \\(p(\\tilde{y}|\\theta)\\) for the values of theta sampled in (1).\n\n\n\nMonte Carlo error\nSince Monte Carlo approximation can be viewed as a sample mean approximating an expected value, CLT applies.\nMore specifically, if \\(\\theta_i |\\vec{y}\\) iid with mean \\(\\theta\\) and finite variance \\(\\sigma^2\\), for \\(i \\in \\{1, \\ldots, N\\}\\), then the sample mean\n\\[\n\\bar{\\theta} \\sim N(\\theta, \\frac{\\sigma^2}{N} ).\n\\]\nand Monte Carlo estimates converge at a rate \\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)\\) regardless of the dimension of the integral!\n\n\nThe sampling view\nIf we have a posterior \\(p(\\theta | y_1, \\ldots y_n)\\) that we can sample from and we want some summary of the posterior‚Ä¶ e.g.¬†we want\n\n\\(p(\\theta < a)\\)\nquantiles of the posterior , or\nthe posterior of some transform \\(f(\\theta)\\),\n\nthen we can simply sample from the posterior to obtain an empirical approximation of the posterior and then report the empirical quantity of interest. This is also called Monte Carlo approximation.\nThe procedure can be written:\n\nsample from the posterior \\(p(\\theta |y_1, \\ldots y_n)\\) some large number of times and then\ncompute the quantity of interest"
  },
  {
    "objectID": "chapterSummaries.html#chapter-5",
    "href": "chapterSummaries.html#chapter-5",
    "title": "Chapter summaries",
    "section": "Chapter 5",
    "text": "Chapter 5\n\nConjugate prior to the normal model\nIf\n\\[\n\\begin{aligned}\nY_i | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\n\\theta | \\sigma^2 & \\sim N(\\mu_0, \\sigma^2/\\kappa_0)\\\\\n\\frac{1}{\\sigma^2} &\\sim \\text{gamma}(\\frac{\\nu_0}{2}, \\frac{\\nu_0}{2} \\sigma_0^2)\n\\end{aligned}\n\\]\nthen\n\\[\n\\begin{aligned}\n\\theta | \\sigma^2, y_1,\\ldots y_n &\\sim \\text{normal}\\\\\n\\sigma^2 | y_1,\\ldots y_n &\\sim \\text{gamma}\n\\end{aligned}\n\\]\nand since\n\\[\n\\begin{aligned}\np(\\theta, \\sigma^2 | y_1, \\ldots y_n) &= p(\\theta |\\sigma^2, y_1,\\ldots y_n) p(\\sigma^2 | y_1,\\ldots y_n),\n\\end{aligned}\n\\]\nwe can sample directly from the joint posterior by sampling from \\(p(\\sigma^2 | y_1,\\ldots y_n)\\) and then from \\(p(\\theta | \\sigma^2, y_1,\\ldots y_n)\\).\n\n\nEstimators\nBe able to define and compute the bias, variance and MSE of an estimator.\n\n\n\n\n\n\nDefinition\n\n\n\nBias is the the difference between the expected value of the estimator and the true value of the parameter.\n\n\\(E[\\hat{\\theta} | \\theta = \\theta_ 0] - \\theta_0\\) is the bias of \\(\\hat{\\theta}\\).\nIf \\(E[\\hat{\\theta} | \\theta = \\theta_0] = \\theta_0\\), then we say \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\).\nIf \\(E[\\hat{\\theta} | \\theta = \\theta_0] \\neq \\theta_0\\), then we say \\(\\hat{\\theta}\\) is a biased estimator of \\(\\theta\\).\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nRecall: variance is average squared distance from the mean. In this context, the variance of an estimator refers to the variance of the sampling distribution of \\(\\hat{\\theta}\\). We write this mathematically,\n\\[\nVar[\\hat{\\theta} | \\theta_0] = E[(\\hat{\\theta} - m)^2 |\\theta_0]\n\\]\nwhere \\(m = E[\\hat{\\theta}|\\theta_0]\\).\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nMean squared error (MSE) is (as the name suggests) the expected value of the squared difference between the estimator and true parameter value. Equivalently, MSE is the variance plus the square bias of the estimator.\n\\[\n\\begin{aligned}\nMSE[\\hat{\\theta}|\\theta_0] &= E[(\\hat{\\theta} - \\theta_0)^2 | \\theta_0]\\\\\n&= Var[\\hat{\\theta} | \\theta_0] + Bias^2[\\hat{\\theta}|\\theta_0]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapterSummaries.html#chapter-6",
    "href": "chapterSummaries.html#chapter-6",
    "title": "Chapter summaries",
    "section": "Chapter 6",
    "text": "Chapter 6\n\nGibbs sampling procedure\nHow can we look at a joint posterior, e.g.¬†\\(p(\\theta_1,\\ldots \\theta_p | y_1,\\ldots y_n)\\), if we have non-conjugate priors?\nWell if we do have the full conditionals, \\(p(\\theta_i | \\theta_{-i}, y_1,\\ldots y_n)\\) then we can sample from the joint posterior via Gibbs sampling. Note: \\(\\theta_{-i}\\) denotes \\(\\{\\theta\\} \\backslash \\theta_i\\), i.e.¬†the set of all theta except \\(\\theta_i\\).\nGibbs sampling proceeds:\nPick a starting point \\(\\theta_2^{(0)}, \\ldots \\theta_p^{(0)}\\), then for s in 1:S,\n\nSample \\(\\theta_1^{(s)} \\sim p(\\theta_1 | \\theta_{2}^{(s-1)}, \\ldots, \\theta_{p}^{(s-1)}, y_1,\\ldots y_n)\\)\nSample \\(\\theta_2^{(s)} \\sim p(\\theta_2 | \\theta_{1}^{(s)}, \\theta_{3}^{(s-1)} \\ldots, \\theta_{p}^{(s-1)}, y_1,\\ldots y_n)\\)\n\n\\(\\vdots\\)\n\nSample \\(\\theta_p^{(s)} \\sim p(\\theta_2 | \\theta_{1}^{(s)}, \\ldots, \\theta_{p-1}^{(s)}, y_1,\\ldots y_n)\\)\n\nIt follows that we have a sequence of dependent samples from the joint posterior. The sequence \\(\\{\\theta^{(s)}\\}\\) is called a Markov chain.\n\\[\n\\frac{1}{S} \\sum_{s=1}^S g(\\theta^{(s)})  \\rightarrow E[g(\\theta)]\n\\]\nGibbs sampling is a form of Markov chain Monte Carlo (MCMC).\n\n\nMCMC diagnostics\nEffective sample size (ESS), autocorrelation, and traceplots are diagnostic tools we use to assess how well our Markov chain approximates the posterior. You should be able to define these terms and interpret their output. See this lecture for details.\nSpecifically, ESS is the number of independent Monte Carlo samples necessary to give the same precision as the MCMC samples. Typically, ESS is a criterion used to figure out how many samples to generate, i.e.¬†how long to run your Markov chain."
  },
  {
    "objectID": "chapterSummaries.html#chapter-7",
    "href": "chapterSummaries.html#chapter-7",
    "title": "Chapter summaries",
    "section": "Chapter 7",
    "text": "Chapter 7\n\nDensity\nWe say a \\(p\\) dimensional vector \\(\\boldsymbol{Y}\\) has a multivariate normal distribution if its sampling density is given by\n\\[\np(\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp\\{\n-\\frac{1}{2}(\\boldsymbol{y}-\\boldsymbol{\\theta})^T \\Sigma^{-1} (\\boldsymbol{y}- \\boldsymbol{\\theta})\n\\}\n\\]\nwhere\n\\[\n\\boldsymbol{y}=  \\left[ {\\begin{array}{cc}\n   y_1 \\\\\n   y_2\\\\\n   \\vdots\\\\\n   y_p\n  \\end{array} } \\right]\n  ~~~\n   \\boldsymbol{\\theta}= \\left[ {\\begin{array}{cc}\n   \\theta_1 \\\\\n   \\theta_2\\\\\n   \\vdots\\\\\n   \\theta_p\n  \\end{array} } \\right]\n  ~~~\n  \\Sigma =\n  \\left[ {\\begin{array}{cc}\n   \\sigma_1^2 & \\sigma_{12}& \\ldots & \\sigma_{1p}\\\\\n   \\sigma_{12} & \\sigma_2^2 &\\ldots & \\sigma_{2p}\\\\\n   \\vdots & \\vdots & & \\vdots\\\\\n   \\sigma_{1p} & \\ldots & \\ldots & \\sigma_p^2\n  \\end{array} } \\right].\n\\]\n\nKey facts\n\n\\(\\boldsymbol{y}\\in \\mathbb{R}^p\\) ; \\(\\boldsymbol{\\theta}\\in \\mathbb{R}^p\\); \\(\\Sigma > 0\\)\n\\(E[\\boldsymbol{y}] = \\boldsymbol{\\theta}\\)\n\\(V[\\boldsymbol{y}] = E[(\\boldsymbol{y}- \\boldsymbol{\\theta})(\\boldsymbol{y}- \\boldsymbol{\\theta})^T] = \\Sigma\\)\nMarginally, \\(y_i \\sim N(\\theta_i, \\sigma_i^2)\\).\nIf \\(\\boldsymbol{\\theta}\\) is a MVN random vector, then the kernel is \\(\\exp\\{-\\frac{1}{2} \\boldsymbol{\\theta}^T A \\boldsymbol{\\theta}+ \\boldsymbol{\\theta}^T \\boldsymbol{b} \\}\\). The mean is \\(A^{-1}\\boldsymbol{b}\\) and the covariance is \\(A^{-1}\\).\n\n\n\n\nsemi-conjugate prior for \\(\\boldsymbol{\\theta}\\)\nIf\n\\[\n\\begin{aligned}\n\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma &\\sim MVN(\\boldsymbol{\\theta}, \\Sigma),\\\\\n\\boldsymbol{\\theta}&\\sim MVN(\\mu_0, \\Lambda_0),\n\\end{aligned}\n\\]\nthen\n\\[\n\\boldsymbol{\\theta}| \\boldsymbol{y}, \\Sigma \\sim MVN\n\\]\n\n\nsemiconjugate prior for \\(\\Sigma\\)\nIf\n\\[\n\\begin{aligned}\n\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma &\\sim MVN(\\boldsymbol{\\theta}, \\Sigma),\\\\\n\\Sigma &\\sim \\text{inverse-Wishart}(\\nu_0, S_0^{-1}),\n\\end{aligned}\n\\]\nthen\n\\[\n\\Sigma | \\boldsymbol{y}, \\boldsymbol{\\theta}\\sim \\text{inverse-Wishart}\n\\]\n\n\nWishart as sum of squares matrix\nFor a given \\(\\nu_0\\) and and a \\(p \\times p\\) covariance matrix \\(S_0\\), we can generate samples from a MVN by the following procedure:\n\nsample \\(\\boldsymbol{z}_1, \\ldots \\boldsymbol{z}_{\\nu_0} \\sim \\text{ i.i.d. } MVN(\\mathbf{0}, S_0)\\)\ncalculate \\(\\mathbf{Z}^T \\mathbf{Z} = \\sum_{i =1}^{\\nu_0} \\boldsymbol{z}_i \\boldsymbol{z}_i^T\\).\n\nIt follows that \\(\\mathbf{Z}^T \\mathbf{Z} > 0\\) and symmetric. \\(E[\\mathbf{Z}^T \\mathbf{Z}] = \\nu_0 S_0\\)"
  },
  {
    "objectID": "chapterSummaries.html#chapter-8",
    "href": "chapterSummaries.html#chapter-8",
    "title": "Chapter summaries",
    "section": "Chapter 8",
    "text": "Chapter 8\nBe able to write a hierarchical model. Review and be able to explain all aspects of the example here."
  },
  {
    "objectID": "hw/hw01.html",
    "href": "hw/hw01.html",
    "title": "Homework 1",
    "section": "",
    "text": "Let \\(X_i \\in \\mathcal{X}\\) for all \\(i \\in \\{1, 2, \\ldots\\}\\) and suppose our belief model for \\(\\mathbf{X} = \\{ X_1, \\ldots X_n \\}\\) is exchangeable for all \\(n\\). Show, using de Finetti‚Äôs theorem, that for all \\(i \\neq j\\),\n\\[\nCov(X_i, X_j) \\geq 0 \\text{ and }\n\\]\n\\[\nCorr(X_i, X_j) \\geq 0.\n\\]"
  },
  {
    "objectID": "hw/hw01.html#exercise-2",
    "href": "hw/hw01.html#exercise-2",
    "title": "Homework 1",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet \\(X, Y, Z\\) be random variables with joint density (discrete or continuous)\n\\[\np(x, y, z) \\propto f(x,z) g(y, z) h(z).\n\\]\nShow that\n\n\\(p(x | y, z) \\propto f(x, z)\\), i.e.¬†\\(p(x | y, z)\\) is a function of \\(x\\) and \\(z\\).\n\\(p(y | x, z) \\propto g(y, z)\\), i.e.¬†\\(p(y | x, z)\\) is a function of \\(y\\) and \\(z\\).\n\\(X\\) and \\(Y\\) are conditionally independent given \\(Z\\)."
  },
  {
    "objectID": "hw/hw01.html#exercise-3",
    "href": "hw/hw01.html#exercise-3",
    "title": "Homework 1",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe number of particles \\(Y\\) emitted from a rock sample depends on the unknown amount \\(\\theta\\) of the sample that is radioactive. For each possible value of \\(\\theta\\),\n\\[\nPr(Y = y | \\theta) = \\theta^y e^{-\\theta} / y!\n\\]\nfor each \\(y \\in \\{0, 1, 2, \\ldots \\}\\). Suppose it is known that the rock is one of three possible types \\(A\\), \\(B\\), or \\(C\\), each with a particular value of \\(\\theta\\), that is, \\(\\theta \\in \\{\\theta_A, \\theta_B, \\theta_C \\}\\) where \\(\\theta_A = 1.1\\), \\(\\theta_B = 3.2\\), and \\(\\theta_C = 4.5\\).\n\nMake a graph of \\(Pr(Y = y | \\theta)\\) as a function of \\(y\\) for each of the three possible values of \\(\\theta\\) (for some reasonable range of \\(y\\)-values.\nNow suppose that the rock is of type \\(A\\), \\(B\\), or \\(C\\) with probabilities \\(.4\\), \\(.3\\) and \\(.3\\) respectively. Compute the marginal probability of \\(Y\\), that is,\n\\(Pr(Y = y) = Pr(Y = y | \\theta_A) Pr(\\text{type} = A) + Pr(Y = y | \\theta_B) Pr(\\text{type} = B) + Pr(Y = y | \\theta_C) Pr(\\text{type} = C)\\)\nPlot this as a function of \\(y\\) and compare the graph to the three graphs from part a.\nSuppose it is observed that \\(Y = 4\\). Using the rules of conditional probability, compute the probabilities of each type conditional on \\(Y = 4\\), that is, compute \\(Pr(\\theta = \\theta_X | Y = 4)\\) for each \\(X \\in \\{A, B, C \\}\\). Compare these probabilities to the prior probabilities (.4, .3, .3)."
  },
  {
    "objectID": "hw/hw01.html#exercise-4",
    "href": "hw/hw01.html#exercise-4",
    "title": "Homework 1",
    "section": "Exercise 4",
    "text": "Exercise 4\nLet \\(Y_1, \\ldots Y_n\\) be binary random variables that are conditionally independent given a value of a parameter \\(\\theta\\), so that \\(Pr(Y_i = 1 | \\theta) = \\theta = 1 - Pr(Y_i = 0 | \\theta)\\).\n\nLet \\(y_1, \\ldots y_n\\) be a binary sequence so that \\(y_i \\in \\{0, 1 \\}\\), for each \\(i = 1, \\ldots, n\\). Using the rules of probability, derive a formula for \\(Pr(Y_1 = y_1, \\ldots, Y_n = y_n | \\theta)\\) as a function of \\(y_1, \\ldots y_n\\) and \\(\\theta\\). Simplify as much as possible.\nLet \\(X = \\sum_{i=1}^n Y_i\\), what is the probability \\(p(X = x | \\theta)\\) for any \\(x \\in \\{0, 1, \\ldots n\\}\\)?\nCompute and compare \\(E(Y_i|\\theta)\\), \\(Var(Y_i|\\theta)\\) to \\(E(X|\\theta)\\) and \\(Var(X|\\theta)\\).\nUsing calculus or otherwise, for a given value of \\(x\\), find ‚Äú\\(\\theta_{MLE}\\)‚Äù, the value of \\(\\theta\\) that maximizes \\(Pr(X = x|\\theta)\\). This value is called the ‚Äúmaximum likelihood estimator‚Äù or MLE, of \\(\\theta\\)."
  },
  {
    "objectID": "notes/exam-notes.html",
    "href": "notes/exam-notes.html",
    "title": "Exam notes",
    "section": "",
    "text": "A random variable \\(X \\in \\mathbb{R}\\) has a \\(N(\\theta, \\sigma^2)\\) distribution if \\(\\sigma^2 > 0\\) and\n\\(p(x | \\theta, \\sigma^2) = (2 \\pi \\sigma^2)^{-\\frac{1}{2}} e^{-\\frac{1}{2\\sigma^2}(x - \\theta)^2} \\ \\ \\ \\text{ for } -\\infty < x < \\infty.\\)\n\n\n\nA random vector \\(X \\in \\mathbb{R}^p\\) has a \\(MVN(\\theta, \\Sigma)\\) distribution if \\(\\Sigma > 0\\) and\n\\(p(x| \\theta, \\Sigma) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp\\{ -\\frac{1}{2}(x - \\theta)^T \\Sigma^{-1} (x - \\theta) \\}\\)\n\n\n\nA random variable \\(X \\in (0, \\infty)\\) has a gamma(a,b) distribution if \\(a > 0, b > 0\\) and\n\\(p(x |a,b) = \\frac{b^a}{\\Gamma(a)} x^{a - 1} e^{-bx} \\ \\ \\ \\text{ for } x > 0.\\)\n\\(E[X | a, b] = a/b\\), \\(Var[X | a,b] = a / b^2\\)\n\n\n\nA random variable \\(X \\in (0, \\infty)\\) has an inverse-gamma(a,b) distribution if 1/X has a gamma(a,b) distribution. If \\(X\\) is inverse-gamma(a,b) then the density of X is\n\\(p(x|a,b) = \\frac{b^a}{\\Gamma(a)} x^{-a-1} e^{-b/x} \\ \\ \\ \\text{ for } x > 0.\\)\n\\(E[X|a,b] = \\frac{b}{a-1}\\) if \\(a>=1\\), \\(\\infty\\) if \\(0<a<1\\)\n\\(Var[X|a,b] = \\frac{b^2}{(a-1)^2(a-2)}\\) if \\(a\\geq2\\), \\(\\infty\\) if \\(0<a<2\\)\n\n\n\nA random \\(p \\times p\\) matrix \\(\\Sigma\\) has an inverse-Wishart distribution if \\(p(\\Sigma | \\nu_0, S_0^{-1}) \\propto |\\Sigma|^{-(\\nu_0 + p + 1)/2} \\times \\exp \\{ -\\frac{1}{2}tr(S_0 \\Sigma^{-1})\\}\\).\n\nthe support is \\(\\Sigma > 0\\) and \\(\\Sigma\\) symmetric \\(p \\times p\\) matrix. \\(\\nu_0 \\in \\mathbb{N}^+\\) and \\(\\nu_0 \\geq p\\). \\(S_0\\) is a \\(p \\times p\\) symmetric positive definite matrix.\n\\(E[\\Sigma^{-1}] = \\nu_0 S_0^{-1}\\) and \\(E[\\Sigma] = \\frac{1}{\\nu_0 - p - 1} S_0\\).\n\n\n\n\nA random variable \\(X \\in \\{0, 1, \\ldots, n\\}\\) has a binomial\\((n, \\theta)\\) distribution if \\(\\theta \\in [0, 1]\\) and\n\\(p(X = x| \\theta, n) = {n \\choose x} \\theta^x (1- \\theta)^{n-x} \\ \\ \\ \\text{ for } x\\in \\{0, 1, \\ldots, n \\}\\)\n\\(E[X|\\theta] = n\\theta\\), \\(Var[X|\\theta] = n\\theta(1-\\theta)\\)\n\n\n\nA random variable \\(X \\in [0, 1]\\) has a beta(a,b) distribution if \\(a > 0, b > 0\\) and\n\\(p(x|a,b) = \\frac{\\Gamma(a + b)}{\\Gamma(a)\\Gamma(b)} x^{a-1} (1-x)^{b-1} \\ \\ \\ \\text{ for } 0 \\leq x \\leq 1.\\)\n\\(E[X|a,b] = \\frac{a}{a + b}\\), \\(Var[X|a,b] = \\frac{ab}{(a + b + 1)(a + b)^2}\\)\n\n\n\nA random variable \\(X \\in \\{0, 1, 2, \\ldots \\}\\) has a Poisson(\\(\\theta\\)) distribution if \\(\\theta > 0\\) and\n\\(p(X = x | \\theta) = \\theta^x \\frac{e^{-\\theta}}{x!} \\ \\ \\ \\text{ for } x \\in \\{0, 1, 2, \\ldots\\}\\)\n\\(E[X|\\theta] = \\theta\\), \\(Var[X|\\theta] = \\theta\\)\n\n\n\nA random variable \\(X \\in [0, \\infty)\\) has a exponential(\\(\\theta\\)) distribution if \\(\\theta >0\\) and\n\\(p(x | \\theta) = \\theta e^{-\\theta x}\\)\n\\(E[X|\\theta] = \\frac{1}{\\theta}\\), \\(Var[X|\\theta] = \\frac{1}{\\theta^2}\\)"
  },
  {
    "objectID": "hw/hw02.html",
    "href": "hw/hw02.html",
    "title": "Homework 2",
    "section": "",
    "text": "Compute the following integrals using the kernel trick discussed in class.\n\n\\(\\int_{0}^{\\infty} \\sigma^{x-1} e^{-b \\sigma} d\\sigma\\)\n\\(\\int_{0}^1 \\alpha \\theta^{\\alpha} (1 - \\theta)^{\\beta - 1} d\\theta\\)\n\\(\\int_{-\\infty}^\\infty x e^{-(x-3)^2} dx\\)"
  },
  {
    "objectID": "hw/hw02.html#exercise-1",
    "href": "hw/hw02.html#exercise-1",
    "title": "Homework 2",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet \\(Y_1, Y_2 | \\theta\\) be i.i.d. binary(\\(\\theta\\)), so that \\(p(y_1, y_2 | \\theta) = \\theta ^{y_1 + y_2} (1- \\theta) ^{2 - y_1 - y_2}\\) and let \\(\\theta \\sim \\text{beta}(\\eta, \\eta)\\)\n\nCompute \\(E~Y_i\\) and \\(Var~Y_i\\) (the mean and variance of \\(Y_i\\) unconditional on \\(\\theta\\)) as a function of \\(\\eta\\)\nCompute \\(E~Y_1 Y_2\\), which is the same as \\(p(Y_1 = 1, Y_2 = 1)\\) unconditional on \\(\\theta\\). You can do this with help from the formula on page 33 of the book.\nUsing the terms you have calculated above, make a graph of the correlation between \\(Y_1\\) and \\(Y_2\\) as a function of \\(\\eta\\).\nInterpreting \\(\\eta\\) as how confident you are that \\(\\theta\\) is near \\(\\frac{1}{2}\\), and interpreting \\(Cor(Y_1, Y_2)\\) as how much information \\(Y_1\\) and \\(Y_2\\) provide about each other, explain in words why the correlation changes as a function of \\(\\eta\\)."
  },
  {
    "objectID": "hw/hw02.html#exercise-2",
    "href": "hw/hw02.html#exercise-2",
    "title": "Homework 2",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet \\(Y_1, Y_2 | \\theta\\) be i.i.d. binary(\\(\\theta\\)), so that \\(p(y_1, y_2 | \\theta) = \\theta ^{y_1 + y_2} (1- \\theta) ^{2 - y_1 - y_2}\\) and let \\(\\theta \\sim \\text{beta}(\\eta, \\eta)\\)\n\nCompute \\(E~Y_i\\) and \\(Var~Y_i\\) (the mean and variance of \\(Y_i\\) unconditional on \\(\\theta\\)) as a function of \\(\\eta\\)\nCompute \\(E~Y_1 Y_2\\), which is the same as \\(p(Y_1 = 1, Y_2 = 1)\\) unconditional on \\(\\theta\\). Hint: \\(Y_1\\) and \\(Y_2\\) are conditionally i.i.d., see law of total expectation.\nUsing the terms you have calculated above, make a graph of the correlation between \\(Y_1\\) and \\(Y_2\\) as a function of \\(\\eta\\).\nInterpreting \\(\\eta\\) as how confident you are that \\(\\theta\\) is near \\(\\frac{1}{2}\\), and interpreting \\(Cor(Y_1, Y_2)\\) as how much information \\(Y_1\\) and \\(Y_2\\) provide about each other, explain in words why the correlation changes as a function of \\(\\eta\\)."
  },
  {
    "objectID": "hw/hw02.html#exercise-3",
    "href": "hw/hw02.html#exercise-3",
    "title": "Homework 2",
    "section": "Exercise 3",
    "text": "Exercise 3\nSuppose \\(n\\) individuals volunteer to count birds in a forest. Let \\(Y_i\\) be the number of birds counted by individual \\(i\\), and let \\(x_i\\) be the number of hours spent in the forest by volunteer \\(i\\). We will model the data \\(Y_1, \\ldots Y_n\\) as being independent given \\(\\theta\\), but not identically distributed. Specifically, our model is that \\(Y_i | \\theta \\sim \\text{Pois}(\\theta x_i)\\), independently for \\(i = 1, \\ldots n\\).\n\nCompute \\(E~Y_i | \\theta\\) and explain what \\(\\theta\\) represents.\nWrite out a formula for the joint pdf \\(p(y_1, \\ldots y_n |\\theta)\\) and simplify as much as possible. Find the MLE, that is, the value of \\(\\theta\\) that maximizes \\(p(y_1, \\ldots y_n | \\theta)\\). Explain why it makes sense.\nLet \\(\\theta \\sim \\text{gamma}(a, b)\\). Write down the posterior \\(p(\\theta | y_1,\\ldots y_n)\\) and find a formula for the posterior mode of \\(\\theta\\). Compare to the MLE."
  },
  {
    "objectID": "hw/hw02.html#exercise-3-1",
    "href": "hw/hw02.html#exercise-3-1",
    "title": "Homework 2",
    "section": "Exercise 3",
    "text": "Exercise 3\nLet \\(\\theta_1\\) be the prevalence of a rare allele among people with Alzheimer‚Äôs disease, and let \\(\\theta_2\\) be the prevalence among people without the disease. To estimate \\(\\theta_1\\) and \\(\\theta_2\\), a sample of \\(n_1 = 19\\) Alzheimer‚Äôs patients and \\(n_2 = 176\\) control subjects are genotyped for the presence of the allele. Let \\(Y_1\\) and \\(Y_2\\) be the number of people in the two samples who have the allele. We will model \\(Y_1\\) and \\(Y_2\\) as independent with \\(Y_1 | \\theta_1 \\sim \\text{binomial}(n_1, \\theta_1)\\) and \\(Y_2 | \\theta_2 \\sim \\text{binomial}(n_2, \\theta_2)\\). Prior studies suggest that \\(\\theta_2 \\sim \\text{beta}(2, 30)\\) is a reasonable prior distribution for \\(\\theta_2\\). For now, we will use the same prior distribution for \\(\\theta_1\\). The study is performed and the data are that \\(Y_1 = 1\\) and \\(Y_2 = 16\\).\n\nState the posterior distributions of \\(\\theta_1\\) and \\(\\theta_2\\). Plot the posterior densities together on a single graph with the prior density, and compare all three curves with words.\nCompute the posterior mean and a 95% posterior interval for each of \\(\\theta_1\\) and \\(\\theta_2\\).\nWith a picture, with words, or mathematically, try to describe different kind of joint prior distribution for \\(\\theta_1\\) and \\(\\theta_2\\) that represents the \\(\\theta_1\\) and \\(\\theta_2\\) are close to each other, but highly uncertain."
  },
  {
    "objectID": "hw/hw02.html#exercise-4",
    "href": "hw/hw02.html#exercise-4",
    "title": "Homework 2",
    "section": "Exercise 4",
    "text": "Exercise 4\nData from the study described in exercise 3 can be downloaded from the course website using the code provided below.\n\nreadr::read_csv(\"https://sta360-fa24.github.io/data/bird-counts.csv\")\n\nIn this problem, we will examine the posterior distribution of \\(\\theta\\) given these data, under a prior distribution for \\(\\theta\\) having density of the form \\(p(\\theta) = c \\theta^{a-1} e^{-b\\theta}\\), where \\(c\\) is a constant that depends on \\(a\\) and \\(b\\) but not \\(\\theta\\). For this problem, we will set \\(a = 2\\) and \\(b = 1/5\\).\n\nMake a plot of \\(p(\\theta)\\) for \\(\\theta \\in (0, 50)\\) as follows: Compute \\(\\theta^{a -1} e^{-b\\theta}\\) on an evenly-spaced grid of 1000 \\(\\theta\\)-values from 0 to 50. Put the results of the computation into a vector of length 1000, then divide the vector by its sum. This vector is a discrete pdf that approximates the continuous density \\(p(\\theta)\\).\nCompute the prior expectation \\(E \\theta\\) using this discrete approximiation.\nThe posterior density of \\(\\theta\\) may be expressed as \\(p(\\theta | y_1, \\ldots y_n) = \\tilde{c} p(\\theta) p(y_1, \\ldots y_n | \\theta)\\), where \\(\\tilde{c}\\) does not depend on \\(\\theta\\). As in part (a), make a discrete approximation to \\(p(\\theta | y)\\), and plot the results along with \\(p(\\theta)\\) and discuss the change from prior to posterior density. Also compare the prior and posterior expectations.\n\nHint: \\(p(\\theta | y_1,\\ldots y_n)\\) is the kernel of a well-known density. You can use a built in R function to help you create a discrete approximation to \\(p(\\theta | y_1,\\ldots y_n)\\)."
  },
  {
    "objectID": "notes/additionalRegressionNotes.html",
    "href": "notes/additionalRegressionNotes.html",
    "title": "Additional regression notes",
    "section": "",
    "text": "Setup:\n\\[\ny_i = z_1 \\beta_1 x_{i, 1} + \\cdots + z_p \\beta_p x_{i, p} + \\epsilon_i\n\\]\n\n\\(y_i \\in \\mathbb{R}\\)\n\\(\\beta_i \\in \\mathbb{R}\\)\n\\(z_i \\in \\{0, 1\\}\\)\n\nThe \\(z_j\\)‚Äôs indicate which regression coefficients are non-zero.\n\n\n\n\n\n\nNote\n\n\n\nNotice that every set of values \\(\\mathbf{z} = \\{z_1, \\ldots z_p \\}\\) corresponds to a different model. So a prior \\(p(z)\\) may be thought of as a prior distribution over models.\n\n\nBayesian model selection proceeds by obtaining a posterior distribution for \\(\\mathbf{z}\\):\n\\[\np(\\mathbf{z}| \\mathbf{y}, \\mathbf{X}) = \\frac{p(\\mathbf{z}) p(\\mathbf{y}| \\mathbf{X}, \\mathbf{z})}{\\sum_\\mathbf{\\tilde{z}} p(\\mathbf{\\tilde{z}}) p(\\mathbf{y}| \\mathbf{X}, \\mathbf{\\tilde{z}})}\n\\]\nAlternatively, we may prefer to compare any two models with the posterior odds:\n\\[\n\\text{odds}(\\mathbf{z}_a, \\mathbf{z}_b | \\mathbf{y}, \\mathbf{X}) = \\frac{p(\\mathbf{z}_a | \\mathbf{y}, \\mathbf{X})}{p(\\mathbf{z}_b | \\mathbf{y}, \\mathbf{X})} = \\frac{p(\\mathbf{z}_a)}{p(\\mathbf{z}_b)} \\times \\frac{p(\\mathbf{y}| \\mathbf{X}, \\mathbf{z}_a)}{p(\\mathbf{y}| \\mathbf{X}, \\mathbf{z}_b)}\n\\]\nNotice that when examining the odds of one model vs another, we avoid computing the denominator of \\(p(\\mathbf{z}| \\mathbf{y}, \\mathbf{X})\\), and thereby exhaustively computing the probability of every model.\n\n\n\n\n\n\nDefinitions\n\n\n\n\n\\(\\frac{p(\\mathbf{z}_a)}{p(\\mathbf{z}_b)}\\) is called the ‚Äúprior odds‚Äù.\n\\(\\frac{p(\\mathbf{y}| \\mathbf{X}, \\mathbf{z}_a)}{p(\\mathbf{y}| \\mathbf{X}, \\mathbf{z}_b)}\\) is called the ‚ÄúBayes factor‚Äù i.e.¬†how much the data favor model \\(\\mathbf{z}_a\\) over model \\(\\mathbf{z}_b\\).\n\n\n\nThis formulation above elicits a need to compute \\(p(\\mathbf{y}| \\mathbf{X}, \\mathbf{z})\\). To compute this in closed form, we will choose a few special priors (given a MVN data generative process).\n\\[\n\\begin{aligned}\n\\mathbf{y}| \\mathbf{X}, \\mathbf{z}, \\sigma^2 &\\sim MVN(X \\text{diag}(\\mathbf{z})\\beta, \\sigma^2 I)\\\\\n\\beta_z | \\mathbf{X}_z, \\sigma^2 &\\sim MVN(\\boldsymbol{0}, g\\sigma^2 [\\mathbf{X}_z^T \\mathbf{X}_z]^{-1})\\\\\n1/\\sigma^2 &\\sim \\text{gamma}(\\nu_0/2, \\nu_0 \\sigma_0^2 /2)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/lec03-Poisson-gamma-exp-families.html",
    "href": "notes/lec03-Poisson-gamma-exp-families.html",
    "title": "NBA Assists",
    "section": "",
    "text": "See libraries used in these notes\nlibrary(tidyverse)\nlibrary(latex2exp)"
  },
  {
    "objectID": "notes/lec03-Poisson-gamma-exp-families.html#data-boston-celtics",
    "href": "notes/lec03-Poisson-gamma-exp-families.html#data-boston-celtics",
    "title": "NBA Assists",
    "section": "Data: Boston Celtics",
    "text": "Data: Boston Celtics\nIn basketball, an ‚Äúassist‚Äù is attributed to a player that passes the ball to a teammate in a way that directly leads to a basket. The data below was scraped from espn.com September 2024. Each row of the data set is an individual Boston Celtics player during a particular game of the 2023-2024 season. The AST records the number of assists made by the player in the particular game. MIN records the number of minutes each player played in a particular game. vs records the opposing team.\n\ndata samplecode\n\n\n\n\n# A tibble: 10 √ó 4\n   player                 AST   MIN vs                   \n   <chr>                <dbl> <dbl> <chr>                \n 1 Payton Pritchard PG      4  23.5 Golden State Warriors\n 2 Derrick White PG         4  33.5 Dallas Mavericks     \n 3 Jaylen Brown SG          4  34   Brooklyn Nets        \n 4 Luke Kornet C            2  21.3 Chicago Bulls        \n 5 Xavier Tillman F *       2  15   Dallas Mavericks     \n 6 Luke Kornet C            1  19   Dallas Mavericks     \n 7 Kristaps Porzingis C     1  23.7 Brooklyn Nets        \n 8 Kristaps Porzingis C     1  30   Chicago Bulls        \n 9 Oshae Brissett SF        0   4   Dallas Mavericks     \n10 Luke Kornet C            0  11   Golden State Warriors\n\n\n\n\n\nset.seed(360)\nyx = read_csv(\"../data/BostonCeltics_Assists_23-24_season.csv\")\n\nyx %>%\n  slice_sample(n = 10) %>%\n  arrange(desc(AST))\n\n\n\n\nQuestion: how many assists on average do we expect a Celtics player to make per minute played?"
  },
  {
    "objectID": "notes/lec03-Poisson-gamma-exp-families.html#bayesian-framework",
    "href": "notes/lec03-Poisson-gamma-exp-families.html#bayesian-framework",
    "title": "NBA Assists",
    "section": "Bayesian framework",
    "text": "Bayesian framework\n\nDefine a data generative model and write down the likelihood (often assuming exchangeability and invoking de Finetti‚Äôs theorem)\nChoose a prior distribution for all things unknown\nCompute or approximate the posterior\nMake inference\n\n\nData-generative model\nTo write down a data generative model, let‚Äôs assume players accumulate assists \\(y\\) at some per-minute rate, \\(\\theta\\). We will further assume that the expected number of assists by a player in a given game is then \\(\\theta x\\) where \\(x\\) is the number of minutes played.\nGiven \\(y\\) takes integer values \\(\\{0, 1, 2, \\ldots \\}\\), a Poisson distribution might make sense. If \\(Y | \\lambda\\) is Poisson\\((\\lambda)\\), then\n\\[\np(y |\\lambda) = \\frac{(\\lambda)^y e^{-\\lambda}}{y!}\n\\]\nHere, \\(\\lambda = \\theta x\\).\nAssuming this conditionally independent model for generating \\(y\\)s, we write the likelihood,\n\\[\np(y_1,\\ldots y_n | \\theta) = \\prod_{i = 1}^n \\frac{(\\theta x_i)^{y_i} e^{-\\theta x_i}}{y_i!}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nWhat are some assumptions are we making about assists in the data generative model above?\n\n\n\n\n\nPrior beliefs\n\nNotice the support: \\(\\theta > 0\\)\nA gamma prior may be suitable\n\n\\[\n\\begin{aligned}\n\\theta &\\sim gamma(a, b)\\\\\np(\\theta | a, b) &= \\frac{b^{a}}{\\Gamma(a)} \\theta^{a - 1} e^{-b \\theta}\n\\end{aligned}\n\\]\n\n\nCompute the posterior\n\nExerciseSolution\n\n\nCompute the posterior.\n\n\n\\[\n\\begin{aligned}\n\\theta | y_1,\\ldots y_n &\\sim gamma(\\alpha, \\beta)\\\\\n\\alpha &= a + \\sum_{i=1}^n y_i\\\\\n\\beta &= b + \\sum_{i=1}^n x_i\n\\end{aligned}\n\\]\n\n\n\nHint: we have conjugacy. You can see this if you view the likelihood and the prior each as a function of \\(\\theta\\), the kernel of each has the same functional form.\nFollow-up:\n\nwhat is \\(E[\\theta | y_1,\\ldots, y_n]\\)? How does it compare to \\(E[\\theta]\\)?\nwhat is \\(Var[\\theta | y_1,\\ldots y_n]\\)?\n\n\nplot posteriorcode\n\n\n\n\n\n\n\n\n\n\na = 9\nb = 3\nsumY = sum(yx$AST)\nsumX = sum(yx$MIN)\n\ndata.frame(x = c(0, 0.5)) %>%\n  ggplot(aes(x = x)) + \n  stat_function(fun = dgamma, args = list(shape = sumY + a,\n                                          rate = sumX + b)) + \n  labs(x = TeX(\"$\\\\theta$\"), y = TeX(\"p($\\\\theta | y_1, \\\\ldots, y_n$)\")) +\n  theme_bw()\n\n\n\n\nGiven our prior of \\(a = 9, b = 3\\), \\(E[\\theta | y_1,\\ldots, y_n] =\\) 0.1194"
  },
  {
    "objectID": "notes/lec03-Poisson-gamma-exp-families.html#exponential-families",
    "href": "notes/lec03-Poisson-gamma-exp-families.html#exponential-families",
    "title": "NBA Assists",
    "section": "Exponential families",
    "text": "Exponential families\nDefinition: a sufficient statistic is a function of the data \\((y_1,\\ldots y_n\\)) that is sufficient to make inference about unknown parameters (\\(\\theta\\)).\nIf density \\(p(y|\\theta)\\) can be written \\(h(y) c(\\phi) e^{\\phi t(y)}\\) for some transform \\(\\phi = f(\\theta)\\) we can say \\(p(y|\\theta)\\) belongs in the exponential family.\n\n\n\n\n\n\nNote\n\n\n\n\\(t(y)\\) is referred to as the sufficient statistic.\n\n\nIf \\(p(y|\\theta)\\) belongs in the exponential family, then the conjugate prior is\n\\[\np(\\phi | n_0, t_0) =c(\\phi)^{n_0} e^{n_0 t_0 \\phi}\n\\]\n\n\n\n\n\n\nNotes about the prior\n\n\n\n\nthe conjugate prior is given over \\(\\phi\\) and we‚Äôd have to transform back if we care about \\(p(\\theta)\\).\n\\(n_0\\) is interpreted as the prior sample size and \\(t_0\\) is the prior guess.\n\n\n\nThe resulting posterior is\n\\[\np(\\phi | y_1,\\ldots y_n) \\propto p \\left(\\phi | n_0 + n, \\frac{n_0 t_0 + n \\sum t(y_i)/n}{n_0 + n}\\right)\n\\]\nIn words, one can show that the kernel of the posterior of \\(\\phi\\) is proportional to the kernel of the prior of \\(\\phi\\) with specific parameters. Thereby, we have conjugacy.\n\nExample offline: Poisson density"
  },
  {
    "objectID": "notes/lec04-reliability.html",
    "href": "notes/lec04-reliability.html",
    "title": "Posterior summaries and reliability",
    "section": "",
    "text": "Show packages used in these notes\nlibrary(tidyverse)\nlibrary(latex2exp)"
  },
  {
    "objectID": "notes/lec04-reliability.html#confidence-regions",
    "href": "notes/lec04-reliability.html#confidence-regions",
    "title": "Posterior summaries and reliability",
    "section": "Confidence regions",
    "text": "Confidence regions\n\nBayesian confidence interval\n\n\n\n\n\n\nDefinition\n\n\n\nLet \\(\\Phi\\) be the support of \\(\\theta\\). An interval \\((l(y), u(y)) \\subset \\Phi\\) has \\(100 \\times (1-\\alpha)\\%\\) posterior coverage if\n\\[\np(l(y) < \\theta < u(y) | y ) = (1-\\alpha)\n\\]\nInterpretation: after observing \\(Y = y\\), our probability that \\(\\theta \\in (l(y), u(y))\\) is \\(100 \\times (1-\\alpha)\\%\\).\nIf \\(\\alpha = 0.05\\), such an interval is called 95% posterior confidence interval (CI). It may also sometimes be referred to as a 95% ‚Äúcredible interval‚Äù to distinguish it from a frequentist CI.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis is a probability statement about \\(\\theta\\)!\n\n\n\n\nFrequentist confidence interval\nContrast posterior coverage to frequentist coverage:\n\n\n\n\n\n\nDefinition\n\n\n\nA random interval \\((l(Y), u(Y)\\)) has 95% frequentist coverage for \\(\\theta\\) if before data are observed,\n\\[\np(l(Y) < \\theta < u(Y) | \\theta) = 0.95\n\\]\nInterpretation: if \\(Y \\sim P_\\theta\\) then the probability that \\((l(Y), u(Y)\\) will cover \\(\\theta\\) is 0.95.\n\n\nIn practice, for many applied problems\n\\[\np(l(y) < \\theta < u(y) | y ) \\approx p(l(Y) < \\theta < u(Y) | \\theta)\n\\]\nsee section 3.1.2. in the book.\n\n\nExample: posterior CI for beta-binomial\nLet \\(Y_1, \\ldots Y_n\\) be binary random variables that are conditionally independent given \\(\\theta\\) so that \\(p(y_i | \\theta) = \\theta^{y_i} (1-\\theta)^{1-y_i}\\). Let \\(\\theta \\sim beta(a, b)\\).\nRecall that \\(\\theta | y_1,\\ldots y_n \\sim beta(a + \\sum{y_i}, b + n - \\sum y_i)\\).\nTherefore, a \\((1-\\alpha)\\) confidence interval can be computed for a given \\(a, b\\) and data \\(n, \\sum y_i\\), see coded example below:\n\n\nShow code\na = 6\nb = 6\nsumY = 8\nn = 10\n\nposteriorMean = (a + sumY) / (a + b + n)\n\nalpha = 0.05\nlower_q = alpha / 2\nupper_q = 1 - (alpha / 2)\ntheta_ci_95  = qbeta(p = c(lower_q, upper_q), a + sumY, b + n - sumY)\n\n\n\n\n\nThe posterior mean \\(E[\\theta | y_1,\\ldots y_n] =\\) 0.64 with 95% Bayesian CI (0.43,0.82).\n\nplotcode\n\n\n\n\n\n\n\n\n\n\ndata.frame(theta = c(0, 1)) %>%\n  ggplot(aes(x = theta)) + \n  stat_function(fun = dbeta, args = list(a + sumY, b + n - sumY)) + \n  theme_bw() +\n  labs(y = \"\") +\n  geom_area(stat = \"function\", fun = dbeta, args = list(a + sumY, b +n - sumY),\n            fill = \"steelblue\", xlim = theta_ci_95, alpha = 0.5)"
  },
  {
    "objectID": "notes/lec04-reliability.html#high-posterior-density",
    "href": "notes/lec04-reliability.html#high-posterior-density",
    "title": "Posterior summaries and reliability",
    "section": "High posterior density",
    "text": "High posterior density\n\n\n\n\n\n\nDefinition\n\n\n\nA \\(100 \\times (1-\\alpha)\\)% high posterior density (HPD) region is a set \\(s(y) \\subset \\Theta\\) such that\n\n\\(p(\\theta \\in s(y) | Y = y) = 1 - \\alpha\\)\nIf \\(\\theta_a \\in s(y)\\) and \\(\\theta_b \\not\\in s(y)\\), then \\(p(\\theta_a | Y = y) > p(\\theta_b | Y = y)\\)\n\n\n\n\nNote: all points inside an HPD region have higher posterior density than points outside the region.\nNote: the HPD region is not always an interval.\n\n\nExample: HPD for a mixture of normals"
  },
  {
    "objectID": "labs/lab-practice1.html",
    "href": "labs/lab-practice1.html",
    "title": "Practice: exponential families and variable transformation",
    "section": "",
    "text": "Important\n\n\n\nThis lab will not be graded."
  },
  {
    "objectID": "labs/lab-practice1.html#exercise-2",
    "href": "labs/lab-practice1.html#exercise-2",
    "title": "Practice: exponential families and variable transformation",
    "section": "Exercise 2",
    "text": "Exercise 2\nTo very exercise 1 numerically, sample from \\(p(\\theta | n_0, t_0)\\) using the code below, for some \\(n_0, t_0\\) of your choosing.\ntheta = rbeta(n = 10000, shape1 = n_0 * t_0, shape2 = n_0)\nNext, transform each sample of the object theta to phi using \\(f(\\theta)\\) as defined in exercise 1.\nFinally, plot a density plot of samples phi and, on the same plot, add \\(p(\\phi | n_0, t_0)\\) from exercise 1, part (c) above."
  },
  {
    "objectID": "labs/lab-practice1.html#exercise-1",
    "href": "labs/lab-practice1.html#exercise-1",
    "title": "Practice: exponential families and variable transformation",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet \\(y | \\theta \\sim binary(\\theta)\\).\n\nWrite down the joint density of several observations, \\(p(y_1, \\ldots y_n | \\theta)\\).\nIdentify the sufficient statistic \\(t(y)\\), and the transform \\(\\phi = f(\\theta)\\) such that the joint density \\(p(y_1,\\ldots y_n | \\phi)\\) can be written as \\(h(y) c(\\phi) e^{\\phi t(y)}\\).\nWrite down the prior \\(p(\\phi|n_0, t_0)\\).\nConfirm that \\(f(\\theta)\\) is monotonic and invertible. Next, using the one-line formula, \\(p_\\theta(\\theta) = p_{\\phi}(f(\\theta))|\\frac{dy}{dx}|\\), show that the prior, \\(p(\\theta | n_0, t_0)\\) is a beta density."
  },
  {
    "objectID": "slides/lab-exp-families.html#exercise-1",
    "href": "slides/lab-exp-families.html#exercise-1",
    "title": "Exponential families & variable transformation",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet \\(y | \\theta \\sim binary(\\theta)\\).\n\nWrite down the joint density of several observations, \\(p(y_1, \\ldots y_n | \\theta)\\).\nIdentify the sufficient statistic \\(t(y)\\), and the transform \\(\\phi = f(\\theta)\\) such that the joint density \\(p(y_1,\\ldots y_n | \\phi)\\) can be written as \\(h(y) c(\\phi) e^{\\phi t(y)}\\).\nWrite down the prior \\(p(\\phi|n_0, t_0)\\).\nConfirm that \\(f(\\theta)\\) is monotonic and invertible. Next, using the one-line formula, \\(p_\\theta(\\theta) = p_{\\phi}(f(\\theta))|\\frac{dy}{dx}|\\), show that the prior, \\(p(\\theta | n_0, t_0)\\) is a beta density."
  },
  {
    "objectID": "slides/lab-exp-families.html#exercise-2",
    "href": "slides/lab-exp-families.html#exercise-2",
    "title": "Exponential families & variable transformation",
    "section": "Exercise 2",
    "text": "Exercise 2\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "slides/lab-exp-families.html#ex-1-exponential-family",
    "href": "slides/lab-exp-families.html#ex-1-exponential-family",
    "title": "Exponential families & variable transformation",
    "section": "Ex 1: exponential family",
    "text": "Ex 1: exponential family\nLet \\(p(y|\\theta) = \\theta^{y}(1-\\theta)^{1-y}\\)\n\nIdentify the transform \\(\\phi = f(\\theta)\\) such that \\(p(y | \\phi)\\) can be written as \\(h(y) c(\\phi) e^{\\phi t(y)}\\). Identify the sufficient statistic \\(t(y)\\), as well as \\(c(\\phi)\\).\nWrite down the prior \\(p(\\phi|n_0, t_0)\\)."
  },
  {
    "objectID": "slides/lab-exp-families.html#ex-2-variable-transform",
    "href": "slides/lab-exp-families.html#ex-2-variable-transform",
    "title": "Exponential families & variable transformation",
    "section": "Ex 2: variable transform",
    "text": "Ex 2: variable transform\nConfirm that \\(f(\\theta)\\) from the previous exercise is monotonic and invertible. Next, using the one-line formula: \\(p_\\theta(\\theta) = p_{\\phi}(f(\\theta))|\\frac{d\\phi}{d\\theta}|\\), show that the prior, \\(p(\\theta | n_0, t_0)\\) is a beta density.\nTo very the above numerically, sample from \\(p(\\theta | n_0, t_0)\\) using the code below, for some \\(n_0, t_0\\) of your choosing.\ntheta = rbeta(n = 10000, shape1 = n_0 * t_0, shape2 = n_0*(1-t_0)\nNext, transform each sample of the object theta to phi using \\(f(\\theta)\\) as defined in exercise 1.\nFinally, plot a density plot of samples phi and, on the same plot, add \\(p(\\phi | n_0, t_0)\\) from exercise 1, part (c)."
  },
  {
    "objectID": "slides/lab-exp-families.html#ex-3-another-variable-transform",
    "href": "slides/lab-exp-families.html#ex-3-another-variable-transform",
    "title": "Exponential families & variable transformation",
    "section": "Ex 3: another variable transform",
    "text": "Ex 3: another variable transform\nLet \\(X \\sim \\text{Unif}(5, 10)\\) and let \\(Y = X^2\\)\nNotice that even though \\(X^2\\) is not a monotonic function everywhere, it is a monotonic function over the support of X.\nExercise: use the change of variables formula to derive \\(p(y)\\). Confirm via simulation, as in exercise 2."
  },
  {
    "objectID": "slides/lab-exp-families.html#ex-1-solution",
    "href": "slides/lab-exp-families.html#ex-1-solution",
    "title": "Exponential families & variable transformation",
    "section": "Ex 1: solution",
    "text": "Ex 1: solution\na\n\\[\n\\phi = log(\\frac{\\theta}{1-\\theta})\n\\]\nTherefore,\n\\[\np(y|\\phi) = e^{\\phi y} \\left(\\frac{1}{1 + e^{\\phi}} \\right)\n\\]\nand\n\\[\n\\begin{aligned}\nt(y) &= y\\\\\nc(\\phi) &= \\left(\\frac{1}{1 + e^{\\phi}} \\right)\n\\end{aligned}\n\\]\nb\n\\[\np(\\phi | n_0, t_0) = \\left({1 + e^{\\phi}} \\right)^{-n_0} e^{n_0 t_0 \\phi}\n\\]"
  },
  {
    "objectID": "slides/lab-exp-families.html#ex-2-solution",
    "href": "slides/lab-exp-families.html#ex-2-solution",
    "title": "Exponential families & variable transformation",
    "section": "Ex 2: solution",
    "text": "Ex 2: solution\nMonotonic if \\(\\theta_1 \\geq \\theta_2\\) implies \\(f(\\theta_1) \\geq f(\\theta_2)\\) over the support \\(\\theta_1, \\theta_2 \\in [0,1]\\).\nLet \\(\\theta_1 > \\theta_2\\), let‚Äôs check:\n\\[\n\\begin{aligned}\n\\log \\left(\\frac{\\theta_1}{1-\\theta_1}\\right) &> \\log \\left(\\frac{\\theta_2}{1-\\theta_2}\\right)\\\\\n\\theta_1 (1-\\theta_2) &> \\theta_2(1-\\theta_1)\\\\\n\\theta_1 > \\theta_2\n\\end{aligned}\n\\]\nIt is monotonic!\nFurthermore, we found it was invertible in ex 1: \\(\\theta = e^{\\phi}/(1 + e^{\\phi})\\).\nOne-line formula: \\(p_\\theta(\\theta) = p_{\\phi}(f(\\theta))|\\frac{d\\phi}{d\\theta}|\\):\n\\[\n\\begin{aligned}\np(\\theta) &= (1-\\theta)^{n_0} (\\theta/(1-\\theta))^{n_0 t_0} \\left|\\frac{d\\phi}{d\\theta} \\right|\\\\\n&= \\theta^{n_0 t_0 -1} (1-\\theta)^{n_0(1 -t_0) - 1}\n\\end{aligned}\n\\]\nSo \\(\\theta \\sim \\text{beta}(n_0 t_0, n_0(1-t_0))\\).\nRemember that \\(a\\) and \\(b\\) of a beta distribution must both be positive! This constrains what \\(n_0\\) and \\(t_0\\) can be."
  },
  {
    "objectID": "slides/lab-exp-families.html#ex-2-solution-pt-2",
    "href": "slides/lab-exp-families.html#ex-2-solution-pt-2",
    "title": "Exponential families & variable transformation",
    "section": "Ex 2: solution (pt 2)",
    "text": "Ex 2: solution (pt 2)\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nn_0 = 5\nt_0 = .5\ntheta = rbeta(n = 100000, shape1 = n_0 * t_0, shape2 = n_0 * (1 - t_0))\nphi = log((theta / (1-theta)))\n\ndf = data.frame(phi)\n\nfphi = function(phi, n_0, t_0) {\n  ( (1 + exp(phi))^(-n_0) ) * exp(n_0 * t_0 * phi) / \n    beta(n_0 * t_0, n_0 * (1 - t_0))\n}\n\ndf %>%\n  ggplot(aes(x = phi)) + \n  stat_function(fun = fphi, args = list(n_0 = n_0, t_0 = t_0)) +\n  geom_histogram(aes(x = phi, y = ..density..),\n                 fill = 'steelblue', alpha = 0.5)\n\n\n\n\nThe normalizing constant can be obtained by transforming from \\(p(\\theta)\\) (which is beta) back to \\(p(\\phi)\\)."
  },
  {
    "objectID": "slides/lab-exp-families.html#ex-3-solution",
    "href": "slides/lab-exp-families.html#ex-3-solution",
    "title": "Exponential families & variable transformation",
    "section": "Ex 3: solution",
    "text": "Ex 3: solution\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nx = runif(100000, 5, 10)\ny = x^2\n\ndf = data.frame(y)\n\nf = function(y) {\n  return(.1/sqrt(y))\n}\n\ndf %>%\n  ggplot(aes(x = y)) + \n  stat_function(fun = f) +\n  geom_histogram(aes(x = y, y = ..density..),\n                 fill = 'steelblue', alpha = 0.5)\n\n\n\n\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "notes/lec04-reliability.html#practice",
    "href": "notes/lec04-reliability.html#practice",
    "title": "Posterior summaries and reliability",
    "section": "Practice",
    "text": "Practice"
  },
  {
    "objectID": "notes/lec04-reliability.html#demos",
    "href": "notes/lec04-reliability.html#demos",
    "title": "Posterior summaries and reliability",
    "section": "Demos",
    "text": "Demos"
  },
  {
    "objectID": "notes/lec04-reliability.html#laplace-approximation",
    "href": "notes/lec04-reliability.html#laplace-approximation",
    "title": "Posterior summaries and reliability",
    "section": "Laplace approximation",
    "text": "Laplace approximation\nPosterior mode: sometimes called ‚ÄúMAP‚Äù or ‚Äúmaximum a posteriori‚Äù estimate, this quantity is given by \\(\\hat{\\theta} = \\arg \\max_{\\theta} p(\\theta | y)\\).\n\nNotice this unwinds to be \\(\\hat{\\theta} = \\arg \\max_{\\theta} p(y | \\theta) p(\\theta)\\).\n\nOne way to report the reliability of the posterior mode is to look at the width of the posterior near the mode, which we can sometimes approximate with a Gaussian distribution:\n\\[\np(\\theta | y) \\approx C e^{\\frac{1}{2} \\frac{d^2L}{d\\theta^2}|_{\\hat{\\theta}} (\\theta - \\hat{\\theta})^2}\n\\]\nwhere \\(C\\) is a normalization constant and \\(L\\) is the log-posterior, \\(\\log p(\\theta | y)\\).\nTaken together, the fitted Gaussian with a mean equal to the posterior mode is called the Laplace approximation.\n\nLet‚Äôs derive the Laplace approximation offline\n\n\n\n\n\n\n\nExercise\n\n\n\nFor the beta-binomial model above, compute the Laplace approximation.\n\n\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nalpha = a + sumY\nbeta = b + n - sumY\n\nd2posterior = function(theta) {\n  ( (1-alpha) / (theta^2) ) - \n    ((beta - 1) / (1 - theta)^2)\n}\n\nthetaHAT = (alpha - 1) / (beta - 2 + alpha)\nsdhat = sqrt(- 1 / d2posterior(thetaHAT))\n\ndata.frame(theta = c(0, 1)) %>%\n  ggplot(aes(x = theta)) + \n  stat_function(aes(color = \"posterior\"),\n                fun = dbeta, args = list(alpha, beta)) + \n  theme_bw() +\n  labs(y = \"\") +\n  stat_function(aes(color = \"Laplace approx.\"), fun = dnorm,\n                args = list(mean = thetaHAT, sd = sdhat))"
  },
  {
    "objectID": "notes/lec04-reliability.html#plot-1",
    "href": "notes/lec04-reliability.html#plot-1",
    "title": "Posterior summaries and reliability",
    "section": "plot",
    "text": "plot"
  },
  {
    "objectID": "notes/lec04-reliability.html#code-1",
    "href": "notes/lec04-reliability.html#code-1",
    "title": "Posterior summaries and reliability",
    "section": "code",
    "text": "code\n\nalpha = a + sumY\nbeta = b + n - sumY\n\nd2posterior = function(theta) {\n  ( (1-alpha) / (theta^2) ) - \n    ((beta - 1) / (1 - theta)^2)\n}\n\nthetaHAT = (alpha - 1) / (beta - 2 + alpha)\nsdhat = sqrt(- 1 / d2posterior(thetaHAT))\n\ndata.frame(theta = c(0, 1)) %>%\n  ggplot(aes(x = theta)) + \n  stat_function(aes(color = \"posterior\"),\n                fun = dbeta, args = list(alpha, beta)) + \n  theme_bw() +\n  labs(y = \"\") +\n  stat_function(aes(color = \"Laplace approx.\"), fun = dnorm,\n                args = list(mean = thetaHAT, sd = sdhat))"
  },
  {
    "objectID": "hw/hw03.html",
    "href": "hw/hw03.html",
    "title": "Homework 3",
    "section": "",
    "text": "Let \\(Y_1, \\ldots Y_n | \\theta\\) be an i.i.d. random sample from a population with pdf \\(p(y|\\theta)\\) where\n\\[\np(y|\\theta) = \\frac{2}{\\Gamma(a)} \\theta^{2a} y^{2a -1} e^{-\\theta^2 y^2}\n\\]\nand \\(y > 0\\), \\(\\theta > 0\\), \\(a > 0\\).\nFor this density,\n\\[\n\\begin{aligned}\nE~Y|\\theta &= \\frac{\\Gamma(a + \\frac{1}{2})}{\\theta \\Gamma(a)}\\\\\nE~Y^2|\\theta &= \\frac{a}{\\theta^2}\n\\end{aligned}\n\\]\nCall this density \\(g^2\\) such that \\(Y_1, \\ldots Y_n | \\theta \\sim g^2(a, \\theta)\\).\n\nFind the joint pdf of \\(Y_1, \\ldots Y_n | \\theta\\) and simplify as much as possible.\nSuppose \\(a\\) is known but \\(\\theta\\) is unknown. Identify a simple conjugate class of priors for \\(\\theta\\). For any arbitrary member of the class, identify the posterior density \\(p(\\theta | y_1, \\ldots y_n)\\).\nObtain a formula for \\(E~ \\theta | Y_1, \\ldots Y_n\\) and \\(Var~\\theta | Y_1, \\ldots Y_n\\) when the prior is in the conjugate class."
  },
  {
    "objectID": "hw/hw03.html#exercise-2",
    "href": "hw/hw03.html#exercise-2",
    "title": "Homework 3",
    "section": "Exercise 2",
    "text": "Exercise 2\nPhysicists studying a radioactive substance measure the times at which the substance emits a particle. They will record \\(n+1\\) emissions and set \\(Y_1\\) to be the time elapsed between the first and second emission, \\(Y_2\\) to be the time elapsed between the second and third emission and so on. They will model the data as \\(Y_1, \\ldots Y_n | \\theta \\sim \\text{i.i.d. } \\text{exponential}(\\theta)\\). The pdf of the exponential(\\(\\theta\\)) distribution is\n\\[\np(y |\\theta) = \\theta e^{-\\theta y} \\ \\text{ for } \\ y>0, \\ \\theta>0.\n\\]\nFor this distribution, \\(E[Y|\\theta] = \\frac{1}{\\theta}\\).\n(a). Write out the corresponding joint density \\(p(y_1, \\ldots, y_n | \\theta)\\) and simplify as much as possible. Justify each step of your calculation.\n(b). Compute the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\), i.e.¬†the value \\(\\hat{\\theta}_{MLE}\\) that maximizes \\(p(y_1,\\ldots y_n | \\theta)\\). Hint: it‚Äôs easier to work with the log-likelihood.\n(c). Choose a prior \\(p(\\theta)\\) that is conjugate to the likelihood. Hint: look at kernels of densities on the distribution sheet. Write out the formula for \\(p(\\theta | y_1, \\ldots y_n)\\), up to a proportionality in \\(\\theta\\), and simplify as much as possible. From this, identify explicitly the posterior distribution of \\(\\theta\\) (i.e., write ‚Äúthe posterior is a blank distribution with parameter(s) blank)‚Äù.\n(d). Obtain the formula for \\(E[\\theta | y_1, \\ldots y_n]\\) as a function of \\(a, b, n\\) and \\(y_1, \\ldots y_n\\), and try to write this as a function of the estimator \\(\\hat{\\theta}\\) you found in part (b). What does \\(E[\\theta | y_1,\\ldots,y_n]\\) get close to as \\(n\\) increases?\n(e). Assume you observe \\((y_1, \\ldots, y_5) = (0.13, 0.31, 0.15, 0.12, 0.29)\\) and let \\(\\theta \\sim \\text{gamma}(1, 1)\\). Report a 95% posterior confidence interval for \\(\\theta\\)."
  },
  {
    "objectID": "hw/hw03.html#exercise-3",
    "href": "hw/hw03.html#exercise-3",
    "title": "Homework 3",
    "section": "Exercise 3",
    "text": "Exercise 3\nSuppose \\(Y|\\theta \\sim \\text{binary}(\\theta)\\) and we believe \\(\\theta \\sim \\text{Uniform}(0, 1)\\) describes our uninformed prior beliefs about \\(\\theta\\). However, we are really interested in the log-odds \\(\\gamma = f(\\theta) = \\log \\frac{\\theta}{1 - \\theta}\\).\n\nFind the prior distribution for \\(\\gamma\\) induced by our prior on \\(\\theta\\). Is the prior informative about \\(\\gamma\\)? Verify \\(p(\\gamma)\\) using Monte Carlo sampling (i.e.¬†sampling from \\(p(\\theta)\\)) and then plotting the empirical density of the transformed samples along with the closed-form solution.\nIn general, is the mean of the transform the same as the transform of the mean? In other words, is \\(E f(\\theta) = f(E[\\theta])\\)? Why or why not? Hint: come up with another example.\nAssume some data come in and \\(\\sum y_i = 7\\) out of \\(n = 10\\) trials. Report the posterior mean and 95% posterior confidence interval for \\(\\gamma\\). Is the transform of the quantile the quantile of the transform? Why or why not?"
  },
  {
    "objectID": "notes/lec05-introMonteCarlo.html",
    "href": "notes/lec05-introMonteCarlo.html",
    "title": "Intro to Monte Carlo",
    "section": "",
    "text": "Load packages:"
  },
  {
    "objectID": "notes/lec05-introMonteCarlo.html#monte-carlo-motivation",
    "href": "notes/lec05-introMonteCarlo.html#monte-carlo-motivation",
    "title": "Intro to Monte Carlo",
    "section": "Monte Carlo motivation",
    "text": "Monte Carlo motivation\nGeneral social survey from the 90s gathered data on the number of children to women of two categories: those with and without a bachelor‚Äôs degree.\nSetup:\n\n\\(Y_{i1}\\): number of children of \\(i\\)th woman in group 1 (no bachelor‚Äôs)\n\\(Y_{i2}\\): number of children of \\(i\\)th woman in group 2 (bachelor‚Äôs)\n\nModel:\n\n\\(Y_{11}, \\ldots, Y_{n_1 1} | \\theta_1 \\overset{\\mathrm{iid}}{\\sim} \\text{Poisson}(\\theta_1)\\)\n\\(Y_{12} \\ldots, Y_{n_2 2} | \\theta_2 \\overset{\\mathrm{iid}}{\\sim} \\text{Poisson}(\\theta_2)\\)\n\nPrior:\n\n\\(\\theta_1 \\sim \\text{gamma}(2, 1)\\)\n\\(\\theta_2 \\sim \\text{gamma}(2, 1)\\)\n\nData:\n\n\\(n_1 = 111\\), \\(\\bar{y_1} = 1.95\\), \\(\\sum y_{i 1} = 217\\)\n\\(n_2 = 44\\), \\(\\bar{y_1} = 1.5\\), \\(\\sum y_{i 1} = 66\\)\n\nPosterior:\n\n\\(\\theta_1 | \\vec{y_1} \\sim \\text{gamma}(219, 112)\\)\n\\(\\theta_2 | \\vec{y_2} \\sim \\text{gamma}(68, 45)\\)\n\nWe already know how to compute\n\nposterior mean: \\(E~\\theta | y = \\alpha / \\beta\\) (shape, rate parameterization)\nposterior density (dgamma)\nposterior quantiles and confidence intervals (qgamma)\n\nWhat about posterior distribution of \\(|\\theta_1 - \\theta_2|\\), \\(\\theta_1 / \\theta_2\\), \\(\\text{max} \\{\\theta_1, \\theta_2 \\}\\)?\nWhat about the probability a woman with a bachelor‚Äôs has more children than a woman without a bachelors? \\(p(\\tilde{y}_1 < \\tilde{y}_2 | \\vec{y_1}, \\vec{y_2})\\)?"
  },
  {
    "objectID": "notes/lec05-introMonteCarlo.html#monte-carlo-integration",
    "href": "notes/lec05-introMonteCarlo.html#monte-carlo-integration",
    "title": "Intro to Monte Carlo",
    "section": "Monte Carlo integration",
    "text": "Monte Carlo integration\n\napproximates an integral by a stochastic average\nshines when other methods of integration are impossible (e.g.¬†high dimensional integration)\nworks because of law of large numbers: for a random variable \\(\\theta\\), the sample mean \\(\\bar{\\theta}_N\\) converges to the true mean \\(\\mu\\) as the number of samples \\(N\\) tends to infinity.\n\nThe key idea is: we obtain independent samples from the posterior,\n\\[\n\\theta^{(1)}, \\ldots \\theta^{(N)} \\overset{\\mathrm{iid}}{\\sim} p(\\theta |\\vec{y})\n\\]\nthen the empirical distribution of the samples approximates the posterior (approximation improves as \\(N\\) increases).\nRecall\n\\[\nE~g(\\theta)|y = \\int_\\mathcal{\\theta} g(\\theta) p(\\theta | y)d\\theta \\approx \\frac{1}{N} \\sum_{i = 1}^N g(\\theta^{(i)}).\n\\]\nThe law of large numbers says that if our samples \\(\\theta^{(i)}\\) are independent, \\(\\frac{1}{N} \\sum_{i = 1}^N g(\\theta^{(i)})\\) to \\(E~\\theta|y\\).\n\n\n\n\n\n\nNote\n\n\n\nIntegrals are expectations, and expectations are integrals."
  },
  {
    "objectID": "notes/lec05-introMonteCarlo.html#examples",
    "href": "notes/lec05-introMonteCarlo.html#examples",
    "title": "Intro to Monte Carlo",
    "section": "Examples",
    "text": "Examples\n\n\\(\\theta_1 | \\vec{y_1} \\sim \\text{gamma}(219, 112)\\)\n\\(\\theta_2 | \\vec{y_2} \\sim \\text{gamma}(68, 45)\\)\n\n\n(1) proof of concept: the mean\n\nset.seed(123)\nN = 5000\nrgamma(N, shape = 219, rate = 112) %>%\n  mean()\n\n[1] 1.95294\n\n\nPretty close to the true mean, 1.9553571.\n\n\n(2) posterior of \\(|\\theta_1 - \\theta_2|\\)\n\nset.seed(123)\ntheta1 = rgamma(N, shape = 219, rate = 112)\ntheta2 = rgamma(N, shape = 68, rate = 45)\n\ndf = data.frame(diff = abs(theta1 - theta2))\n\ndf %>%\n  ggplot(aes(x = diff)) + \n  geom_density() +\n  theme_bw() +\n  labs(x = TeX(\"$|\\\\theta_1 - \\\\theta_2|$\"),\n       y = TeX(\"$p(|\\\\theta_1 - \\\\theta_2 || {y}_1, {y}_2)$\"))\n\n\n\n\n\n\n(3) \\(p(|\\theta_1 - \\theta_2|> .5)\\)\n\nmean(df$diff > .5)\n\n[1] 0.4108\n\n\n\nExerciseFull solutionQuick Monte Carlo\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\\(\\theta \\sim \\text{uniform}(0, 2)\\)\nLet \\(\\phi = \\log \\theta\\)\nVisualize \\(p(\\phi)\\) using Monte Carlo simulation, then show using the change of variables formula and plotting the closed form of the density.\n\n\n\n\n\n\n# sample from p(theta)\ntheta = runif(10000, 0, 2)\n\n# define transform function\nf = function(x) {\n  return(0.5 *exp(x))\n}\n\n# create a df for each plot\ndf = data.frame(phi = -7:0)\ndf2 = data.frame(phiSamples = log(theta))\n\n# make plots\ndf %>%\n  ggplot(aes(x = phi)) +\n  stat_function(fun = f, col = 'red', alpha = 0.5) +\n  geom_histogram(data = df2, aes(x = phiSamples,\n                                 y = ..density..),\n               fill = 'steelblue', alpha = 0.5)\n\n\n\n\n\n\n\n# Just making the Monte Carlo part of the plot \n# in 3 lines\ntheta = runif(10000, 0, 2)\nphi = log(theta)\nhist(phi)\n\n\n\n\n\n\n\n\nExerciseSolution\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWhat is \\(Pr(\\tilde{y}_1 < \\tilde{y}_2 | \\vec{y_1}, \\vec{y_2})\\)?\n\n\n\n\n\ntheta1 = rgamma(N, shape = 219, rate = 112)\ntheta2 = rgamma(N, shape = 68, rate = 45)\n\ny1tilde = rpois(N, theta1)\ny2tilde = rpois(N, theta2)\n\n# y1: no. children to parent w/ no bachelors\n# y2: no. children to parent w/ bachelors\nmean(y1tilde < y2tilde)\n\n[1] 0.307"
  },
  {
    "objectID": "notes/lec05-introMonteCarlo.html#exercise",
    "href": "notes/lec05-introMonteCarlo.html#exercise",
    "title": "Intro to Monte Carlo",
    "section": "Exercise",
    "text": "Exercise\nWhat about \\(p(\\tilde{y}_1 < \\tilde{y}_2 | \\vec{y_1}, \\vec{y_2})\\)?"
  },
  {
    "objectID": "notes/lec05-introMonteCarlo.html#solution",
    "href": "notes/lec05-introMonteCarlo.html#solution",
    "title": "Intro to Monte Carlo",
    "section": "Solution",
    "text": "Solution\n\ntheta1 = rgamma(N, shape = 219, rate = 112)\ntheta2 = rgamma(N, shape = 68, rate = 45)\n\ny1tilde = rpois(N, theta1)\ny2tilde = rpois(N, theta2)\n\nmean(y1tilde < y2tilde)\n\n[1] 0.307\n\n# y1: no. children to parent w/ no bachelors\n# y2: no children to parent w/ bachelors\n\nY1tilde = NULL\nY2tilde = NULL\n\nfor (i in 1:N) {\n  theta1 = rgamma(1, shape = 219, rate = 112)\n  theta2 = rgamma(1, shape = 68, rate = 45)\n  \n  Y1tilde = c(Y1tilde, rpois(1, theta1))\n  Y2tilde = c(Y2tilde, rpois(1, theta2))\n}\n\nmean(Y1tilde < Y2tilde)\n\n[1] 0.3048\n\nhist(Y2tilde)"
  },
  {
    "objectID": "notes/lec06-MonteCarloPredictionError.html",
    "href": "notes/lec06-MonteCarloPredictionError.html",
    "title": "Prediction checks and Monte Carlo Error",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\nlibrary(latex2exp)"
  },
  {
    "objectID": "notes/lec06-MonteCarloPredictionError.html#monte-carlo-prediction",
    "href": "notes/lec06-MonteCarloPredictionError.html#monte-carlo-prediction",
    "title": "Prediction checks and Monte Carlo Error",
    "section": "Monte Carlo prediction",
    "text": "Monte Carlo prediction\n\nPrior predictive distribution\nWe can use Monte Carlo to sample new observation, \\(\\tilde{y}\\), from the prior predictive distribution\n\\[\np(\\tilde{y}) = \\int p(\\tilde{y}|\\theta)p(\\theta) d\\theta,\n\\]\nwhere we proceed by following the iterative procedure below\n1. sample theta_i from the prior p(theta)\n2. sample ytilde from p(ytilde | theta_i)\n3. repeat steps 1 and 2\n\nthis can be useful to see if a prior for \\(p(\\theta)\\) actually translate to reasonable prior beliefs about the data.\n\n\n\n\n\n\n\nExercise\n\n\n\nFor \\(p(\\theta) = \\text{gamma}(8,2)\\), plot \\(p(\\tilde{y})\\) assuming \\(\\tilde{y} | \\theta \\sim \\text{Poisson}(\\theta)\\).\n\n\n\n\n\n\n\nPosterior predictive distribution\nWe can also sample \\(\\tilde{y}\\) from the posterior predictive distribution,\n\\[\np(\\tilde{y} | y_1, \\ldots y_n) = \\int p(\\tilde{y}|\\theta) p(\\theta|y_1, \\ldots, y_n)d\\theta,\n\\]\nwhere the procedure is the same as before, except step 1 is replace with sampling \\(\\theta\\) from the posterior \\(p(\\theta | y_1,\\ldots, y_n)\\).\nThe resulting sequence \\(\\{(\\theta^{(1)}, \\tilde{y}^{(1)}), \\ldots, (\\theta^{(S)}, \\tilde{y}^{(S)})\\}\\) constitutes \\(S\\) independent samples from the joint posterior of \\((\\theta, \\tilde{Y})\\). The sequence \\(\\{\\tilde{y}^{(1)}, \\ldots, \\tilde{y}^{(S)}\\}\\) constitutes \\(S\\) independent samples from the marginal posterior distribution of \\(\\tilde{Y}\\), aka the posterior predictive distribution."
  },
  {
    "objectID": "notes/lec06-MonteCarloPredictionError.html#posterior-predictive-model-checking",
    "href": "notes/lec06-MonteCarloPredictionError.html#posterior-predictive-model-checking",
    "title": "Prediction checks and Monte Carlo Error",
    "section": "Posterior predictive model checking",
    "text": "Posterior predictive model checking\nWe can assess the fit of a model by comparing the posterior predictive distribution to the empirical distribution.\n\nExample: is our Poisson model flawed?\n\n# load general social survey data\ngss = read_csv(\"https://sta360-fa23.github.io/data/gss.csv\")\n\n\ny1 = gss$CHILDS[gss$FEMALE == 1 &  gss$YEAR >= 1990  & gss$AGE == 40 & \n                   gss$DEGREE < 3 ]\ny1 = y1[!is.na(y1)]\nn = length(y1)\n\nWe are examining the number of children \\(Y_i\\) belonging to \\(n=\\) 111 40 year old women surveyed 1990 or later without a bachelor‚Äôs. These data come from the general social survey.\nSuppose\n\\[\n\\begin{aligned}\nY_i & \\sim \\text{Poisson}(\\theta)\\\\\n\\theta & \\sim \\text{gamma}(2, 1).\n\\end{aligned}\n\\]\nThe empirical and predictive distributions of the data are both plotted below.\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\n# posterior predictive distribution\nytotal = sum(y1)\na = 2 ; b = 1\nN = 10000\ntheta.post.mc = rgamma(N, ytotal + a, b + n)\ny1.mc = rpois(N, theta.post.mc)\n\n# data\ndf = data.frame(y1) # empirical\ndf2 = data.frame(y1.mc) # post predictive\n  \n# make plot\ndf %>%\n  ggplot(aes(x = y1)) +\n  geom_bar(aes(x = y1 + .15, y = (..count..)/sum(..count..),\n               fill = \"empirical\"), alpha = 0.6, width = 0.3) +\n  geom_bar(data = df2, \n                 aes(x = y1.mc -.15, y = (..count..) / sum(..count..),\n                     fill = \"predictive\"), alpha = 0.4, width = 0.3) +\n  labs(x = \"number of children\", \n       y = TeX(\"$p(Y_i = y_i)$\"),\n       fill = \"\") +\n  scale_x_continuous(breaks = c(0:7), labels = c(0:7),\n                     limits = c(-.5,7.5)) +\n  \n  theme_bw()\n\n\n\n\n\nExerciseSolution\n\n\nLet \\(\\mathbf{y}\\) be a vector of length 111. Let \\(t(\\mathbf{y})\\) be the ratio of \\(2\\)s to \\(1\\)s in \\(\\mathbf{y}\\). For our observed data, this test statistic \\(t(\\mathbf{y}_{obs}) = 38 / 19 = 2\\). What is the tail probability \\(p(t(\\tilde{\\mathbf{Y}}) \\geq t(\\mathbf{y}_{obs}))\\) under the posterior predictive distribution?\n\n\n\nset.seed(123)\n\nt.mc = NULL\nfor (i in 1:10000) {\n  theta1 = rgamma(1, ytotal + a, b + n) # draw 1 theta from posterior\n  y1.mc = rpois(n, theta1) # draw y from post pred of n = 111\n  t.mc = c(t.mc,\n           sum(y1.mc == 2) / sum(y1.mc == 1))# compute t\n}\nhist(t.mc)\n\n\n\nmean(t.mc >= 2)\n\n[1] 0.0059"
  },
  {
    "objectID": "notes/lec06-MonteCarloPredictionError.html#monte-carlo-error",
    "href": "notes/lec06-MonteCarloPredictionError.html#monte-carlo-error",
    "title": "Prediction checks and Monte Carlo Error",
    "section": "Monte Carlo error",
    "text": "Monte Carlo error\n\nHow many values should we simulate?\nRecall: expected values are integrals, and integrals are expected values. Since central limit theorem (CLT) deals with expected values‚Ä¶\nRecall: CLT states that if \\(\\theta_i |\\vec{y}\\) iid with mean \\(\\theta\\) and finite variance \\(\\sigma^2\\), for \\(i \\in \\{1, \\ldots, N\\}\\), then the sample mean\n\\[\n\\bar{\\theta} \\sim N(\\theta, \\frac{\\sigma^2}{N} ).\n\\]\n\nHow to remember this/show this? Offline notes.\n\nSo to estimate \\(\\theta\\), we can generate \\(\\bar{\\theta}\\) by Monte Carlo simulation and report a confidence interval using quantiles of the normal given above in conjunction with the Monte Carlo standard error \\(\\frac{\\hat{\\sigma}}{\\sqrt{N}}\\)\nThis means we get convergence at the rate \\(\\mathcal{O}\\left(\\frac{1}{\\sqrt{N}}\\right)\\) regardless of the dimension of the integral!\nRecall:\n\nsd1 = pnorm(1) - pnorm(-1)\nsd2 = pnorm(2) - pnorm(-2)\nsd3 = pnorm(3) - pnorm(-3)\n\n\na 0.6826895% confidence interval can be obtained using \\(\\pm 1\\cdot \\hat{\\sigma}/\\sqrt{N}\\)\na 0.9544997% confidence interval can be obtained using \\(\\pm 2\\cdot \\hat{\\sigma}/\\sqrt{N}\\)\na 0.9973002% confidence interval can be obtained using \\(\\pm 3\\cdot \\hat{\\sigma}/\\sqrt{N}\\)\n\n\n\nExample\n\n# Let theta be \"x\" in the code below\nset.seed(123)\n\n# binomial(n, p)\nn = 20\np = 0.4\n\n# mean, variance, sd of a binomial(n, p)\nEX = n*p # 20*.4 = 8\nVarX = n*p*(1-p) # 20*.4*.6 = 4.8\nsdX = sqrt(VarX) # 2.19089\n\n# Monte Carlo sample of size N\nN = 100\nxSamples = rbinom(N, size = n, prob = p) \n\n# sample mean, var, sd\nxbar = mean(xSamples)\nxvar = var(xSamples)\nxsigma = sd(xSamples) # = sqrt(sum((xSamples - xbar)^2) / (N -1))\n\nse = xsigma / sqrt(N)\n\nlb = round(xbar - (2*se), 3)\nub = round(xbar + (2*se), 3)\n\nFor N = 100 Monte Carlo samples, The posterior mean of \\(\\theta\\) is \\(\\bar{\\theta} =\\) 8.01 with 95% confidence interval (7.57 8.45).\n\nExercise\n\n\nAbove we estimate \\(Var(\\theta)\\) to be 4.838 and the standard error for \\(N = 100\\) was 0.22.\nIf you wanted to state \\(p(\\theta \\in (\\hat{\\theta} \\pm 0.01)) = 0.95\\), how large would \\(N\\) have to be?\nCheck your answer by adjusting \\(N\\) above."
  },
  {
    "objectID": "hw/hw04.html",
    "href": "hw/hw04.html",
    "title": "Homework 4",
    "section": "",
    "text": "Let \\(\\theta\\) be the rate of mutation for a certain cluster of cancer cells. Biologists encode their prior uncertainty about \\(\\theta\\) with the following density:\n\\[\np(\\theta) = \\frac{4}{10}e^{-4\\theta} + \\frac{9}{10 \\Gamma(8)} \\theta^7 e^{-\\theta}\n\\]\n\nMake a plot of this prior density and explain what it means, in context, to the biologists.\nLet \\(Y_i\\) be the number of mutations produced by the \\(i\\)th cell, such that\n\n\\[\nY_i | \\theta \\sim Poisson(\\theta)\n\\]\nWrite out the posterior distribution of \\(\\theta\\) given \\(y_1,\\ldots y_n\\) (up to a proportionality constant) and simplify as much as possible. Hint: Be careful when writing your proportionality statement!\n\nThe posterior is a mixture (weighted average) of two distributions that you know. Identify these two distributions, including their parameters.\nAssume you obtain mutation data from two cells, \\(y_1 = 3, y_2 = 1\\). Compute the posterior exactly (i.e.¬†find the appropriate integration constant) and plot the posterior density.\nIn part (c) you identified the posterior as a mixture (weighted average) of two distributions. Given the data in part (d), compute the weights of each density in the mixture."
  },
  {
    "objectID": "hw/hw04.html#exercise-1",
    "href": "hw/hw04.html#exercise-1",
    "title": "Homework 4",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet \\(\\theta\\) be the rate of mutation for a certain cluster of cancer cells. Biologists encode their prior uncertainty about \\(\\theta\\) with the following density:\n\\[\np(\\theta) = \\frac{4}{10}e^{-4\\theta} + \\frac{9}{10 \\Gamma(8)} \\theta^7 e^{-\\theta}\n\\]\n\nMake a plot of this prior density and explain what it means, in context, to the biologists.\nLet \\(Y_i\\) be the number of mutations produced by the \\(i\\)th cell, such that\n\n\\[\nY_i | \\theta \\sim Poisson(\\theta)\n\\]\nWrite out the posterior distribution of \\(\\theta\\) given \\(y_1,\\ldots y_n\\) (up to a proportionality constant) and simplify as much as possible. Hint: Be careful when writing your proportionality statement!\n\nThe posterior is a mixture (weighted average) of two distributions that you know. Identify these two distributions, including their parameters.\nAssume you obtain mutation data from two cells, \\(y_1 = 3, y_2 = 1\\). Compute the posterior exactly (i.e.¬†find the appropriate integration constant) and plot the posterior density.\nIn part (c) you identified the posterior as a mixture (weighted average) of two distributions. Given the data in part (d), compute the weights of each density in the mixture."
  },
  {
    "objectID": "hw/hw04.html#exercise-2",
    "href": "hw/hw04.html#exercise-2",
    "title": "Homework 4",
    "section": "Exercise 2",
    "text": "Exercise 2\nA group of scientists have mutation data,\n\ny = c(0,3,0,1,5,2,0,4,1,1)\n\nand are interested in assessing how well the Poisson model from exercise 1 fits their data. Using the data generative model and prior from exercise 1, generate posterior predictive datasets \\(y^{(1)}, y^{(2)}, \\ldots y^{(S)}\\), where each data set \\(y^{(s)}\\) is a vector of length 10 whose entries are sampled from the Poisson distribution with parameters \\(\\theta^{(s)}\\). Each \\(\\theta^{(s)}\\) itself is a sample from the posterior \\(p(\\theta | y_1, \\ldots y_{10})\\). For each \\(s\\), let \\(t(s)\\) be the sample average of the 10 values of \\(y^{(s)}\\) divided by the sample standard deviation of \\(y^{(s)}\\). Make a histogram of \\(t(s)\\) and compare to the observed value of this statistic. Based on this statistic, assess the fit of the Poisson model for these data."
  },
  {
    "objectID": "hw/hw04.html#exercise-3",
    "href": "hw/hw04.html#exercise-3",
    "title": "Homework 4",
    "section": "Exercise 3",
    "text": "Exercise 3\nChimowitz et al.¬†(2011) https://doi.org/10.1056/NEJMoa1105335 investigate if stents are effective treatment to manage strokes in patients with atherosclerotic intracranial arterial stenosis. You can load the data using the code below. Data sourced from openintro package.\n\nstents = readr::read_csv(\"https://sta360-fa24.github.io/data/stent365.csv\")\n\nEach row of the data set is an individual patient. The group column indicates whether the patient was treated with a stent or not. The outcome column reports whether the patient had a stroke or not within a year.\n\nWrite down a data generative model (possibly as two separate densities) for this data.\nWrite down your prior beliefs about unknown parameters in your model above using a conjugate density. Choose parameters for the priors. Explain your choices.\nReport (using Monte Carlo sampling or otherwise) the posterior mean of the relative risk, i.e.¬†the posterior mean of the probability of stroke in the treatment group versus in the control, (think \\(E~[\\frac{\\theta_t}{\\theta_c}~|~\\text{data}]\\)). Additionally, include a 95% posterior confidence interval for the relative risk.\nPlot the posterior of the relative risk from part c.¬†Do you believe the treatment is effective?"
  },
  {
    "objectID": "hw/hw2023_04.html",
    "href": "hw/hw2023_04.html",
    "title": "Homework 4",
    "section": "",
    "text": "Let\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, 1/\\gamma)\\\\\n\\theta | \\sigma^2 &\\sim N(\\mu_0, 1/\\gamma \\kappa_0)\\\\\n\\gamma &\\sim \\text{gamma}(a, b)\n\\end{aligned}\n\\]\nso \\(\\gamma\\) is the precision (inverse-variance) of the normal distribution.\n\nDerive and simplify the joint pdf \\(p(y_1, \\ldots y_n | \\theta, \\gamma)\\)\nDerive the posterior of the precision, \\(p(\\gamma| y_1, \\ldots y_n)\\).\nDerive the posterior of \\(\\theta\\), \\(p(\\theta | y_1, \\ldots y_n)\\)"
  },
  {
    "objectID": "hw/hw2023_04.html#exercise-2",
    "href": "hw/hw2023_04.html#exercise-2",
    "title": "Homework 4",
    "section": "Exercise 2",
    "text": "Exercise 2\nExercise 5.1 from Hoff. You can read in the data from the three schools with the R code below. Hint: the problem specification is the same as exercise 1, except \\(a = \\nu_0/2\\) and \\(b = \\nu_0 \\sigma_0^2/2\\).\n\nlibrary(tidyverse)\nschool1 = read_csv(\"https://sta360-fa23.github.io/data/school1.csv\")\nschool2 = read_csv(\"https://sta360-fa23.github.io/data/school2.csv\")\nschool3 = read_csv(\"https://sta360-fa23.github.io/data/school3.csv\")"
  },
  {
    "objectID": "hw/hw2023_04.html#exercise-3",
    "href": "hw/hw2023_04.html#exercise-3",
    "title": "Homework 4",
    "section": "Exercise 3",
    "text": "Exercise 3\n3.12 from Hoff."
  },
  {
    "objectID": "hw/hw2023_05.html",
    "href": "hw/hw2023_05.html",
    "title": "Homework 5",
    "section": "",
    "text": "Risk calculation: Let \\(Y_1, \\ldots, Y_n | \\theta \\sim \\text{ i.i.d. Poission}(\\theta)\\).\n\nFor the case that \\(\\theta \\sim \\text{gamma}(a, b)\\), show that the posterior mean of \\(\\theta\\) given \\(Y_1, \\ldots, Y_n\\) can be written as \\(\\hat{\\theta}_w = w \\bar{y} + (1-w)\\mu\\) for values \\(w\\) and \\(\\mu\\) that depend on \\(n\\), \\(a\\) and \\(b\\).\nNow consider how good this estimator is for a specific value of \\(\\theta\\). Compute \\(E[\\hat{\\theta}_w|\\theta]\\), \\(V[\\hat{\\theta}_w|\\theta]\\), and \\(E[\\bar{y}|\\theta]\\) and \\(V[\\bar{y}|\\theta]\\).\nFind some nice conditions on \\(w\\) and \\(\\mu\\) so that \\(MSE[\\hat{\\theta}_w] < MSE[\\bar{y}]\\)\n[Optional] Now suppose \\(n = 10\\) and \\(\\theta = 5\\). Pick a value of \\(w\\) and \\(\\mu\\) so that your condition in c.¬†is met. Now verify the condition numerically with a Monte Carlo simulation, by simulating 1000 samples of size \\(n=10\\) from the Poisson(5) distribution, computing \\(\\bar{y}\\) and \\(\\hat{\\theta}_w\\) for each simulated sample, and then approximating the MSE of each estimator using the 1000 simulated values of each. Also make histograms or density plots of the simulated estimators, to confirm that one has low(er) variance but positive bias, and the other has zero bias but high(er) variance."
  },
  {
    "objectID": "hw/hw2023_05.html#exercise-2",
    "href": "hw/hw2023_05.html#exercise-2",
    "title": "Homework 5",
    "section": "Exercise 2",
    "text": "Exercise 2\n6.1 from Hoff. Let \\(\\theta\\) and \\(\\gamma\\) be independent. Use the code below to load the data.\n\nbach30 = readr::read_csv(\"https://sta360-fa23.github.io/data/bach30.csv\")\n\nnobach30 = readr::read_csv(\"https://sta360-fa23.github.io/data/nobach30.csv\")"
  },
  {
    "objectID": "hw/hw2023_05.html#exercise-3",
    "href": "hw/hw2023_05.html#exercise-3",
    "title": "Homework 5",
    "section": "Exercise 3",
    "text": "Exercise 3\n6.2 from Hoff. Note the typo: \\(1/\\sigma_j^2\\) is gamma, not \\(1/\\sigma_j\\). Use the code below to load the data.\n\nglucose = readr::read_csv(\"https://sta360-fa23.github.io/data/glucose.csv\")"
  },
  {
    "objectID": "hw/hw2023_06.html",
    "href": "hw/hw2023_06.html",
    "title": "Homework 6",
    "section": "",
    "text": "6.3 from Hoff. You can simulate from a constrained normal distribution with mean mean and standard deviation sd, constrained to lie in the interval \\((a,b)\\) using the following function:\n\nrcnorm<-function(n, mean=0, sd=1, a=-Inf, b=Inf){\n  u = runif(n, pnorm((a - mean) / sd), pnorm((b - mean) / sd))\n  mean + (sd * qnorm(u))\n}\n\nNote that you can use this function to simulate a vector of constrained normal random variables, each with a potentially different mean, standard deviation, and constraints.\nTo load the data for this exercise, run the code below\n\ndivorce = readr::read_csv(\"https://sta360-fa23.github.io/data/divorce.csv\")"
  },
  {
    "objectID": "hw/hw2023_06.html#exercise-2",
    "href": "hw/hw2023_06.html#exercise-2",
    "title": "Homework 6",
    "section": "Exercise 2",
    "text": "Exercise 2\nShow that if \\(W \\sim \\text{Wishart}(m, S)\\) then \\(E[W] = mS\\)."
  },
  {
    "objectID": "hw/hw2023_06.html#exercise-3",
    "href": "hw/hw2023_06.html#exercise-3",
    "title": "Homework 6",
    "section": "Exercise 3",
    "text": "Exercise 3\nSuppose \\(Y\\) is a random normal vector \\(Y \\sim N_p(\\theta, \\Sigma)\\). Let \\(Y_A\\) be the first \\(p_1\\) elements of \\(Y\\) and \\(Y_B\\) be the last \\(p_2 = p - p_1\\) elements, so that \\(Y = (Y_A, Y_B)\\). Similarly, write \\(\\theta = (\\theta_A, \\theta_B)\\). Finally, let\n\\[\n\\Sigma^{-} \\equiv \\Psi = \\left[ {\\begin{array}{cc}\n   \\Psi_{AA} & \\Psi_{AB} \\\\\n   \\Psi_{BA} & \\Psi_{BB} \\\\\n  \\end{array} } \\right]\n\\]\nand note that \\(\\Psi_{AB} = \\Psi_{BA}^T\\). Find the conditional distribution of \\(Y_B\\) given \\(Y_A\\) in terms of \\(\\theta_A\\), \\(\\theta_B\\) and components of \\(\\Psi\\). Try to interpret how \\(E[Y_B|Y_A]\\) differs from \\(E[Y_B]\\) and how \\(V[Y_B|Y_A]\\) differs from \\(V[Y_B]\\).\n\nIdentities for exercise 3\nSome of the following identities will be helpful for interpretation.\nLet\n\\[\n\\Sigma = \\left[ {\\begin{array}{cc}\n   \\Sigma_{AA} & \\Sigma_{AB} \\\\\n   \\Sigma_{BA} & \\Sigma_{BB} \\\\\n  \\end{array} } \\right]\n\\]\nand\n\\[\n\\Psi = \\left[ {\\begin{array}{cc}\n   \\Psi_{AA} & \\Psi_{AB} \\\\\n   \\Psi_{BA} & \\Psi_{BB} \\\\\n  \\end{array} } \\right].\n\\]\nThen\n\\[\n\\begin{aligned}\n\\Psi_{AA}^- &= \\Sigma_{AA} - \\Sigma_{AB} \\Sigma_{BB}^- \\Sigma_{BA}\\\\\n\\Psi_{BB}^- &= \\Sigma_{BB} - \\Sigma_{BA} \\Sigma_{AA}^- \\Sigma_{AB}\\\\\n\\Psi_{AB} &= -\\Psi_{AA} \\Sigma_{AB} \\Sigma_{BB}^-\\\\\n\\Psi_{BA} &= -\\Psi_{BB} \\Sigma_{BA} \\Sigma_{AA}^-,\n\\end{aligned}\n\\]\nand note that \\(\\Sigma_{AB} = \\Sigma_{BA}^T\\) and \\(\\Psi_{AB} = \\Psi_{BA}^T\\)."
  },
  {
    "objectID": "hw/hw2023_07.html",
    "href": "hw/hw2023_07.html",
    "title": "Homework 7",
    "section": "",
    "text": "7.3 from Hoff.\nRun the code below to load the data.\n\n\n\n\nlibrary(readr)\nbluecrab = read_csv(\"https://sta360-fa23.github.io/data/bluecrab.csv\")\norangecrab = read_csv(\"https://sta360-fa23.github.io/data/orangecrab.csv\")"
  },
  {
    "objectID": "hw/hw2023_07.html#exercise-2",
    "href": "hw/hw2023_07.html#exercise-2",
    "title": "Homework 7",
    "section": "Exercise 2",
    "text": "Exercise 2\n8.1 from Hoff. Note there is a typo in this exercise. Every \\(\\theta_i\\) in the exercise prompt should be replaced by \\(\\theta_j\\)."
  },
  {
    "objectID": "hw/hw2023_07.html#exercise-3",
    "href": "hw/hw2023_07.html#exercise-3",
    "title": "Homework 7",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\n\n8.3 from Hoff\nRun the code below to load the data.\n\nlibrary(readr)\nlibrary(glue)\n\nfor(i in 1:8) {\nassign(paste0(\"school\", i), \n       read_csv(glue(\"https://sta360-fa23.github.io/data/school{i}.csv\")))\n}"
  },
  {
    "objectID": "hw/hw2023_08.html",
    "href": "hw/hw2023_08.html",
    "title": "Homework 8",
    "section": "",
    "text": "Weighted regression: Suppose \\(y_i \\sim N(\\beta x_i, \\sigma^2 / w_i)\\) independently for \\(i = 1,\\ldots n\\), where \\(x_1, \\ldots, x_n\\) and \\(w_1, \\ldots w_n\\) are known scalars, and \\(\\beta\\) and \\(\\sigma^2\\) are unknown.\n\nFind the formula for the OLS estimator \\(\\hat{\\beta}_{OLS}\\) and compute its variance \\(V[\\hat{\\beta}_{OLS} | \\beta, \\sigma^2]\\).\nWrite out the sampling density \\(p(y_1, \\ldots, y_n | \\sigma^2, \\beta)\\) as a function of \\(\\beta\\) (i.e.¬†the likelihood) and find the value of \\(\\beta\\) that maximizes this function (the MLE). Denote this maximizing value as \\(\\hat{\\beta}_{MLE}\\). Compute \\(V[\\hat{\\beta}_{MLE} | \\beta, \\sigma^2]\\) and compare it to that of \\(\\hat{\\beta}_{OLS}\\).\nUnder the prior distribution \\(\\beta \\sim N(0, \\tau^2)\\), find \\(E[\\beta | y_1, \\ldots, y_n, \\sigma^2]\\). What does this estimator get close to as the prior precision goes to zero (\\(\\tau^2 \\rightarrow \\infty\\))?"
  },
  {
    "objectID": "hw/hw2023_08.html#exercise-2",
    "href": "hw/hw2023_08.html#exercise-2",
    "title": "Homework 8",
    "section": "Exercise 2",
    "text": "Exercise 2\nRidge regression theory: Let \\(y \\sim N_n(X \\beta, \\sigma^2 I)\\). Consider estimating \\(\\beta\\) with the prior distribution \\(\\beta | \\sigma^2 \\sim N_p(0, \\sigma^2 I / \\lambda)\\), where \\(\\lambda\\) is known and \\(\\beta\\) and \\(\\sigma^2\\) are unknown.\n\nDerive the conditional distribution of \\(\\beta | y, \\sigma^2\\) and, in particular, show that \\(E[\\beta | y] = (X^TX + I \\lambda)^{-1} X^Ty\\). Denote this expectation \\(\\hat{\\beta}_\\lambda\\), which we can use as an estimator of \\(\\beta\\). What happens to \\(\\hat{\\beta}_\\lambda\\) as \\(\\lambda \\rightarrow 0\\)?\nConsider the special case that \\(X^TX\\) is a diagonal matrix with entries \\(x_1^T x_1, \\ldots, x_p^T x_p\\). Find the mathematical relationship between each element of \\(\\hat{\\beta}_{\\lambda}\\) and the corresponding element of the OLS estimator \\(\\hat{\\beta}_{OLS}\\). Explain in words the effect of \\(\\lambda\\)."
  },
  {
    "objectID": "hw/hw2023_08.html#exercise-3",
    "href": "hw/hw2023_08.html#exercise-3",
    "title": "Homework 8",
    "section": "Exercise 3",
    "text": "Exercise 3\nRidge regression application: The data set yX.diabetes.train contains data on diabetes progression (first column) and 64 predictor variables. These data can be loaded with with command\n\nyX<-dget(url(\"https://www2.stat.duke.edu/~pdh10/FCBS/Inline/yX.diabetes.train\"))\n\n\nFor each value of \\(\\lambda \\in \\{0, 1, \\ldots, 99, 100 \\}\\) compute the estimator \\(\\hat{\\beta}_{\\lambda}\\) and plot this in some way (maybe using matplot).\nLoad the data set yX.diabetes.test using the code below\n\n\nyX.diabetes.test<-dget(\n  url(\"https://www2.stat.duke.edu/~pdh10/FCBS/Inline/yX.diabetes.test\"))\n\nUse yX.diabetes.test to evaluate the predictive performance of each estimate you obtained in part a. Specifically, compute the predictive error sum of squares \\(PSS(\\lambda) = ||y_{test} - X_{test} \\hat{\\beta}_{\\lambda}||^2\\) for each value of \\(\\lambda\\) (IMPORTANT: \\(\\hat{\\beta}_{\\lambda}\\) is obtained from the training data in part a, not the test data). Make a plot of PSS versus \\(\\lambda\\). How good is the unbiased OLS estimate for prediction, relative to the other estimates?\n\nIdentify the value of \\(\\lambda\\) that has the best predictive performance. For this best value of \\(\\lambda\\), report which x-variables have the largest effects."
  },
  {
    "objectID": "hw/hw2023_08.html#exercise-4",
    "href": "hw/hw2023_08.html#exercise-4",
    "title": "Homework 8",
    "section": "Exercise 4",
    "text": "Exercise 4\nFor this exercise, use the code below to load the data\n\nyX = readRDS(\n  url(\"http://www2.stat.duke.edu/~pdh10/Teaching/360/Materials/yXSS.rds\"))\n\nSource separation: The first column y of the dataset yXSS.rds is the vectorization of a spectroscopy image of a water sample taken from the Neuse River in North Carolina. You can view the image with the following code: y<-yX[,1] ; image(matrix(y,151,43)). The water sample is of unknown origin, but it is assumed that it is a mix of water from 9 different categories, whose average spectroscopy images are given by the remaining 9 columns \\(X\\) of yX. You can view these images with the same code above, applied to each column of \\(X\\).\n\nFrom \\(y\\) and \\(X\\), infer the sources of the water sample using the linear model \\(E[y|X, \\beta] = X\\beta\\). Assuming the normal linear model and with priors \\(\\beta \\sim N_9(1/9, \\ldots 1/9), I_9)\\), \\(1/\\sigma^2 \\sim \\text{gamma}(1, 1)\\), use a Gibbs sampler to obtain a posterior distribution of \\(\\beta\\) and \\(\\sigma^2\\) given \\(y\\). Plot the posterior density of \\(\\sigma^2\\), and obtain posterior 95% confidence intervals for each element of \\(\\beta\\). Which of the nine categories are the main sources of the water sample?\nEvaluate the assumptions of the normal linear model using some residual plots, addressing the assumption that the entries of \\(y\\) have constant variance, are uncorrelated, and are normally distributed.\nFor this problem it doesn‚Äôt make sense for the coefficients of \\(\\beta\\) to be negative. Think of a modification to the prior distribution for \\(\\beta\\) that takes this fact into account, and describe how a Gibbs sampler could be constructed to sample from the corresponding posterior distribution."
  },
  {
    "objectID": "hw/hw2023_09.html",
    "href": "hw/hw2023_09.html",
    "title": "Homework 9",
    "section": "",
    "text": "To load the data for this exercise, run the code below.\n\n\n\n\nyX = readr::read_csv(\"https://sta360-fa23.github.io/data/azdiabetes-train.csv\")\nyX.test = readr::read_csv(\"https://sta360-fa23.github.io/data/azdiabetes-test.csv\")\n\nThe file azdiabetes-train.csv contains data on health-related variables of a population of 432 women. In this exercise we will be modeling the conditional distribution of glucose level (glu) as a linear combination of the other variables, excluding the variable diabetes.\n\nFit a regression model using the g-prior with \\(g = n\\), \\(\\nu_0 = 2\\) and \\(\\sigma_0^2 = 1\\). Obtain 95% posterior confidence intervals for all of the parameters. Note: you do not need a Gibbs sampler for this problem, see p 159 of Hoff.\nFit a MVN linear model using rstanarm with priors \\(\\beta_i \\sim N(0, 1)\\) and a flat prior on \\(\\sigma\\). Report the posterior mean and 95% confidence intervals for all parameters.\nPerform the model selection and averaging procedure described in section 9.3. See secton 9.3.1 for the model and pg 168 for sample code to sample \\(\\mathbf{z}\\). Obtain \\(Pr(\\beta_j \\neq 0 | y)\\), as well as posterior confidence intervals for all of the parameters. Compare to the results in part (a) and (b). Additionally, compare the average squared error of predictions using the data set azdiabetes-test.csv under all three fitted models."
  },
  {
    "objectID": "hw/hw2023_09.html#exercise-2",
    "href": "hw/hw2023_09.html#exercise-2",
    "title": "Homework 9",
    "section": "Exercise 2",
    "text": "Exercise 2\nExercise 10.2 from Hoff.\nTo load the data for this exercise, run the code below,\n\nyXsparrow = readr::read_csv(\"https://sta360-fa23.github.io/data/yXsparrow.csv\")\n\nInstead of 1000, please run your chain until you reach at least 100 effective sample size."
  },
  {
    "objectID": "hw/hw2023_09.html#exercise-3",
    "href": "hw/hw2023_09.html#exercise-3",
    "title": "Homework 9",
    "section": "Exercise 3",
    "text": "Exercise 3\nCode for this exercise is provided below,\n\n# load the data\ntrans.prob.mat = readRDS(url(\"https://sta360-fa23.github.io/data/trans-prob-mat.rds\"))\ncipher_text = readLines(\"https://sta360-fa23.github.io/data/ciphertext.txt\")\n\npl = function(decoded) {\n  logprob = 0\n  prevletter = \"SPACE\"\n  for (i in 1:nchar(decoded)) {\n    curletter = substring(decoded, i, i)\n    if(curletter == \" \") {\n      curletter = \"SPACE\"\n    }\n    logprob = logprob + log(trans.prob.mat[rownames(trans.prob.mat) == prevletter,\n                                             colnames(trans.prob.mat) == curletter])\n    prevletter = curletter\n  }\n  return(logprob)\n} \n\nIn this exercise we will re-create the cryptanalysis tool described here to decrypt a secret message. Read pages 1-3 of the article by Persi Diaconis linked above.\n\nLoad the object trans.prob.matrix using the code above and examine. Based on your reading of the article, how can you interpret the entries of this matrix? Is it symmetric or not? Why does this make sense? The function pl(), given above, computes the ‚Äúplausibility‚Äù score for a given decoding. Explain in detail what the code comprising pl() does.\nFollow the pseudo-code outlined on page 2 of the article to write a MCMC algorithm and decrypt the secret message. Run your Markov chain for at least 1000 iterations and report the decoding with the highest plausibility score."
  },
  {
    "objectID": "slides/lab-mixtures.html#what-is-a-mixture-distribution",
    "href": "slides/lab-mixtures.html#what-is-a-mixture-distribution",
    "title": "Mixture distributions",
    "section": "What is a mixture distribution?",
    "text": "What is a mixture distribution?\nA mixture distribution is a convex combination of densities.\nIn other words‚Ä¶\n\\[\nf(x) = \\sum_{i=1}^nw_i p_i(x),\n\\]\nwhere \\(p_i(x)\\) is a pdf and \\(w_i > 0\\) for all \\(i\\) and \\(\\sum w_i = 1\\). We say: \\(f(x)\\) is a (finite) mixture density.\n\nExercise\nProve that \\(\\int f(x) dx = 1\\)."
  },
  {
    "objectID": "slides/lab-mixtures.html#example-1",
    "href": "slides/lab-mixtures.html#example-1",
    "title": "Mixture distributions",
    "section": "Example 1",
    "text": "Example 1\n\\[\nf(x) = \\sum_{i=1}^nw_i p_i(x),\n\\]\nwhere \\(p_i(x)\\) is a pdf and \\(w_i > 0\\) for all \\(i\\) and \\(\\sum w_i = 1\\). We say: \\(f(x)\\) is a (finite) mixture distribution.\n\nExercise\nProve that \\(\\int f(x) dx = 1\\)."
  },
  {
    "objectID": "slides/lab-mixtures.html#ex-1-exponential-family",
    "href": "slides/lab-mixtures.html#ex-1-exponential-family",
    "title": "Mixture densities",
    "section": "Ex 1: exponential family",
    "text": "Ex 1: exponential family\nLet \\(p(y|\\theta) = \\theta^{y}(1-\\theta)^{1-y}\\)\n\nIdentify the transform \\(\\phi = f(\\theta)\\) such that \\(p(y | \\phi)\\) can be written as \\(h(y) c(\\phi) e^{\\phi t(y)}\\). Identify the sufficient statistic \\(t(y)\\), as well as \\(c(\\phi)\\).\nWrite down the prior \\(p(\\phi|n_0, t_0)\\)."
  },
  {
    "objectID": "slides/lab-mixtures.html#ex-2-variable-transform",
    "href": "slides/lab-mixtures.html#ex-2-variable-transform",
    "title": "Mixture densities",
    "section": "Ex 2: variable transform",
    "text": "Ex 2: variable transform\nConfirm that \\(f(\\theta)\\) from the previous exercise is monotonic and invertible. Next, using the one-line formula: \\(p_\\theta(\\theta) = p_{\\phi}(f(\\theta))|\\frac{d\\phi}{d\\theta}|\\), show that the prior, \\(p(\\theta | n_0, t_0)\\) is a beta density.\nTo very the above numerically, sample from \\(p(\\theta | n_0, t_0)\\) using the code below, for some \\(n_0, t_0\\) of your choosing.\ntheta = rbeta(n = 10000, shape1 = n_0 * t_0, shape2 = n_0*(1-t_0)\nNext, transform each sample of the object theta to phi using \\(f(\\theta)\\) as defined in exercise 1.\nFinally, plot a density plot of samples phi and, on the same plot, add \\(p(\\phi | n_0, t_0)\\) from exercise 1, part (c)."
  },
  {
    "objectID": "slides/lab-mixtures.html#ex-3-another-variable-transform",
    "href": "slides/lab-mixtures.html#ex-3-another-variable-transform",
    "title": "Mixture densities",
    "section": "Ex 3: another variable transform",
    "text": "Ex 3: another variable transform\nLet \\(X \\sim \\text{Unif}(5, 10)\\) and let \\(Y = X^2\\)\nNotice that even though \\(X^2\\) is not a monotonic function everywhere, it is a monotonic function over the support of X.\nExercise: use the change of variables formula to derive \\(p(y)\\). Confirm via simulation, as in exercise 2."
  },
  {
    "objectID": "slides/lab-mixtures.html#ex-1-solution",
    "href": "slides/lab-mixtures.html#ex-1-solution",
    "title": "Mixture densities",
    "section": "Ex 1: solution",
    "text": "Ex 1: solution\na\n\\[\n\\phi = log(\\frac{\\theta}{1-\\theta})\n\\]\nTherefore,\n\\[\np(y|\\phi) = e^{\\phi y} \\left(\\frac{1}{1 + e^{\\phi}} \\right)\n\\]\nand\n\\[\n\\begin{aligned}\nt(y) &= y\\\\\nc(\\phi) &= \\left(\\frac{1}{1 + e^{\\phi}} \\right)\n\\end{aligned}\n\\]\nb\n\\[\np(\\phi | n_0, t_0) = \\left({1 + e^{\\phi}} \\right)^{-n_0} e^{n_0 t_0 \\phi}\n\\]"
  },
  {
    "objectID": "slides/lab-mixtures.html#ex-2-solution",
    "href": "slides/lab-mixtures.html#ex-2-solution",
    "title": "Mixture densities",
    "section": "Ex 2: solution",
    "text": "Ex 2: solution\nMonotonic if \\(\\theta_1 \\geq \\theta_2\\) implies \\(f(\\theta_1) \\geq f(\\theta_2)\\) over the support \\(\\theta_1, \\theta_2 \\in [0,1]\\).\nLet \\(\\theta_1 > \\theta_2\\), let‚Äôs check:\n\\[\n\\begin{aligned}\n\\log \\left(\\frac{\\theta_1}{1-\\theta_1}\\right) &> \\log \\left(\\frac{\\theta_2}{1-\\theta_2}\\right)\\\\\n\\theta_1 (1-\\theta_2) &> \\theta_2(1-\\theta_1)\\\\\n\\theta_1 > \\theta_2\n\\end{aligned}\n\\]\nIt is monotonic!\nFurthermore, we found it was invertible in ex 1: \\(\\theta = e^{\\phi}/(1 + e^{\\phi})\\).\nOne-line formula: \\(p_\\theta(\\theta) = p_{\\phi}(f(\\theta))|\\frac{d\\phi}{d\\theta}|\\):\n\\[\n\\begin{aligned}\np(\\theta) &= (1-\\theta)^{n_0} (\\theta/(1-\\theta))^{n_0 t_0} \\left|\\frac{d\\phi}{d\\theta} \\right|\\\\\n&= \\theta^{n_0 t_0 -1} (1-\\theta)^{n_0(1 -t_0) - 1}\n\\end{aligned}\n\\]\nSo \\(\\theta \\sim \\text{beta}(n_0 t_0, n_0(1-t_0))\\).\nRemember that \\(a\\) and \\(b\\) of a beta distribution must both be positive! This constrains what \\(n_0\\) and \\(t_0\\) can be."
  },
  {
    "objectID": "slides/lab-mixtures.html#ex-2-solution-pt-2",
    "href": "slides/lab-mixtures.html#ex-2-solution-pt-2",
    "title": "Mixture densities",
    "section": "Ex 2: solution (pt 2)",
    "text": "Ex 2: solution (pt 2)\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nn_0 = 5\nt_0 = .5\ntheta = rbeta(n = 100000, shape1 = n_0 * t_0, shape2 = n_0 * (1 - t_0))\nphi = log((theta / (1-theta)))\n\ndf = data.frame(phi)\n\nfphi = function(phi, n_0, t_0) {\n  ( (1 + exp(phi))^(-n_0) ) * exp(n_0 * t_0 * phi) / \n    beta(n_0 * t_0, n_0 * (1 - t_0))\n}\n\ndf %>%\n  ggplot(aes(x = phi)) + \n  stat_function(fun = fphi, args = list(n_0 = n_0, t_0 = t_0)) +\n  geom_histogram(aes(x = phi, y = ..density..),\n                 fill = 'steelblue', alpha = 0.5)\n\n\n\n\nThe normalizing constant can be obtained by transforming from \\(p(\\theta)\\) (which is beta) back to \\(p(\\phi)\\)."
  },
  {
    "objectID": "slides/lab-mixtures.html#ex-3-solution",
    "href": "slides/lab-mixtures.html#ex-3-solution",
    "title": "Mixture densities",
    "section": "Ex 3: solution",
    "text": "Ex 3: solution\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nx = runif(100000, 5, 10)\ny = x^2\n\ndf = data.frame(y)\n\nf = function(y) {\n  return(.1/sqrt(y))\n}\n\ndf %>%\n  ggplot(aes(x = y)) + \n  stat_function(fun = f) +\n  geom_histogram(aes(x = y, y = ..density..),\n                 fill = 'steelblue', alpha = 0.5)\n\n\n\n\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "slides/lab-mixtures.html#what-is-a-mixture-density",
    "href": "slides/lab-mixtures.html#what-is-a-mixture-density",
    "title": "Mixture densities",
    "section": "What is a mixture density?",
    "text": "What is a mixture density?\nA mixture density (sometimes called an ‚Äúadmixture‚Äù density) is a convex combination (i.e.¬†weighted sum, with non-negative weights that sum to 1) of other density functions.\nIn other words‚Ä¶\n\\[\nf(x) = \\sum_{i=1}^nw_i p_i(x),\n\\]\nwhere \\(p_i(x)\\) is a pdf and \\(w_i > 0\\) for all \\(i\\) and \\(\\sum w_i = 1\\). We say: \\(f(x)\\) is a (finite) mixture density.\nMixture densities are often used to model distinct sub-populations within a population. This allows us to create flexible prior distributions.\n\nExercise\nProve that \\(f(x)\\) is a proper density function, i.e.¬†that \\(f(x) \\geq 0\\) everywhere and \\(\\int f(x) dx = 1\\)."
  },
  {
    "objectID": "slides/lab-mixtures.html#exercise-1",
    "href": "slides/lab-mixtures.html#exercise-1",
    "title": "Mixture densities",
    "section": "Exercise 1",
    "text": "Exercise 1\nCreate and plot a function \\(f(x)\\) that is a mixture of two densities and approximates the histogram below."
  },
  {
    "objectID": "slides/lab-mixtures.html#exercise-2",
    "href": "slides/lab-mixtures.html#exercise-2",
    "title": "Mixture densities",
    "section": "Exercise 2",
    "text": "Exercise 2\nSuppose an experimental machine in a lab is either fine, or comes from a bad batch of machines that are to be recalled by the manufacturer. Scientists in the lab want to estimate the failure rate of their machine and decide whether or not to return it. They encode their prior uncertainty about the failure rate \\(\\theta\\) with the following density:\n\\[\np(\\theta) = \\frac{1}{4} \\frac{\\Gamma(10)}{\\Gamma(2)\\Gamma(8)}\\left[\n3 \\theta (1 - \\theta)^7 + \\theta^7(1- \\theta)\n\\right]\n\\]\n(a). Make a plot of this prior density and explain why it makes sense for the scientists. Based on the prior density, which do the scientists think is more likely - that their machine is fine, or bad?\n(b). The scientists run the machine \\(n\\) times. Let \\(y_i\\) be one if the machine fails on the \\(i\\)th run, and zero otherwise. Write out the posterior distribution of \\(\\theta\\) given \\(y_1, \\ldots, y_n\\) (up to a proportionality constant) and simplify as much as possible.\n(c). The posterior is a mixture (weighted average) of two distributions that you know. Identify these two distributions, including their parameters."
  },
  {
    "objectID": "slides/lab-mixtures.html#solution-1",
    "href": "slides/lab-mixtures.html#solution-1",
    "title": "Mixture densities",
    "section": "Solution 1",
    "text": "Solution 1\nThe plot looks like a mixture of normals. The first centered on 0 and the second on 10. More weight is given to the first density.\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nmixNormal = function(x) {\n  0.75 *dnorm(x, 0, 1) +\n  0.25 * dnorm(x, 10, 1)\n}\n\ndata.frame(x = -2:13) %>%\nggplot(aes(x = x)) +\n  stat_function(fun = mixNormal) +\n  labs(y = \"f(x)\")"
  },
  {
    "objectID": "slides/lab-mixtures.html#solution-2",
    "href": "slides/lab-mixtures.html#solution-2",
    "title": "Mixture densities",
    "section": "Solution 2",
    "text": "Solution 2\nIn lab.\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "notes/lec06-MonteCarloPredictionError.html#predictive-distributions-with-monte-carlo",
    "href": "notes/lec06-MonteCarloPredictionError.html#predictive-distributions-with-monte-carlo",
    "title": "Prediction checks and Monte Carlo Error",
    "section": "Predictive distributions with Monte Carlo",
    "text": "Predictive distributions with Monte Carlo\nGoal: evaluate our model both a priori and a posteriori i.e.¬†before and after looking at the data.\n\nPrior predictive distribution\nWe can use Monte Carlo to sample new observation, \\(\\tilde{y}\\), from the prior predictive distribution\n\\[\np(\\tilde{y}) = \\int p(\\tilde{y}|\\theta)p(\\theta) d\\theta,\n\\]\nwhere we proceed by following the iterative procedure below\n1. sample theta_i from the prior p(theta)\n2. sample ytilde from p(ytilde | theta_i)\n3. repeat steps 1 and 2\n\nthis can be useful to see if a prior for \\(p(\\theta)\\) actually translate to reasonable prior beliefs about the data.\n\n\nExercise\n\n\nFor \\(p(\\theta) = \\text{gamma}(8,2)\\), plot \\(p(\\tilde{y})\\) assuming \\(\\tilde{y} | \\theta \\sim \\text{Poisson}(\\theta)\\).\n\n\n\n\n\n\n\n\nPosterior predictive distribution\nWe can also sample \\(\\tilde{y}\\) from the posterior predictive distribution,\n\\[\np(\\tilde{y} | y_1, \\ldots y_n) = \\int p(\\tilde{y}|\\theta) p(\\theta|y_1, \\ldots, y_n)d\\theta,\n\\]\nwhere the procedure is the same as before, except step 1 is replace with sampling \\(\\theta\\) from the posterior \\(p(\\theta | y_1,\\ldots, y_n)\\).\nThe resulting sequence \\(\\{(\\theta^{(1)}, \\tilde{y}^{(1)}), \\ldots, (\\theta^{(S)}, \\tilde{y}^{(S)})\\}\\) constitutes \\(S\\) independent samples from the joint posterior of \\((\\theta, \\tilde{Y})\\). The sequence \\(\\{\\tilde{y}^{(1)}, \\ldots, \\tilde{y}^{(S)}\\}\\) constitutes \\(S\\) independent samples from the marginal posterior distribution of \\(\\tilde{Y}\\), aka the posterior predictive distribution."
  },
  {
    "objectID": "notes/lec07-normalModel.html",
    "href": "notes/lec07-normalModel.html",
    "title": "The normal model",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\nlibrary(latex2exp)"
  },
  {
    "objectID": "notes/lec07-normalModel.html#background",
    "href": "notes/lec07-normalModel.html#background",
    "title": "The normal model",
    "section": "Background",
    "text": "Background\n\nDefinition and vocabulary\nLet \\(Y\\) be normally distributed with mean \\(\\theta\\) and variance \\(\\sigma^2\\). Mathematically,\n\\[\nY | \\theta, \\sigma^2  \\sim N(\\theta, \\sigma^2).\n\\]\nThe density\n\\[\n\\begin{aligned}\np(y ~|~ \\theta, \\sigma^2 ) &= (2\\pi\\sigma^2)^{-1/2} e^{-\\frac{1}{2\\sigma^2} (y-\\theta)^2},\\\\\ny &\\in \\mathbb{R},\\\\\n\\theta &\\in \\mathbb{R},\\\\\n\\sigma &\\in \\mathbb{R}^+.\n\\end{aligned}\n\\]\n\nlocation, scale\n\n\\(\\theta\\) is called the ‚Äòlocation‚Äô parameter\n\\(\\sigma\\) is called the ‚Äòscale‚Äô parameter\n\n\n\nprecision\nNotice that every time \\(\\sigma^2\\) appears in the density, it is inverted. For this reason, the inverse variance \\((\\frac{1}{\\sigma^2})\\) has a special name, precision. Intuitively, precision tells us how close \\(y\\) is to the mean \\(\\theta\\). (Large precision = small variance = closer).\n\n\n\nplots of normal densities\n\n\n\n\n\n\nWarning\n\n\n\nIn R, the arguments of pnorm, dnorm, rnorm are the mean and standard deviation (not the variance!)\n\n\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nset.seed(123)\nN = 10000\ny.mc = rnorm(N, mean = 3, sd = 2)\n\ndf = data.frame(y.mc)\n\ndf %>%\n  ggplot(aes(x = y.mc)) +\n  geom_histogram(aes(y = ..density..), alpha = 0.6, fill = 'steelblue') +\n  stat_function(fun = dnorm, args = list(mean = 3, sd = 2), aes(color = \"N(3, 4)\")) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = \"N(0, 1)\")) +\n  theme_bw() +\n  labs(x = \"y\", y = \"density\", title = \"Normal densities\",\n       color = \"\")"
  },
  {
    "objectID": "notes/lec07-normalModel.html#bayesian-inference",
    "href": "notes/lec07-normalModel.html#bayesian-inference",
    "title": "The normal model",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nIn general, we wish to make inference about \\(\\theta\\) and \\(\\sigma^2\\) after observing some data \\(y_1, \\ldots y_n\\) and thus are interested in the posterior \\(p(\\theta, \\sigma^2 | y_1, \\ldots y_n)\\). This is the standard task we have seen thus far, and requires us to specify a joint prior \\(p(\\theta, \\sigma^2)\\). Below, we will work to find a class of conjugate priors over \\(\\theta\\) and \\(\\sigma^2\\).\nWe can break up the joint posterior into two pieces from the axioms of probability:\n\\[\np(\\theta, \\sigma^2 | y_1, \\ldots y_n) = p(\\theta | \\sigma^2, y_1, \\ldots, y_n)p(\\sigma^2|y_1, \\ldots y_n)\n\\]\nThis suggests that we calculate the joint posterior by:\n\nfirst finding the full conditional of \\(\\theta\\): \\(p(\\theta| \\sigma^2, \\vec{y})\\)\nand then finding the marginal posterior of \\(\\sigma^2\\): \\(p(\\sigma^2 | \\vec{y})\\),\n\nwhere \\(\\vec{y} = \\{y_1, \\ldots y_n\\}\\).\n\nThe full conditional of \\(\\theta\\)\nBy Bayes‚Äô theorem,\n\\[\np(\\theta| \\sigma^2, \\vec{y}) \\propto \\underbrace{p(\\vec{y} |\\theta, \\sigma^2)}_{\\text{likelihood}} \\underbrace{p(\\theta|\\sigma^2)}_{\\text{prior}}.\n\\]\nTo arrive at the full conditional posterior of \\(\\theta\\), we must first specify a prior on \\(\\theta\\).\nConsidering we have a normal likelihood, what is a conjugate class of densities for \\(\\theta\\)?\n\n\n\n\n\n\nanswer\n\n\n\n\n\n\\(\\theta | \\sigma^2 \\sim N(\\mu_0, \\tau_0^2)\\) for some \\(\\mu_0 \\in \\mathbb{R}\\) and \\(\\tau_0^2 \\in \\mathbb{R}^+\\) is conjugate.\n\n\n\nWith the conjugate prior, our full conditional posterior \\(\\{ \\theta| \\sigma^2, \\vec{y} \\} \\sim N(\\mu_n, \\tau_n^2)\\) where\n\\[\n\\begin{aligned}\n\\mu_n &=\n\\frac{\\frac{1}{\\tau_0^2}\\mu_0 + \\frac{n}{\\sigma^2} \\bar{y}}{\n\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\n}\n\\\\\n\\\\\n\\tau_n^2 &= \\frac{1}{\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}}\n\\end{aligned}\n\\]\n\nLet‚Äôs sketch out ‚Äòcompleting the square‚Äô to derive the parameters offline.\n\n\n\nIntuitive posterior parameters\nIf we consider the posterior precision, \\(\\frac{1}{\\tau_n^2}\\), we can re-arrange the terms above to illuminate how posterior information = prior information + data information;.\n\\[\n\\frac{1}{\\tau_n^2}= \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\n\\]\nIn words, posterior precision is equivalent to prior precision plus sampling precision. If we name each precision term, \\(\\lambda_0 = \\frac{1}{\\tau_0}\\) and \\(\\lambda = \\frac{1}{\\sigma}\\) then\n\\[\n\\mu_n = \\frac{\\lambda_0^2}{\\lambda_0^2 + n\\lambda^2} \\mu_0 +\n\\frac{n\\lambda^2}{\\lambda_0^2 + n\\lambda^2} \\bar{y}\n\\]\ni.e.¬†the posterior mean is the weighted average of prior and sample mean, where the weights are the relative contribution of each precision!\nWe can re-define \\(\\lambda_0^2 = \\kappa_0 \\lambda^2\\) (or equivalently \\(\\tau_0^2 = \\frac{\\sigma^2}{\\kappa_0}\\)) and obtain\n\\[\n\\begin{aligned}\n\\mu_n &= \\frac{\\kappa_0}{\\kappa_0 + n} \\mu_0 + \\frac{n}{\\kappa_0 + n} \\bar{y},\\\\\n\\frac{1}{\\tau_n^2} &= \\frac{\\kappa_0 + n}{\\sigma^2}\n\\end{aligned}\n\\]\nwhere we can interpret \\(\\kappa_0\\) as the prior sample size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that \\(\\tau_n^2\\) is the posterior variance of the full conditional posterior of \\(\\theta\\). This is distinct from \\(\\sigma_n^2\\), defined below.\n\n\n\n\nPrior on \\(\\sigma^2\\)\nRemember, we want \\(p(\\theta, \\sigma^2 | \\vec{y}) = p(\\theta | \\sigma^2, y_1, \\ldots, y_n)p(\\sigma^2|y_1, \\ldots y_n)\\). We have the first component of the right hand side, what about the second component?\nNotice that\n\\[\np(\\sigma^2 | \\vec{y}) \\propto p(\\sigma^2)\\int p(\\vec{y} | \\theta, \\sigma^2) p(\\theta|\\sigma^2) d\\theta\n\\]\nBut how do we choose \\(p(\\sigma^2)\\) to be conjugate? We can proceed in multiple ways: one is noting that the integral is really a convolution of normals, (thereby a sum of normals) and is therefore a normal density.\nUpon inspection, we can see a suitable choice is \\(\\frac{1}{\\sigma^2} \\sim \\text{gamma}(a, b)\\).\n\n\nThe inverse-gamma\nA random variable \\(X \\in (0, \\infty)\\) has an inverse-gamma(a,b) distribution if \\(\\frac{1}{X}\\) has a gamma(a,b) distribution.\nIf X has an inverse-gamma distribution, the density of X is\n\\[\np(x | a, b) = \\frac{b^a}{\\Gamma(a)} x^{-a-1}e^{-b/x} \\ \\text{for } \\ x > 0\n\\]\nand\n\\[\n\\begin{aligned}\nEX &= \\frac{b}{(a-1)} \\text{ if } a \\geq 1; \\ \\infty \\text{ if } 0<a<1,\\\\\nVar(X) &= \\frac{b^2}{(a-1)^2(a-2)} \\ \\text{if } a \\geq 2; \\ \\infty \\text{ if } 0 < a < 2,\\\\\nMode(X) &= \\frac{b}{a +1}.\n\\end{aligned}\n\\]\n\n\nThe marginal posterior of \\(\\sigma^2\\)\nTaken all together, if we let our sampling model and prior distributions be such that\n\\[\n\\begin{aligned}\nY_i | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\n\\theta | \\sigma^2 & \\sim N(\\mu_0, \\sigma^2/\\kappa_0)\\\\\n\\frac{1}{\\sigma^2} &\\sim \\text{gamma}(\\frac{\\nu_0}{2}, \\frac{\\nu_0}{2} \\sigma_0^2)\n\\end{aligned}\n\\]\nthen the posterior\n\\[\n\\frac{1}{\\sigma^2} | \\vec{y} \\sim \\text{gamma}(\\frac{\\nu_n}{2}, \\frac{\\nu_n \\sigma^2_n}{2}),\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\nu_n &= \\nu_0 + n,\\\\\n\\sigma^2_n &= \\frac{1}{\\nu_n} \\left[\n\\nu_0 \\sigma^2_0 +(n-1)s^2 + \\frac{\\kappa_0 n}{\\kappa_0 + n}(\\bar{y} - \\mu_0)^2\n\\right],\n\\end{aligned}\n\\]\nand \\(s^2\\) is the sample variance, \\(\\frac{1}{n-1} \\sum_i (y_i - \\bar{y})^2\\)."
  },
  {
    "objectID": "notes/lec07-normalModel.html#sampling-from-the-joint-posterior",
    "href": "notes/lec07-normalModel.html#sampling-from-the-joint-posterior",
    "title": "The normal model",
    "section": "Sampling from the joint posterior",
    "text": "Sampling from the joint posterior\nSince \\(p(\\theta, \\sigma^2 | \\vec{y}) = p(\\theta | \\sigma^2, y_1, \\ldots, y_n)p(\\sigma^2|y_1, \\ldots y_n)\\), we can sample from the joint posterior by first sampling from \\(p(\\sigma^2|y_1, \\ldots y_n)\\) and then sampling from \\(p(\\theta | \\sigma^2, y_1, \\ldots, y_n)\\).\n\nExample\nProof of concept\nWe have some data:\n\n# generating 10 samples from the population\nset.seed(123)\ntrue.theta = 4\ntrue.sigma = 1\ny = rnorm(10, true.theta, true.sigma)\n\nybar = mean(y) # sample mean\nn = length(y) # sample size\ns2 = var(y) # sample variance\n\nWe make inference about \\(\\theta\\) and \\(\\sigma^2\\):\n\n# priors\n# theta prior\nmu_0 = 2; k_0 = 1\n# sigma2 prior\nnu_0 = 1; s2_0 = 0.010\n\n# posterior parameters\nkn = k_0 + n\nnun = nu_0 + n\nmun = (k_0 * mu_0 + n * ybar) /kn\ns2n = (nu_0 * s2_0 + (n - 1) * s2 + k_0 * n * (ybar - mu_0)^2 / (kn)) / (nun)\n\ns2.postsample = 1 / rgamma(10000, nun / 2, s2n * nun / 2)\ntheta.postsample = rnorm(10000, mun, sqrt(s2.postsample / kn))\n\ndf = data.frame(theta.postsample, s2.postsample)\n\ndf %>%\n  ggplot(aes(x = theta.postsample, y = s2.postsample)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  labs(x = TeX(\"$\\\\theta$\"),\n       y = TeX(\"$\\\\sigma^2$\"),\n       fill = TeX(\"$p(\\\\theta, \\\\sigma^2 | y_1, \\\\ldots y_n)$\")) +\n  theme_bw()"
  },
  {
    "objectID": "slides/lab-normal.html#exercise",
    "href": "slides/lab-normal.html#exercise",
    "title": "Normal modeling",
    "section": "Exercise",
    "text": "Exercise\nPhysicists studying a radioactive substance measure the times at which the substance emits a particle. They will record \\(n+1\\) emissions and set \\(Y_1\\) to be the time elapsed between the first and second emission, \\(Y_2\\) to be the time elapsed between the second and third emission and so on. They will model the data as \\(Y_1, \\ldots Y_n | \\theta \\sim \\text{exponential}(\\theta)\\). The pdf of the exponential(\\(\\theta\\)) distribution is\n\\[\np(y |\\theta) = \\theta e^{-\\theta y} \\ \\text{ for } \\ y>0, \\ \\theta>0.\n\\]\nFor this distribution, \\(E[Y|\\theta] = \\frac{1}{\\theta}\\).\n(a). Write out the corresponding joint density \\(p(y_1, \\ldots, y_n | \\theta)\\) and simplify as much as possible. Justify each step of your calculation.\n(b). Compute the maximum likelihood estimate \\(\\hat{\\theta}_{MLE}\\), i.e.¬†the value \\(\\hat{\\theta}_{MLE}\\) that maximizes \\(p(y_1,\\ldots y_n | \\theta)\\). Hint: it‚Äôs easier to work with the log-likelihood.\n(c). Choose a prior \\(p(\\theta)\\) that is conjugate to the likelihood. Hint: look at kernels of densities on the distribution sheet. Write out the formula for \\(p(\\theta | y_1, \\ldots y_n)\\), up to a proportionality in \\(\\theta\\), and simplify as much as possible. From this, identify explicitly the posterior distribution of \\(\\theta\\) (i.e., write ‚Äúthe posterior is a blank distribution with parameter(s) blank)‚Äù.\n(d). Obtain the formula for \\(E[\\theta, y_1, \\ldots y_n]\\) as a function of \\(a, b, n\\) and \\(y_1, \\ldots y_n\\), and try to write this as a function of the estimator \\(\\hat{\\theta}\\) you found in part (b). What does \\(E[\\theta | y_1,\\ldots,y_n]\\) get close to as \\(n\\) increases?\n(e). Report a 95% posterior confidence interval for \\(\\theta\\).\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "slides/lab-normal.html#data",
    "href": "slides/lab-normal.html#data",
    "title": "Normal modeling",
    "section": "Data",
    "text": "Data\n\n\n\n\nbass = read_csv(\"https://sta360-fa24.github.io/data/bass.csv\")\n\n\nglimpse(bass)\n\nRows: 171\nColumns: 5\n$ river   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ station <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ length  <dbl> 47.0, 48.7, 55.7, 45.2, 44.7, 43.8, 38.5, 45.8, 44.0, 40.4, 47‚Ä¶\n$ weight  <dbl> 1.616, 1.862, 2.855, 1.199, 1.320, 1.225, 0.870, 1.455, 1.220,‚Ä¶\n$ mercury <dbl> 1.60, 1.50, 1.70, 0.73, 0.56, 0.51, 0.48, 0.95, 1.40, 0.50, 0.‚Ä¶\n\n\nMercury, is a naturally occurring element that can have toxic effects on the nervous, digestive and immune systems of humans. Bass from the Waccamaw and Lumber Rivers (NC) were caught randomly, weighed, and measured. In addition, a filet from each fish caught was sent to the lab so that the tissue concentration of mercury could be determined for each fish. Each fish caught corresponds to a single row of the data frame. Today we will examine two columns from the data set: mercury (concentration of mercury in ppm) and weight (weight of the fish in kg). We‚Äôll model the mercury content \\(y\\) of each fish as a function of the fish‚Äôs weight \\(x\\)."
  },
  {
    "objectID": "slides/lab-normal.html#model",
    "href": "slides/lab-normal.html#model",
    "title": "Normal modeling",
    "section": "Model",
    "text": "Model\nLet\n\\[\n\\begin{aligned}\nY_i | \\theta &\\sim \\text{ iid  } N(\\theta x_i, 1)\\\\\n\\theta &\\sim N(\\mu_0, 1 / \\kappa_0)\n\\end{aligned}\n\\]\nLet \\(\\mu_0 = 0\\), \\(\\kappa_0 = 1\\).\n(a). Suppose you observe data \\(y_1,\\ldots y_n\\). Write out the formula for \\(p(\\theta | y_1, \\ldots y_n)\\).\n(b). Given the data on the previous slide, use Monte Carlo simulation to plot \\(p(\\theta | y_1, \\ldots, y_n)\\). Additionally, report \\(E[\\theta | y_1,\\ldots y_n]\\) together with a 95% posterior confidence interval.\n(c). If you caught a new fish with weight 4kg, what would you predict the mercury content to be? In other words, let x = 4 and compute \\(E[\\tilde{y}|y_1,\\ldots, y_n, x = 4]\\). Additionally, plot the the posterior predictive density \\(p(\\tilde{y} | y_1, \\ldots y_n, x = 4)\\).\n\nCritique your model. Hint: compare to the models below:\n\n\nlm(mercury ~ weight, data = bass)\nlm(mercury ~ 0 + weight, data = bass)"
  },
  {
    "objectID": "slides/lab-normal.html#solution",
    "href": "slides/lab-normal.html#solution",
    "title": "Normal modeling",
    "section": "Solution",
    "text": "Solution\na\n\\[\n\\theta | \\sigma^2, y_1, \\ldots y_n \\sim N(\\mu_n, \\tau_n^2)\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\mu_n &= \\frac{\\kappa_0 \\mu_0 + \\sum y_i x_i}{\\kappa_0 + \\sum x_i^2}\\\\\n\\tau_n^2 &= \\frac{\\sigma^2}{\\kappa_0 + \\sum x_i^2}\n\\end{aligned}\n\\]\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "slides/lab-normal.html#solution-a",
    "href": "slides/lab-normal.html#solution-a",
    "title": "Normal modeling",
    "section": "Solution (a)",
    "text": "Solution (a)\na\n\\[\n\\theta |  y_1, \\ldots y_n \\sim N(\\mu_n, \\tau_n^2)\n\\]\n\nwhere\n\n\n\n\n\\[\n\\begin{aligned}\n\\mu_n &= \\frac{\\kappa_0 \\mu_0 + \\sum y_i x_i}{\\kappa_0 + \\sum x_i^2}\\\\\n\\tau_n^2 &= \\frac{1}{\\kappa_0 + \\sum x_i^2}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lab-normal.html#solution-b",
    "href": "slides/lab-normal.html#solution-b",
    "title": "Normal modeling",
    "section": "Solution (b)",
    "text": "Solution (b)\nb\n\ndemodemo plotsolution codesolution plotsummary\n\n\nDemo with simulated data to make sure code works:\n\n# simulated data\nset.seed(123)\ntrue.theta = 4\ntrue.sigma = 1\nN = 10\nx = seq(from = 1, to = 10, length = N)\ny = rnorm(N, true.theta * x, true.sigma)\n\n# prior parameters\nk0 = 1\nmu0 = 1\n\nsumYX = sum(y * x)\nd = (k0 + sum(x^2))\nmun = (k0 + mu0 + sumYX) / d\ntn = sqrt(1 / d)\n\ntheta.postsample = rnorm(10000, mun, tn)\nhist(theta.postsample)\n\n\n\n\n\n\n\n\n\n\n\nx = bass$weight\ny = bass$mercury\n\n# prior parameters\nk0 = 1\nmu0 = 1\n\nsumYX = sum(y * x)\nd = (k0 + sum(x^2))\nmun = (k0 + mu0 + sumYX) / d\ntn = sqrt(1 / d)\n\ntheta.postsample = rnorm(10000, mun, tn)\nhist(theta.postsample)\n\n\n\n\n\n\n\n\n\n\n\nmean(theta.postsample)\n\n[1] 0.8370612\n\nquantile(theta.postsample, c(0.025, 0.975))\n\n     2.5%     97.5% \n0.7340858 0.9412182"
  },
  {
    "objectID": "slides/lab-normal.html#solution-c",
    "href": "slides/lab-normal.html#solution-c",
    "title": "Normal modeling",
    "section": "Solution (c)",
    "text": "Solution (c)\n\n# use posterior samples of theta and x = 4 to simulate ytilde\n\nytilde = rnorm(10000, theta.postsample * 4, 1)\nhist(ytilde)\n\nmean(ytilde)\n\n[1] 3.341773\n\n\nThis matches intuition (law of total expectation gives the closed form solution: 4 * 0.838 = 3.352)."
  },
  {
    "objectID": "slides/lab-normal.html#solution-d",
    "href": "slides/lab-normal.html#solution-d",
    "title": "Normal modeling",
    "section": "Solution (d)",
    "text": "Solution (d)\nWe have no intercept term. We are assuming that our regression line goes through the origin. This is a strong assumption. Our model will be most similar to the lm model without an intercept term:\n\nlm(mercury ~ 0 + weight, data = bass)\n\n\nCall:\nlm(formula = mercury ~ 0 + weight, data = bass)\n\nCoefficients:\nweight  \n0.8343  \n\n\nHowever, we‚Äôll get a different estimate of \\(\\hat{\\theta}\\) if we include an intercept term,\n\nlm(mercury ~ weight, data = bass)\n\n\nCall:\nlm(formula = mercury ~ weight, data = bass)\n\nCoefficients:\n(Intercept)       weight  \n     0.6387       0.4818  \n\n\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "notes/dynamicalSystems.html#model",
    "href": "notes/dynamicalSystems.html#model",
    "title": "Parameter Estimation in Dynamical Systems",
    "section": "Model",
    "text": "Model\nAssumption: iid N(0, 1) noise.\nData generative model:\n\\[\nY(t)| N_0, r_0, K \\sim Normal(N_0 f(t, N_0, K, r_0), 1)\n\\]\nHorrible priors:\n\\[\n\\begin{aligned}\nN_0 &\\sim Unif(1,50)\\\\\nK &\\sim Unif(40, 1000)\\\\\nr_0 &\\sim Unif(0, 100)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/dynamicalSystems.html#inference",
    "href": "notes/dynamicalSystems.html#inference",
    "title": "Parameter Estimation in Dynamical Systems",
    "section": "Inference",
    "text": "Inference\n\nset.seed(360)\n# starting points\nK_s =  75 #runif(1, 40, 1000)\nK = NULL\nN0_s = 25 #runif(1, 1, 50)\nN0 = NULL\nr0_s = 10\nr0 = NULL\nS = 100000 # number of iterations\n\ndelta = 1 # proposal variance\naccept = 0 # keep track of acceptance rate\n\ngetLogPosterior = function(K, r, N0) {\n  mu = growth(K = K, r = r, N0 = N0, t = df$t)\n  if (N0 < 0) {\n    return(-Inf)\n  }\n  if(K < 0) {\n    return(-Inf)\n  }\n  if(r < 0) {\n    return(-Inf)\n  }\n  else {\n  return(sum(dnorm(df$Y, mu, sd = rep(1, nrow(df)), log = TRUE)))\n  }\n  # todo: should add indicator whether or not within uniform ranges !\n}\n\nfor (s in 1:S) {\n  # log everything for numerical stability #\n  \n  ## propose K\n  K_proposal = rnorm(1, mean = K_s, sd = delta)\n  log.r = getLogPosterior(K = K_proposal, r = r0_s, N0 = N0_s) -\n    getLogPosterior(K = K_s, r = r0_s, N0 = N0_s)\n  \n   if(log(runif(1)) < log.r)  {\n    K_s = K_proposal\n    accept = accept + 1 \n   }\n  if(s %% 10 == 0) {\n  K = c(K, K_s)\n  }\n  \n  # propose N0\n\n  N0_proposal = rnorm(1, N0_s, 1)\n\n  log.r = getLogPosterior(K = K_s, r = r0_s, N0 = N0_proposal) -\n    getLogPosterior(K = K_s, r = r0_s, N0 = N0_s)\n\n   if(log(runif(1)) < log.r)  {\n    N0_s = N0_proposal\n    accept = accept + 1\n   }\n  if(s %% 10 == 0) {\n  N0 = c(N0, N0_s)\n  }\n\n\n  ## propose r\n\n  r0_proposal = rnorm(1, r0_s, .5)\n\n  log.r = getLogPosterior(K = K_s, r = r0_proposal, N0 = N0_s) -\n    getLogPosterior(K = K_s, r = r0_s, N0 = N0_s)\n\n   if(log(runif(1)) < log.r)  {\n    r0_s = r0_proposal\n    accept = accept + 1\n   }\n  if(s %% 10 == 0) {\n  r0 = c(r0, r0_s)\n  }\n}\n\naccept\n\n[1] 69129\n\n\n\nPOST = data.frame(r0 = r0,\n                K = K,\n                N0 = N0)\n\nnumBurn = 0.5*nrow(POST) \nPOST = POST[-c(1:numBurn),]\nPOST %>%\n  ggplot(aes(x = 1:nrow(POST), y = r0)) + \n  geom_line() +\n  theme_bw() +\n  labs(x = \"iteration\")"
  },
  {
    "objectID": "notes/lec08-estimators.html",
    "href": "notes/lec08-estimators.html",
    "title": "Estimators",
    "section": "",
    "text": "Definition\n\n\n\nA point estimator of an unknown parameter \\(\\theta\\) is a function that converts data into a single element of parameter space \\(\\Theta\\).\n\n\nExample: imagine \\(\\theta\\) is the population mean. The following are each point estimators of \\(\\theta\\):\n\n\\(\\bar{y}\\)\n\\(y_1\\)\n\\(\\frac{y_1 + y_2}{2}\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is convention to write the population parameter as a Greek character and the estimator as the same Greek character, but with a ‚Äúhat‚Äù. For example, \\(\\theta\\) is the parameter and \\(\\hat{\\theta}\\) is the estimator.\n\n\nSampling properties of a point estimator refer to the estimator‚Äôs behavior under hypothetical repeatable surveys or experiments.\nThree common sampling properties of estimators we will see again and again are:\n\nbias\nvariance\nmean squared error (MSE)\n\n\n\nBefore we discuss bias, variance and mean squared error of an estimator, it‚Äôs important to understand that an estimator is a statistic (function of the data) and therefore a random variable. Because of this, estimator‚Äôs have a sampling distribution.\nExercise: What does the example below show? What is x?\n\nset.seed(360)\n\nx = vector()\nfor (i in 1:100) {\n  y = rnorm(10)\n  x = append(min(y), x)\n}\nhist(x, freq = FALSE)\nabline(v = mean(x), col= \"steelblue\", lwd = 4)\n\n\n\ncat(\"The variance of x is \", round(var(x), 3))\n\nThe variance of x is  0.291\n\n\n\n\n\nIn the rest of these notes, let \\(\\theta_0\\) be the true value of the population parameter \\(\\theta\\).\n\n\n\n\n\n\nDefinition\n\n\n\nBias is the the difference between the expected value of the estimator and the true value of the parameter.\n\n\\(E[\\hat{\\theta} | \\theta = \\theta_ 0] - \\theta_0\\) is the bias of \\(\\hat{\\theta}\\).\nIf \\(E[\\hat{\\theta} | \\theta = \\theta_0] = \\theta_0\\), then we say \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\).\nIf \\(E[\\hat{\\theta} | \\theta = \\theta_0] \\neq \\theta_0\\), then we say \\(\\hat{\\theta}\\) is a biased estimator of \\(\\theta\\).\n\n\n\nExercise: Imagine \\(\\hat{\\theta}_a\\) and \\(\\hat{\\theta}_b\\) are two different estimators of \\(\\theta\\). The true value of \\(\\theta\\) is \\(\\theta_0 = 0\\). The sampling distributions of the two estimators are given below. Which estimator do you prefer?\n\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nRecall: variance is average squared distance from the mean. In this context, the variance of an estimator refers to the variance of the sampling distribution of \\(\\hat{\\theta}\\). We write this mathematically,\n\\[\nVar[\\hat{\\theta} | \\theta_0] = E[(\\hat{\\theta} - m)^2 |\\theta_0]\n\\]\nwhere \\(m = E[\\hat{\\theta}|\\theta_0]\\).\n\n\nWhile it may seem desirable to have an estimator with zero bias, the estimator may still be far away from the true parameter value if the variance is too large. The mean squared error quantifies how close an estimator is to the true parameter value.\n\n\n\n\n\n\nDefinition\n\n\n\nMean squared error (MSE) is (as the name suggests) the expected value of the squared difference between the estimator and true parameter value. Equivalently, MSE is the variance plus the square bias of the estimator.\n\\[\n\\begin{aligned}\nMSE[\\hat{\\theta}|\\theta_0] &= E[(\\hat{\\theta} - \\theta_0)^2 | \\theta_0]\\\\\n&= Var[\\hat{\\theta} | \\theta_0] + Bias^2[\\hat{\\theta}|\\theta_0]\n\\end{aligned}\n\\]\n\n\n\nLet‚Äôs show this offline."
  },
  {
    "objectID": "notes/lec08-estimators.html#practice",
    "href": "notes/lec08-estimators.html#practice",
    "title": "Estimators",
    "section": "Practice",
    "text": "Practice\nSuppose you wish to make inference about the average bill length of Chinstrap penguins.\nYou make the modeling assumption that \\(Y\\), the bill length of a penguin is normally distributed, i.e.¬†\\(Y \\sim N(\\theta, \\sigma^2)\\) and you set up a conjugate prior as we‚Äôve done before.\nOne can then show that the posterior mean estimator of \\(\\theta\\) is\n\\[\n\\hat{\\theta}_b = E[\\theta | y_1,\\ldots y_n] = \\frac{n}{\\kappa_0 + n} \\bar{y} + \\frac{\\kappa_0}{\\kappa_0 + n} \\mu_0 = w\\bar{y} + (1-w) \\mu_0\n\\]\nExercise: compare \\(\\hat{\\theta}_b\\) to the estimator \\(\\hat{\\theta}_e = \\bar{y}\\). Compute the expected value of each estimator, which one is biased? Compute the variance of each estimator. Which has lower variance?\n\nLet‚Äôs compute the MSE and discuss when the Bayesian estimator \\(\\hat{\\theta}_b\\) has lower MSE than the sample mean offline."
  },
  {
    "objectID": "notes/lec08-estimators.html#extra-practice",
    "href": "notes/lec08-estimators.html#extra-practice",
    "title": "Estimators",
    "section": "Extra practice",
    "text": "Extra practice\n\n\n\n\n\nSuppose you know that you know Gentoo penguins are closely related to Chinstrap penguins. Previously, you‚Äôve measured the bill length of three Gentoo penguins and found their mean bill length to be 46.2. Accordingly, you set \\(\\mu_0 = 46.2\\).\n\n\n\n\n\n\n\n\nSuppose (for illustrative purposes) that you know the true population mean and variance for Chinstrap penguin bill length,\n\\[\n\\begin{aligned}\n\\theta_0 &= 48.8\\\\\n\\sigma^2 &= 3.3.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nCompute \\(MSE[\\hat{\\theta}_e|\\theta_0]\\) and \\(MSE[\\hat{\\theta_b}|\\theta_0]\\) and plot the ratio \\(MSE[\\hat{\\theta}_b]/MSE[\\hat{\\theta}_e|\\theta_0]\\) as a function of \\(n\\) for \\(\\kappa_0 = 0, 1, 2, 3\\)."
  },
  {
    "objectID": "notes/lec08-Metropolis-algorithm.html",
    "href": "notes/lec08-Metropolis-algorithm.html",
    "title": "Metropolis Algorithm",
    "section": "",
    "text": "See libraries used in these notes\nlibrary(tidyverse)\nlibrary(latex2exp)\nlibrary(patchwork)\nlibrary(tidymodels)\nlibrary(mvtnorm)\nlibrary(coda)\nlibrary(animation)"
  },
  {
    "objectID": "notes/lec08-Metropolis-algorithm.html#example-sparrows",
    "href": "notes/lec08-Metropolis-algorithm.html#example-sparrows",
    "title": "Metropolis Algorithm",
    "section": "Example: Sparrows",
    "text": "Example: Sparrows\nIdea: the number of fledglings a sparrow has in a mating season depends on the age of the sparrow.\nLet \\(Y\\) be the number of fledglings (offspring) of a sparrow.\nLet \\(X\\) be the age of the sparrow.\nLet \\(Y|X\\) be conditionally iid.\n\nStep 1: data generative model\n\nExercise 1.1Solution (1.1)\n\n\nWrite down a data generative model for the data.\n\n\n\\[\nY | X \\sim Poisson(\\theta_x)\n\\]\nwhere \\(\\theta_x\\) is the age-specific expected number of off-spring. Note that \\(Y \\in \\{0, 1, 2, \\ldots\\}\\).\n\n\n\n\nExercise 1.2Poor solutionFalse solutionSolution (1.2)\n\n\nHow can we specify \\(\\theta_x\\)?\n\n\nLet each \\(\\theta_i\\) be unique and specific to age \\(i\\) of the sparrow.\nProblem: if we don‚Äôt collect much data on sparrows of a certain age \\(i\\), then our estimates for \\(\\theta_i\\) will be poor.\n\n\nLet \\(\\theta_x = f(x) = \\beta_1 + \\beta_2 x + \\beta_3 x^2\\).\nHere we have a way of relating ages to the expected number of fledglings and admit an \\(x^2\\) term to model the fact that the relationship between number of fledglings and age may not be a linear function of age.\nProblem: \\(\\theta_x\\) must be positive! The equation above can evaluate to negative values.\n\n\nLog-transform!\n\\(\\log \\mathbb{E}~Y|X = \\log \\theta_x = \\beta_1 + \\beta_2 x + \\beta_3 x^2\\)\nIn other words,\n\\(\\theta_x = e^{\\beta_1 + \\beta_2x + \\beta_3 x^2}\\) > 0\n\n\n\nSome terminology that will be useful in the future: \\(Y|X \\sim Poisson(e^{\\beta^T \\mathbf{x}})\\) is ‚ÄúPoisson regression‚Äù. Where \\(\\beta^T \\mathbf{x} = \\beta_1 + \\beta_2 x + \\beta_2 x^2\\).\n\\(\\beta^T \\mathbf{x}\\) is called the ‚Äúlinear predictor‚Äù.\n\nExercise 1.3Solution (1.3)\n\n\nWrite down \\(p(y_1,\\ldots y_n | x_1, \\ldots, x_n, \\beta_1, \\beta_2, \\beta_3)\\).\n\n\n\\[\n\\begin{aligned}\np(y_1,\\ldots y_n | x_1, \\ldots, x_n, \\beta_1, \\beta_2, \\beta_3) &= \\prod_{i = 1}^n p(y_i|x_i, \\beta_1, \\beta_2, \\beta_3) \\text{ by conditionally iid}\\\\\n&= \\prod_{i=1}^n \\theta_{x_i}^{y_i} e^{- \\theta_{x_i}}\\frac{1}{y_i!}\\\\\n&= \\prod_{i=1}^n e^{(\\beta^T \\mathbf{x}_i) y_i - e^{(\\beta^T \\mathbf{x}_i)}} \\frac{1}{y_i!}\\\\\n&= e^{\\sum_{i=1}^n \\left[(\\beta^T \\mathbf{x}_i) y_i - e^{(\\beta^T \\mathbf{x}_i)} \\right]} \\cdot\n\\prod_{i=1}^n \\frac{1}{y_i!}\n\\end{aligned}\n\\]\n\n\n\n\n\nStep 2: prior\n\nExercise 2.1Solution (2.1)\n\n\nWhat‚Äôs unknown?\n\n\n\\[\n\\beta_1, \\beta_2, \\beta_3\n\\]\n\n\n\n\nExercise 2.2Solution (2.2)\n\n\nWrite down a prior distribution for the unknowns\n\n\nOne possible prior: independent normals on each \\(\\beta_i\\).\n\\[\n\\begin{aligned}\n\\beta_i &\\sim N(0, 10)\\\\\np(\\beta_1, \\beta_2, \\beta_3) &= \\text{dnorm}(\\beta_1; 0, \\sqrt{10}) \\cdot \\text{dnorm}(\\beta_2; 0, \\sqrt{10}) \\cdot \\text{dnorm}(\\beta_3; 0, \\sqrt{10})\n\\end{aligned}\n\\]\n\n\n\n\n\nStep 3: posterior sampling\nThe posterior, given by\n\\[\np(\\beta_1, \\beta_2, \\beta_3 | y_1,\\ldots, y_n, x_1,\\ldots, x_n) =\n\\frac{p(\\beta_1, \\beta_2,\\beta_3) p(y_1,\\ldots y_n | x_1, \\ldots, x_n, \\beta_1, \\beta_2, \\beta_3)}{\n\\int \\int \\int p(\\beta_1, \\beta_2,\\beta_3) p(y_1,\\ldots y_n | x_1, \\ldots, x_n, \\beta_1, \\beta_2, \\beta_3)~d\\beta_1 d\\beta_2 d\\beta_3\n}\n\\]\nis too complicated to write down a closed form expression for due to the denominator ‚Äú\\(p(y)\\)‚Äù.\nOur goal: generate a series of dependent samples from the posterior as an empirical approximation to make inference.\nThe Metropolis algorithm is one of many methods (but not the only method) to construct a Markov chain comprised of dependent samples from the target distribution.\nMore broadly, constructing a Markov chain of dependent samples and using these samples to approximate the target distribution is called Markov chain Monte Carlo (MCMC).\n\nMarkov chainMarkov propertyExample\n\n\nDefinition: a sequence of random variables \\(\\theta^{(1)}, \\theta^{(2)}, \\theta^{(3)}, \\ldots\\) satisfying the ‚ÄúMarkov property‚Äù.\n\n\n\\[\np(\\theta^{(s+1)}| \\theta^{(1)}, \\theta^{(2)}, \\ldots, \\theta^{(s)}) = p(\\theta^{(s+1)}| \\theta^{(s)})\n\\]\nFor all states \\(s\\).\nThis is also called the ‚Äúmemoryless property‚Äù. In other words, ‚ÄúWhat happens next depends only on the state of affairs now‚Äù.\n\n\n\nset.seed(360)\nS = 10\ntheta_s = 0 # starting point for the Markov chain\nTHETA = NULL\nfor(i in 1:S) {\n  THETA = c(THETA, theta_s)\n  theta_s = rnorm(1, theta_s, 3)\n}\nTHETA\n\n [1]  0.0000000  4.3124838  5.2802035  4.6673136  1.6700287  1.5575283\n [7] -0.6967649 -2.6485497 -3.1040911 -5.6181892\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMCMC sampling algorithms are not models. They do not generate more information than is in the data and the prior. They are simply ways of ‚Äúlooking at‚Äù the posterior."
  },
  {
    "objectID": "notes/lec08-Metropolis-algorithm.html#the-bayesian-statistical-procedure",
    "href": "notes/lec08-Metropolis-algorithm.html#the-bayesian-statistical-procedure",
    "title": "Metropolis Algorithm",
    "section": "The Bayesian statistical procedure",
    "text": "The Bayesian statistical procedure\n\nWe setup a data generative model, \\(p(y | \\boldsymbol{\\theta})\\) where \\(\\boldsymbol{\\theta} = \\{ \\theta_1, \\theta_2, \\ldots \\theta_n\\}\\).\nNext, we choose a prior distribution for the unknown model parameters \\(p(\\boldsymbol{\\theta})\\).\nWe wish to make inferences using the data we collect \\(\\boldsymbol{y} = \\{y_1,\\ldots y_n\\}\\). All inferences we make require the posterior \\(p(\\boldsymbol{\\theta}| \\boldsymbol{y})\\), which we obtain from the data generative model and the prior via Bayes‚Äô rule.\n\nIn general, the inferences we wish to make, e.g.¬†\\(p(g(\\boldsymbol{\\theta}) \\in A |~\\boldsymbol{y})\\), are complicated or impossible to compute analytically. Here, Monte Carlo approximation helps. The key idea is that we sample from the posterior and then use the samples an an empirical approximation to make inference.\nQuestion: What do we do when we can‚Äôt sample directly from the posterior?"
  },
  {
    "objectID": "notes/lec08-Metropolis-algorithm.html#metropolis-algorithm",
    "href": "notes/lec08-Metropolis-algorithm.html#metropolis-algorithm",
    "title": "Metropolis Algorithm",
    "section": "Metropolis Algorithm",
    "text": "Metropolis Algorithm\n\n\n\n\n\n\n\n\\(\\theta\\)\nVector of unknown parameters.\n\n\n\\(\\theta^{(s)}\\)\nCurrent state of \\(\\theta\\) in the Markov chain.\n\n\n\\(J(\\theta | \\theta^{(s)})\\)\nProposal distribution. Note: for this to be a ‚ÄúMetropolis algorithm‚Äù, J needs to be symmetric, i.e.¬†\\(J(\\theta_a | \\theta_b) = J(\\theta_b | \\theta_a)\\) for all \\(\\{\\theta_a, \\theta_b\\}\\).\n\n\n\\(\\pi(\\theta)\\)\nTarget distribution that we wish to sample from (in our cases, this is the posterior \\(p(\\theta | y_1,\\ldots, y_n)\\)).\n\n\n\nThe Metropolis algorithm proceeds:\n\nSample \\(\\theta^* | \\theta^{(s)} \\sim J(\\theta | \\theta^{(s)})\\)\nCompute the acceptance ratio \\(r = \\frac{\\pi(\\theta^*)}{\\pi(\\theta^{(s)})}\\)\nLet \\[\n\\theta^{(s+1)} =\n\\begin{cases}\n\\theta^* \\text{ with probability } \\min(r, 1)\\\\\n\\theta^{(s)} \\text{ with probability } 1 - \\min(r, 1)\n\\end{cases}\n\\]\n\n\n\n\n\n\n\nBig idea\n\n\n\n\nOur target distribution, \\(\\pi(\\theta)\\), is the posterior \\(p(\\theta | y_1,\\ldots, y_n)\\). So the acceptance ratio in the algorithm is a ratio of posteriors.\nWhen we evaluate the ratio of posteriors at different \\(\\theta\\), the high dimensional integral in the denominator of the posterior, \\(p(y_1,\\ldots,y_n)\\), cancels out! We don‚Äôt have to compute it!"
  },
  {
    "objectID": "notes/lec08-Metropolis-algorithm.html#finishing-step-3",
    "href": "notes/lec08-Metropolis-algorithm.html#finishing-step-3",
    "title": "Metropolis Algorithm",
    "section": "Finishing step (3)",
    "text": "Finishing step (3)\nThe fledglings of female song sparrows. To begin, let‚Äôs load the data.\n\n\nLoad the data\nyX = structure(c(3, 1, 1, 2, 0, 0, 6, 3, 4, 2, 1, 6, 2, 3, 3, 4, 7, \n2, 2, 1, 1, 3, 5, 5, 0, 2, 1, 2, 6, 6, 2, 2, 0, 2, 4, 1, 2, 5, \n1, 2, 1, 0, 0, 2, 4, 2, 2, 2, 2, 0, 3, 2, 1, 1, 1, 1, 1, 1, 1, \n1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, \n2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 5, \n5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 6, 1, 1, 9, 9, 1, 1, 1, 1, 1, 1, \n1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 25, 25, 16, 16, 16, 16, 16, \n16, 16, 16, 16, 16, 16, 16, 25, 16, 16, 16, 16, 25, 25, 25, 25, \n9, 9, 9, 9, 9, 9, 9, 36, 1, 1), .Dim = c(52L, 4L), .Dimnames = list(\n    NULL, c(\"fledged\", \"intercept\", \"age\", \"age2\")))\n\n\n\nyX %>%\n  head(n = 5)\n\n     fledged intercept age age2\n[1,]       3         1   3    9\n[2,]       1         1   3    9\n[3,]       1         1   1    1\n[4,]       2         1   1    1\n[5,]       0         1   1    1\n\ny = yX[,1]\nX = yX[,-1]\n\nThe model:\n\\[\n\\begin{aligned}\nY | X &\\sim \\text{Poisson}(\\exp[ \\beta^T \\boldsymbol{x}])\\\\\n\\beta &\\sim MVN(0, \\sqrt{10})\n\\end{aligned}\n\\]\nThe Metropolis algorithm with\n\\[\nJ(\\beta | \\beta^{(s)}) = MVN(\\beta^{(s)}, \\hat{\\sigma}^2(X^TX)^{-1})\n\\]\nwhere \\(\\hat{\\sigma}^2\\) is the sample variance of \\(\\{\\log(y_1 + 1/2), \\ldots, \\log(y_n + 1/2)\\}\\).\n\n\n\n\n\n\nNote\n\n\n\n\nThis variance is intuitively useful choice for \\(\\delta\\) since the posterior variance would be \\(\\sigma^2 (X^TX)^{-1}\\) in a normal regression problem.\nWe use \\(\\log(y + 1/2)\\) instead of \\(\\log y\\) because if \\(y=0\\), \\(\\log y\\) would be \\(-\\infty\\).\n\n\n\n\nset.seed(360)\nn = length(y)\np = ncol(X)\n\npmn.beta = rep(0, p) # prior mean beta\npsd.beta = rep(10, p) # prior sd beta\n\nvar.prop = var(log(y + 1/2)) * solve(t(X) %*% X) # proposal variance\n\nS = 10000\nbeta = rep(0, p); accepts = 0\nBETA = matrix(0, nrow = S, ncol = p)\nset.seed(1)\n\nfor (s in 1:S) {\n  # multivariate proposal of beta\n  beta.p = t(rmvnorm(1, beta, var.prop))\n  \n  # log ratio\n  lhr = sum(dpois(y, exp(X %*%beta.p), log = TRUE)) -\n    sum(dpois(y, exp(X %*% beta), log = TRUE)) + \n    sum(dnorm(beta.p, pmn.beta, psd.beta, log = TRUE)) -\n    sum(dnorm(beta, pmn.beta, psd.beta, log = TRUE)) \n  \n  if (log(runif(1)) < lhr) {\n    beta = beta.p ; accepts = accepts + 1\n  }\n  \n  BETA[s,] = beta\n}\n\nThe acceptance ratio is 0.428\nLet‚Äôs examine convergence.\n\ntrace plotsplot codeESSacf\n\n\n\n\n\n\n\n\n\n\nvalue = c(BETA[,1], BETA[,2], BETA[,3])\nn = length(value)\nbeta = c(rep(\"beta1\", n/3), rep(\"beta2\", n/3), rep(\"beta3\", n/3))\ndf = data.frame(value = value,\n                beta = beta) \n\ndf %>%\n  ggplot(aes(x = 1:nrow(df), y = value)) + \n  geom_line() + \n  facet_wrap(~ beta, scales = \"free_x\") +\n  theme_bw() +\n  labs(x = \"iteration\")\n\n\n\n\n# effective sample size\nBETA %>%\n  apply(2, effectiveSize)\n\n[1] 867.4750 825.6214 692.0495\n\n\n\n\n\npar(mfrow=c(1,3))\nacf(BETA[,1])\nacf(BETA[,2])\nacf(BETA[,3])"
  },
  {
    "objectID": "notes/lec08-Metropolis-algorithm.html#another-example",
    "href": "notes/lec08-Metropolis-algorithm.html#another-example",
    "title": "Metropolis Algorithm",
    "section": "Another example",
    "text": "Another example\nLet \\(\\pi(\\theta) = \\text{dnorm}(\\theta, 10, 1)\\) and let \\(J(\\theta | \\theta^{(s)}) = \\text{normal}(\\theta^{(s)},\\delta^2)\\).\nWe have to choose \\(\\delta\\). How should we choose it? Let‚Äôs gain some intuition by trying out three different values of \\(\\delta\\).\n\nset.seed(360)\ntheta_s = 0 # starting point\nTHETA = NULL # empty object to save iterations in\nS = 10000 # number of iterations\ndelta = 1 # proposal sd\naccept = 0 # keep track of acceptance rate\n\nfor (s in 1:S) {\n  # log everything for numerical stability #\n  \n  ### generate proposal and compute ratio r ###\n  theta_proposal = rnorm(1, mean = theta_s, sd = delta) \n  log.r = dnorm(theta_proposal, mean = 10, sd = 1, log = TRUE) - \n    dnorm(theta_s, mean = 10, sd = 1, log = TRUE)\n  \n  ### accept or reject proposal and add to chain ###\n  if(log(runif(1)) < log.r)  {\n    theta_s = theta_proposal\n    accept = accept + 1 \n  }\n  THETA = c(THETA, theta_s)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs look at how various \\(\\delta\\) let us sample the target:\n\n\\(\\delta = 0.1\\)\\(\\delta = 1\\)\\(\\delta = 4\\)\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs look at the trace plots for each \\(\\delta\\).\n\n\n\n\n\n\n\n\n\n\n\nAcceptance rate for each delta\n\n\n0.1\n1\n4\n\n\n\n\n0.96\n0.7\n0.29"
  },
  {
    "objectID": "notes/lec09-MCMC.html",
    "href": "notes/lec09-MCMC.html",
    "title": "MCMC Properties and Diagnostics",
    "section": "",
    "text": "View libraries used in these notes\nlibrary(tidyverse)"
  },
  {
    "objectID": "notes/lec09-MCMC.html#ergodic-theorem",
    "href": "notes/lec09-MCMC.html#ergodic-theorem",
    "title": "MCMC Properties and Diagnostics",
    "section": "Ergodic theorem",
    "text": "Ergodic theorem\nUnder what conditions does Metropolis-Hastings MCMC work?\nErgodic theorem: If \\(\\{\\theta^{(1)}, \\theta^{(2)}, \\ldots \\}\\) is an irreducible, aperiodic and recurrent Markov chain, then there is a unique probability distribution \\(\\pi\\) such that as \\(s \\rightarrow \\infty\\),\n\n\\(Pr(\\theta^{(s)} \\in \\mathcal{A}) \\rightarrow \\pi(\\mathcal{A})\\) for any set \\(\\mathcal{A}\\);\n\\(\\frac{1}{S} \\sum g(\\theta^{(s)}) \\rightarrow \\int g(x) \\pi(x) dx\\)."
  },
  {
    "objectID": "notes/lec09-MCMC.html#definitions",
    "href": "notes/lec09-MCMC.html#definitions",
    "title": "MCMC Properties and Diagnostics",
    "section": "Definitions",
    "text": "Definitions\n\nstationary distribution\n\\(\\pi\\) is called the stationary distribution of the Markov chain because if \\(\\theta^{(s)} \\sim \\pi\\) and \\(\\theta^{(s+1)}\\) is generated from the Markov chain starting at \\(\\theta^{(s)}\\), then \\(Pr(\\theta^{(s+1)} \\in \\mathcal{A}) = \\pi(\\mathcal{A})\\).\n\n\nirreducible\nA chain is reducible if the state-space can be divided into non-overlapping sets (due to some \\(J\\)). In practice, the proposal \\(J(\\theta^* | \\theta^{(s)})\\) needs to let us go from any value of \\(\\theta\\) to any other, eventually.\n\n\naperiodic\nWe want our Markov chain to be aperiodic. A value \\(\\theta\\) is said to be periodic with period \\(k>1\\) if it can only be visited every \\(k\\)th iteration. A Markov chain without periodic states is aperiodic.\n\n\nrecurrent\nA value \\(\\theta\\) is recurrent if we are guaranteed to return to it eventually."
  },
  {
    "objectID": "notes/lec09-MCMC.html#mcmc-vocabulary",
    "href": "notes/lec09-MCMC.html#mcmc-vocabulary",
    "title": "MCMC Properties and Diagnostics",
    "section": "MCMC Vocabulary",
    "text": "MCMC Vocabulary\n\nautocorrelation: how correlated consecutive values in the Markov chain are. Mathematically, we compute the sample autocorrelation between elements in the sequence that are \\(t\\) steps apart using\n\n\\[\n\\text{acf}_t(\\boldsymbol{\\phi}) =\n\\frac{\\frac{1}{S - t} \\sum_{s = 1}^{S-t} (\\phi_s - \\bar{\\phi})(\\phi_{s+t} - \\bar{\\phi})}\n{\\frac{1}{S-1} \\sum_{s = 1}^S (\\phi_s - \\bar{\\phi})^2}\n\\] where \\(\\boldsymbol{\\phi}\\) is a sequence of length \\(S\\) and \\(\\bar{\\phi}\\) is the mean of the sequence. Practically, we use acf function in R. Example:\n\nacf(THETA1, plot = FALSE)\n\n\nAutocorrelations of series 'THETA1', by lag\n\n    0     1     2     3     4     5     6     7     8     9    10    11    12 \n1.000 0.890 0.808 0.746 0.695 0.652 0.614 0.578 0.541 0.507 0.475 0.445 0.422 \n   13    14    15    16    17    18    19    20    21    22    23    24    25 \n0.401 0.382 0.364 0.346 0.325 0.303 0.282 0.266 0.250 0.235 0.223 0.212 0.202 \n   26    27    28    29    30    31    32    33    34    35    36    37    38 \n0.192 0.185 0.177 0.169 0.163 0.160 0.157 0.154 0.148 0.141 0.136 0.131 0.129 \n   39    40 \n0.123 0.115 \n\n\nThe higher the autocorrelation, the more samples we need to obtain a given level of precision for our approximation. One way to state how precise our approximation is, is with effective sample size.\n\neffective sample size (ESS): intuitively this is the effective number of exact samples ‚Äúcontained‚Äù in the Markov chain (see Betancourt 2018). For further reading on ESS, see the stan manual. In practice we use coda::effectiveSize() function to compute. Example:\n\n\nlibrary(coda)\neffectiveSize(THETA1)\n\n    var1 \n356.3798 \n\n\nMore precisely, the effective sample size (ESS) is the value \\(S_{eff}\\) such that\n\\[\nVar_{MCMC}[\\bar{\\phi}] = \\frac{Var[\\phi]}{S_{eff}}.\n\\] In words, it‚Äôs the number of independent Monte Carlo samples necessary to give the same precision as the MCMC samples. For comparison, recall \\(Var_{MC}[\\bar{\\phi}] = Var[\\phi]/S\\)\n\nStationarity is when samples taken in one part of the chain have a similar distribution to samples taken from other parts of the chain. Intuitively, we want the particle to move from our arbitrary starting point to regions of higher probability\\(^*\\), then we will say it has achieved stationarity.\n\nTraceplots are a great way to visually inspect whether a chain has converged, or achieved stationarity. In the traceplot from the previous lecture, we can see that samples from the beginning of the chain look very different than samples at the end for delta = 0.1.\n\\(^*\\) recall that probability is really a volume in high dimensions of parameter space, and so it is not enough for a pdf to evaluate to a high value, there must also be sufficient volume.\n\nMixing: how well the particle moves between sets of high probability. Some might refer to this as how well the particle sojourns across the ‚Äútypical set‚Äù (regions of high probability)."
  },
  {
    "objectID": "notes/lec09-MCMC.html#the-blind-monkey-on-an-island",
    "href": "notes/lec09-MCMC.html#the-blind-monkey-on-an-island",
    "title": "MCMC Properties and Diagnostics",
    "section": "The blind monkey on an island",
    "text": "The blind monkey on an island"
  },
  {
    "objectID": "hw/hw05.html",
    "href": "hw/hw05.html",
    "title": "Homework 5",
    "section": "",
    "text": "You‚Äôve been hired as a columnist to write for the statistics/mathematics section of an online magazine. Your editor knows you are enrolled in STA360 and has asked you to write about the Metropolis algorithm for the magazine. Specifically, your editor tells you that your article should answer the following:\n\nWhat is the Metropolis algorithm? What is the history of the algorithm?\nWhat can the Metropolis algorithm be used for?\nWhat is a proposal distribution?\nWhat‚Äôs the intuition behind why the algorithm works?\n\nAdditionally, the editor adds: your article needs to be strictly under 500 words (the shorter the better). Furthermore, your target audience is non-technical. For this reason, you should avoid using unnecessary jargon. If/when you do use statistical terminology, you must define it."
  },
  {
    "objectID": "hw/hw05.html#exercise-2",
    "href": "hw/hw05.html#exercise-2",
    "title": "Homework 5",
    "section": "Exercise 2",
    "text": "Exercise 2\nYounger male sparrows may or may not nest during a mating season, perhaps depending on their physical characteristics. Researchers have recorded the nesting success of 43 young male sparrows of the same age, as well as their wingspan. Run the code below to load the data:\n\nyXsparrow = readr::read_csv(\"https://sta360-fa24.github.io/data/yXsparrow.csv\")\n\nLet \\(Y_i\\) be the binary indicator that sparrow \\(i\\) successfully nests, and let \\(x_i\\) denote their wingspan. Our model for \\(Y_i\\) is logit \\(Pr(Y_i = 1 | \\alpha, \\beta, x_i) = \\alpha + \\beta x_i\\), where the logit function is given by logit \\(\\theta = \\log \\left[\\frac{\\theta}{(1 - \\theta)} \\right]\\).\n\nWrite down the likelihood,\n\n\\[\n\\prod_{i=1}^n p(y_i | \\alpha, \\beta, x_i)\n\\]\nand simplify as much as possible.\n\nFormulate a prior probability distribution over \\(\\alpha\\) and \\(\\beta\\) by considering the range of \\(Pr(Y = 1 | \\alpha, \\beta, x)\\) as \\(x\\) ranges over 10 to 15, the approximate range of observed wingspans.\nImplement a Metropolis algorithm that approximates \\(p(\\alpha, \\beta | \\mathbf{y}, \\mathbf{x})\\). \\(\\mathbf{y} = y_1, \\ldots, y_n\\) and \\(\\mathbf{x} = x_1, \\ldots x_n\\). Adjust the proposal distribution to achieve a reasonable acceptance rate, and run the algorithm long enough so that the effective sample size is at least 100 for each parameter.\nReport trace plots for \\(\\alpha\\) and \\(\\beta\\). Additionally, compare the posterior densities of \\(\\alpha\\) and \\(\\beta\\) to their prior densities.\nUsing the output of the Metropolis algorithm, come up with a way to make a confidence band for the following function \\(f_{\\alpha, \\beta}(x)\\) of wingspan:\n\n\\[\nf_{\\alpha, \\beta}(x) = \\frac{e^{\\alpha + \\beta x}}{1 + e^{\\alpha + \\beta x}}\n\\]\nwhere \\(\\alpha\\) and \\(\\beta\\) are the parameters in your data generative model. Make a plot of such a band."
  },
  {
    "objectID": "hw/hw05.html#exercise-3",
    "href": "hw/hw05.html#exercise-3",
    "title": "Homework 5",
    "section": "Exercise 3",
    "text": "Exercise 3\nCode for this exercise is provided below,\n\n# load the data\ntrans.prob.mat = readRDS(url(\"https://sta360-fa24.github.io/data/trans-prob-mat.rds\"))\ncipher_text = readLines(\"https://sta360-fa24.github.io/data/ciphertext.txt\")\n\npl = function(decoded) {\n  logprob = 0\n  prevletter = \"SPACE\"\n  for (i in 1:nchar(decoded)) {\n    curletter = substring(decoded, i, i)\n    if(curletter == \" \") {\n      curletter = \"SPACE\"\n    }\n    logprob = logprob + log(trans.prob.mat[rownames(trans.prob.mat) == prevletter,\n                                             colnames(trans.prob.mat) == curletter])\n    prevletter = curletter\n  }\n  return(logprob)\n} \n\nIn this exercise we will re-create the cryptanalysis tool described here to decrypt a secret message. Read pages 1-3 of the article by Persi Diaconis linked above.\n\nLoad the object trans.prob.matrix using the code above and examine. Based on your reading of the article, how can you interpret the entries of this matrix? Is it symmetric or not? Why does this make sense? The function pl(), given above, computes the ‚Äúplausibility‚Äù score for a given decoding. Explain in detail what the code comprising pl() does.\nThe following code can help you get started:\n\n\n## alphabet with space\nalphabet = c(LETTERS, \" \")\n\n## decode: replace ciphertext with mapping\n\ndecode = function(mapping, coded) {\n  coded = toupper(coded)\n  decoded = coded\n  for (i in 1:nchar(coded)) {\n      substring(decoded, i, i) = alphabet[mapping == substring(coded, i, i)]\n  }\n  decoded\n}\n\nExplain, line-by-line what the function decode() above does. What are the arguments? What does the function return?\n\nFollow the pseudo-code outlined on page 2 of the article to write a MCMC algorithm and decrypt the secret message. Run your Markov chain for at least 1000 iterations and report the decoding with the highest plausibility score."
  },
  {
    "objectID": "slides/lab6-mcmc-d-practice.html#exercise-3",
    "href": "slides/lab6-mcmc-d-practice.html#exercise-3",
    "title": "MCMC diagnostics practice",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "notes/lec08-Metropolis-algorithm.html#the-blind-monkey-on-an-island",
    "href": "notes/lec08-Metropolis-algorithm.html#the-blind-monkey-on-an-island",
    "title": "Metropolis Algorithm",
    "section": "The blind monkey on an island",
    "text": "The blind monkey on an island"
  },
  {
    "objectID": "slides/lab9-MH-MCMC.html#metropolis-hastings-algorithm",
    "href": "slides/lab9-MH-MCMC.html#metropolis-hastings-algorithm",
    "title": "MCMC Practice",
    "section": "Metropolis-Hastings algorithm",
    "text": "Metropolis-Hastings algorithm\nLet \\(\\pi(\\theta)\\) be the target distribution. The Metropolis-Hastings algorithm proceeds:\n\nsample \\(\\theta^{*} \\sim J(\\theta | \\theta^{(s)})\\);\ncompute the acceptance ratio\n\n\\[\nr = \\frac{\\pi(\\theta^*)}{\\pi(\\theta^{(s)})} \\times\n\\frac{J(\\theta^{(s)}| \\theta^*)}{\nJ(\\theta^{*}| \\theta^{(s)})\n}\n\\]\n\nset \\(\\theta^{(s+1)}\\) to \\(\\theta^*\\) with probability \\(\\min(1, r)\\), otherwise set \\(\\theta^{(s+1)}\\) to \\(\\theta^{(s)}\\).\n\nImportant: We correct for asymmetry; the proposal distribution \\(J\\) need not be symmetric!"
  },
  {
    "objectID": "slides/confidenceBand-and-diagnostics.html#model",
    "href": "slides/confidenceBand-and-diagnostics.html#model",
    "title": "MCMC diagonstics & confidence bands",
    "section": "Model",
    "text": "Model\nConsider the following data generative model and priors:\n\\[\n\\begin{aligned}\nY_i | \\beta_0, \\beta_1, x_i &\\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\\\\n\\beta_0, \\beta_1 &\\sim \\text{ iid } N(0, 2)\\\\\n\\sigma^2 &\\sim \\text{inverse-gamma}(2, 3)\n\\end{aligned}\n\\]\nTip: load library(invgamma) to use the dinvgamma function."
  },
  {
    "objectID": "slides/confidenceBand-and-diagnostics.html#exercise-1",
    "href": "slides/confidenceBand-and-diagnostics.html#exercise-1",
    "title": "MCMC diagonstics & confidence bands",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nIs a Metropolis or Metropolis-Hastings algorithm used to sample \\(\\sigma^2\\)? Hint: What is the proposal distribution used to sample \\(\\sigma^2\\)? Is it symmetric? Can you show that it is or is not symmetric in R?\nCreate a trace plot for each parameter. How many iterations does it take for \\(\\beta_1\\) to converge to the posterior? What is going on with the \\(\\sigma^2\\) trace plot?\nReport the effective sample size of each parameter, \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\). Hint: load the coda package and check the lecture notes."
  },
  {
    "objectID": "slides/confidenceBand-and-diagnostics.html#setup",
    "href": "slides/confidenceBand-and-diagnostics.html#setup",
    "title": "MCMC diagonstics & confidence bands",
    "section": "Setup",
    "text": "Setup\nConsider the following data generative model and priors:\n\\[\n\\begin{aligned}\nY_i | \\beta_0, \\beta_1, x_i &\\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\\\\n\\beta_0, \\beta_1 &\\sim \\text{ iid } N(0, 2)\\\\\n\\sigma^2 &\\sim \\text{inverse-gamma}(2, 3)\n\\end{aligned}\n\\]\nTip: load library(invgamma) to use the dinvgamma function.\n\nlibrary(tidyverse)\nlibrary(invgamma)"
  },
  {
    "objectID": "slides/confidenceBand-and-diagnostics.html#code",
    "href": "slides/confidenceBand-and-diagnostics.html#code",
    "title": "MCMC diagonstics & confidence bands",
    "section": "Code",
    "text": "Code\nBelow is code to sample from\n\\[\np(\\beta_1, \\beta_2, \\sigma^2 | y_1, \\ldots, y_n, x_1, \\ldots, x_n)\n\\].\n\nset.seed(360)\nlogLikelihood = function(beta0, beta1, sigma2) {\n  mu = beta0 + (beta1 * x)\n  sum(dnorm(y, mu, sqrt(sigma2), log = TRUE))\n}\n\nlogPrior = function(beta0, beta1, sigma2) {\n  dnorm(beta0, 0, sqrt(2), log = TRUE) + \n    dnorm(beta1, 0, sqrt(2), log = TRUE) +\n    dinvgamma(sigma2, 2, 3, log = TRUE)\n}\n\nlogPosterior = function(beta0, beta1, sigma2) {\n  logLikelihood(beta0, beta1, sigma2) + logPrior(beta0, beta1, sigma2)\n}\n\n\nBETA0 = NULL\nBETA1 = NULL\nSIGMA2 = NULL\n\naccept1 = 0\naccept2 = 0\naccept3 = 0\n\nS = 500\n\nbeta0_s = 0.1\nbeta1_s = 10\nsigma2_s = 1\nfor (s in 1:S) {\n  \n  ## propose and update beta0\n  beta0_proposal = rnorm(1, mean = beta0_s, .5)\n   log.r = logPosterior(beta0_proposal, beta1_s, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta0_s = beta0_proposal\n    accept1 = accept1 + 1 \n   }\n   \n   BETA0 = c(BETA0, beta0_s)\n   \n   ## propose and update beta1\n    beta1_proposal = rnorm(1, mean = beta1_s, .5)\n   log.r = logPosterior(beta0_s, beta1_proposal, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta1_s = beta1_proposal\n    accept2 = accept2 + 1 \n   }\n   \n   BETA1 = c(BETA1, beta1_s)\n   \n   ## propose and update sigma2\n   ### note: sigma2 is positive only, we want to only propose positive values\n   sigma2_proposal = 1 / rgamma(1, shape = 1, sigma2_s)\n   log.r = logPosterior(beta0_s, beta1_s, sigma2_proposal) - \n     logPosterior(beta0_s, beta1_s, sigma2_s) + \n     dinvgamma(sigma2_proposal, 1, sigma2_s, log = TRUE) - \n     dinvgamma(sigma2_s, 1, sigma2_proposal, log = TRUE) \n   \n   if(log(runif(1)) < log.r)  {\n    sigma2_s = sigma2_proposal\n    accept3 = accept3 + 1 \n   }\n   \n   SIGMA2 = c(SIGMA2, sigma2_s)\n   \n}"
  },
  {
    "objectID": "slides/confidenceBand-and-diagnostics.html#exercise-2",
    "href": "slides/confidenceBand-and-diagnostics.html#exercise-2",
    "title": "MCMC diagonstics & confidence bands",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nGo back and increase the length of the chain, \\(S\\), until you obtain approximately 200 ESS for each parameter. Re-examine your plots and discuss.\nReport the posterior median and a 90% confidence interval for each unknown. Why is the median a better summary than the mean for \\(\\sigma^2\\)?"
  },
  {
    "objectID": "slides/confidenceBand-and-diagnostics.html#exercise-3",
    "href": "slides/confidenceBand-and-diagnostics.html#exercise-3",
    "title": "MCMC diagonstics & confidence bands",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nPlot the posterior mean \\(\\theta(x)\\) vs \\(x\\) where \\(\\theta(x) \\equiv \\beta_0 + \\beta_1 x\\).\nAdditionally, plot a 95% confidence band for \\(\\theta(x)\\) vs \\(x\\) and interpret your plot."
  },
  {
    "objectID": "slides/confidenceBand-and-diagnostics.html#solution-1-2",
    "href": "slides/confidenceBand-and-diagnostics.html#solution-1-2",
    "title": "MCMC diagonstics & confidence bands",
    "section": "Solution 1, 2",
    "text": "Solution 1, 2\n\nAlgo.Trace plots (img)Trace plots (code)ESSLonger runPost medians\n\n\nA Metropolis-Hastings algorithm. inverse-gamma proposal is asymmetric.\nShowing with R code:\n\nsigma2_proposal = 1.5\nsigma2_s = 1\ndinvgamma(sigma2_proposal, 1, sigma2_s) == \n  dinvgamma(sigma2_s, 1, sigma2_proposal)\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\\(\\beta_1\\) converges to the target after about 50 iterations.\nSome samples from inv-gamma are huge, the huge variance results in a flatter likelihood and still get accepted.\n\n\n\n\nlibrary(patchwork)\nparameterDF = data.frame(BETA0, BETA1, SIGMA2)\nn = nrow(parameterDF)\n\np0 = parameterDF %>%\n  ggplot(aes(x = 1:n)) +\n  geom_line(aes(y = BETA0)) +\n  theme_bw() +\n  labs(x = \"iteration\", y = \"beta0\")\n\np1 = parameterDF %>%\n  ggplot(aes(x = 1:n)) +\n  geom_line(aes(y = BETA1)) +\n  theme_bw() +\n  labs(x = \"iteration\", y = \"beta1\")\n\np2 = parameterDF %>%\n  ggplot(aes(x = 1:n)) +\n  geom_line(aes(y = SIGMA2)) +\n  theme_bw() +\n  labs(x = \"iteration\", y = \"sigma2\")\n\np0 + p1 + p2\n\n\n\n\nlibrary(coda) \n\nparameterDF = data.frame(BETA0, BETA1, SIGMA2)\napply(parameterDF, 2, effectiveSize)\n\n   BETA0    BETA1   SIGMA2 \n20.70251 12.72726 40.55270 \n\n\n\n\n\nset.seed(360)\nlogLikelihood = function(beta0, beta1, sigma2) {\n  mu = beta0 + (beta1 * x)\n  sum(dnorm(y, mu, sqrt(sigma2), log = TRUE))\n}\n\nlogPrior = function(beta0, beta1, sigma2) {\n  dnorm(beta0, 0, sqrt(2), log = TRUE) + \n    dnorm(beta1, 0, sqrt(2), log = TRUE) +\n    dinvgamma(sigma2, 2, 3, log = TRUE)\n}\n\nlogPosterior = function(beta0, beta1, sigma2) {\n  logLikelihood(beta0, beta1, sigma2) + logPrior(beta0, beta1, sigma2)\n}\n\n\nBETA0 = NULL\nBETA1 = NULL\nSIGMA2 = NULL\n\naccept1 = 0\naccept2 = 0\naccept3 = 0\n\nS = 10000\n\nbeta0_s = 0.1\nbeta1_s = 10\nsigma2_s = 1\nfor (s in 1:S) {\n  \n  ## propose and update beta0\n  beta0_proposal = rnorm(1, mean = beta0_s, .5)\n   log.r = logPosterior(beta0_proposal, beta1_s, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta0_s = beta0_proposal\n    accept1 = accept1 + 1 \n   }\n   \n   BETA0 = c(BETA0, beta0_s)\n   \n   ## propose and update beta1\n    beta1_proposal = rnorm(1, mean = beta1_s, .5)\n   log.r = logPosterior(beta0_s, beta1_proposal, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta1_s = beta1_proposal\n    accept2 = accept2 + 1 \n   }\n   \n   BETA1 = c(BETA1, beta1_s)\n   \n   ## propose and update sigma2\n   ### note: sigma2 is positive only, we want to only propose positive values\n   sigma2_proposal = 1 / rgamma(1, shape = 1, sigma2_s)\n   log.r = logPosterior(beta0_s, beta1_s, sigma2_proposal) - \n     logPosterior(beta0_s, beta1_s, sigma2_s) + \n     dinvgamma(sigma2_proposal, 1, sigma2_s, log = TRUE) - \n     dinvgamma(sigma2_s, 1, sigma2_proposal, log = TRUE) \n   \n   if(log(runif(1)) < log.r)  {\n    sigma2_s = sigma2_proposal\n    accept3 = accept3 + 1 \n   }\n   \n   SIGMA2 = c(SIGMA2, sigma2_s)\n   \n}\n\nparameterDF = data.frame(BETA0, BETA1, SIGMA2)\napply(parameterDF, 2, effectiveSize)\n\n   BETA0    BETA1   SIGMA2 \n405.7465 241.8751 826.7580 \n\n\n\n\n\n\n\n\n\n\npost. median\nCI\n\n\n\n\nbeta0\n0.5\n(-1.1, 1.9)\n\n\nbeta1\n2\n(-0.1, 2.5)\n\n\nsigma2\n13.4\n(7.3, 2537295.4)"
  },
  {
    "objectID": "slides/confidenceBand-and-diagnostics.html#solution-3",
    "href": "slides/confidenceBand-and-diagnostics.html#solution-3",
    "title": "MCMC diagonstics & confidence bands",
    "section": "Solution 3",
    "text": "Solution 3\n\nplotcode\n\n\n\n\n\n\n\nThe red line shows our posterior expectation of \\(\\theta(x)\\) for each \\(x\\). The black bands show our 95% confidence interval \\(\\theta(x)\\).\n\n\n\nget_theta_CI = function(X) {\n     f = BETA0 + (BETA1 * X)\n     return(quantile(f, c(0.025, 0.975)))\n}\n\nget_theta_mean = function(X) {\n  f = BETA0 + (BETA1 * X)\n  return(mean(f))\n}\n\nxlo = min(x)\nxhi = max(x)\nxVal = seq(xlo, xhi, by = 0.01)\nlower = NULL\nupper = NULL\nM = NULL\n   \nfor (i in seq_along(xVal)) {\n  theta_CI = get_theta_CI(xVal[i])\n  lower = c(lower, theta_CI[[1]])\n  upper = c(upper, theta_CI[[2]])\n  M = c(M, get_theta_mean(xVal[i]))\n}\n\ndf = data.frame(xVal, lower, upper, M)\ndf %>%\n  ggplot(aes(x = xVal)) +\n  geom_line(aes(y = lower)) +\n  geom_line(aes(y = upper)) +\n  geom_line(aes(y = M), col = \"red\") +\n  theme_bw() +\n  labs(y = \"theta\",\n       x = \"x\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "slides/lab-conf-band.html#setup",
    "href": "slides/lab-conf-band.html#setup",
    "title": "MCMC & confidence bands",
    "section": "Setup",
    "text": "Setup\nConsider the following data generative model and priors:\n\\[\n\\begin{aligned}\nY_i | \\beta_0, \\beta_1, x_i &\\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\\\\n\\beta_0, \\beta_1 &\\sim \\text{ iid } N(0, 2)\\\\\n\\sigma^2 &\\sim \\text{inverse-gamma}(2, 3)\n\\end{aligned}\n\\]\nTip: load library(invgamma) to use the dinvgamma function.\n\nlibrary(tidyverse)\nlibrary(invgamma)\n\n\n\n\n\n# load the data\nYX = readr::read_csv(\"https://sta360-fa24.github.io/data/simulatedXY.csv\")\ny = YX$y\nx = YX$x"
  },
  {
    "objectID": "slides/lab-conf-band.html#code",
    "href": "slides/lab-conf-band.html#code",
    "title": "MCMC & confidence bands",
    "section": "Code",
    "text": "Code\nBelow is code to sample from\n\\[\np(\\beta_1, \\beta_2, \\sigma^2 | y_1, \\ldots, y_n, x_1, \\ldots, x_n)\n\\].\n\nset.seed(360)\nlogLikelihood = function(beta0, beta1, sigma2) {\n  mu = beta0 + (beta1 * x)\n  sum(dnorm(y, mu, sqrt(sigma2), log = TRUE))\n}\n\nlogPrior = function(beta0, beta1, sigma2) {\n  dnorm(beta0, 0, sqrt(2), log = TRUE) + \n    dnorm(beta1, 0, sqrt(2), log = TRUE) +\n    dinvgamma(sigma2, 2, 3, log = TRUE)\n}\n\nlogPosterior = function(beta0, beta1, sigma2) {\n  logLikelihood(beta0, beta1, sigma2) + logPrior(beta0, beta1, sigma2)\n}\n\n\nBETA0 = NULL\nBETA1 = NULL\nSIGMA2 = NULL\n\naccept1 = 0\naccept2 = 0\naccept3 = 0\n\nS = 500\n\nbeta0_s = 0.1\nbeta1_s = 10\nsigma2_s = 1\nfor (s in 1:S) {\n  \n  ## propose and update beta0\n  beta0_proposal = rnorm(1, mean = beta0_s, .5)\n   log.r = logPosterior(beta0_proposal, beta1_s, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta0_s = beta0_proposal\n    accept1 = accept1 + 1 \n   }\n   \n   BETA0 = c(BETA0, beta0_s)\n   \n   ## propose and update beta1\n    beta1_proposal = rnorm(1, mean = beta1_s, .5)\n   log.r = logPosterior(beta0_s, beta1_proposal, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta1_s = beta1_proposal\n    accept2 = accept2 + 1 \n   }\n   \n   BETA1 = c(BETA1, beta1_s)\n   \n   ## propose and update sigma2\n   ### note: sigma2 is positive only, we want to only propose positive values\n   sigma2_proposal = 1 / rgamma(1, shape = 1, sigma2_s)\n   log.r = logPosterior(beta0_s, beta1_s, sigma2_proposal) - \n     logPosterior(beta0_s, beta1_s, sigma2_s) + \n     dinvgamma(sigma2_proposal, 1, sigma2_s, log = TRUE) - \n     dinvgamma(sigma2_s, 1, sigma2_proposal, log = TRUE) \n   \n   if(log(runif(1)) < log.r)  {\n    sigma2_s = sigma2_proposal\n    accept3 = accept3 + 1 \n   }\n   \n   SIGMA2 = c(SIGMA2, sigma2_s)\n   \n}"
  },
  {
    "objectID": "slides/lab-conf-band.html#exercise-1",
    "href": "slides/lab-conf-band.html#exercise-1",
    "title": "MCMC & confidence bands",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nIs a Metropolis or Metropolis-Hastings algorithm used to sample \\(\\sigma^2\\)? Hint: What is the proposal distribution used to sample \\(\\sigma^2\\)? Is it symmetric? Can you show that it is or is not symmetric in R?\nCreate a trace plot for each parameter. How many iterations does it take for \\(\\beta_1\\) to converge to the posterior? What is going on with the \\(\\sigma^2\\) trace plot?\nReport the effective sample size of each parameter, \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\). Hint: load the coda package and check the lecture notes."
  },
  {
    "objectID": "slides/lab-conf-band.html#exercise-2",
    "href": "slides/lab-conf-band.html#exercise-2",
    "title": "MCMC & confidence bands",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nGo back and increase the length of the chain, \\(S\\), until you obtain approximately 200 ESS for each parameter. Re-examine your plots and discuss.\nReport the posterior median and a 90% confidence interval for each unknown. Why is the median a better summary than the mean for \\(\\sigma^2\\)?"
  },
  {
    "objectID": "slides/lab-conf-band.html#exercise-3",
    "href": "slides/lab-conf-band.html#exercise-3",
    "title": "MCMC & confidence bands",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nPlot the posterior mean \\(\\theta(x)\\) vs \\(x\\) where \\(\\theta(x) \\equiv \\beta_0 + \\beta_1 x\\).\nAdditionally, plot a 95% confidence band for \\(\\theta(x)\\) vs \\(x\\) and interpret your plot."
  },
  {
    "objectID": "slides/lab-conf-band.html#solution-1-2",
    "href": "slides/lab-conf-band.html#solution-1-2",
    "title": "MCMC & confidence bands",
    "section": "Solution 1, 2",
    "text": "Solution 1, 2\n\nAlgo.Trace plots (img)Trace plots (code)ESSLonger runPost medians\n\n\nA Metropolis-Hastings algorithm. inverse-gamma proposal is asymmetric.\nShowing with R code:\n\nsigma2_proposal = 1.5\nsigma2_s = 1\ndinvgamma(sigma2_proposal, 1, sigma2_s) == \n  dinvgamma(sigma2_s, 1, sigma2_proposal)\n\n[1] FALSE\n\n\n\n\n\n\n\n\n\n\n\\(\\beta_1\\) converges to the target after about 50 iterations.\nSome samples from inv-gamma are huge, the huge variance results in a flatter likelihood and still get accepted.\n\n\n\n\nlibrary(patchwork)\nparameterDF = data.frame(BETA0, BETA1, SIGMA2)\nn = nrow(parameterDF)\n\np0 = parameterDF %>%\n  ggplot(aes(x = 1:n)) +\n  geom_line(aes(y = BETA0)) +\n  theme_bw() +\n  labs(x = \"iteration\", y = \"beta0\")\n\np1 = parameterDF %>%\n  ggplot(aes(x = 1:n)) +\n  geom_line(aes(y = BETA1)) +\n  theme_bw() +\n  labs(x = \"iteration\", y = \"beta1\")\n\np2 = parameterDF %>%\n  ggplot(aes(x = 1:n)) +\n  geom_line(aes(y = SIGMA2)) +\n  theme_bw() +\n  labs(x = \"iteration\", y = \"sigma2\")\n\np0 + p1 + p2\n\n\n\n\nlibrary(coda) \n\nparameterDF = data.frame(BETA0, BETA1, SIGMA2)\napply(parameterDF, 2, effectiveSize)\n\n   BETA0    BETA1   SIGMA2 \n20.70251 12.72726 40.55270 \n\n\n\n\n\nset.seed(360)\nlogLikelihood = function(beta0, beta1, sigma2) {\n  mu = beta0 + (beta1 * x)\n  sum(dnorm(y, mu, sqrt(sigma2), log = TRUE))\n}\n\nlogPrior = function(beta0, beta1, sigma2) {\n  dnorm(beta0, 0, sqrt(2), log = TRUE) + \n    dnorm(beta1, 0, sqrt(2), log = TRUE) +\n    dinvgamma(sigma2, 2, 3, log = TRUE)\n}\n\nlogPosterior = function(beta0, beta1, sigma2) {\n  logLikelihood(beta0, beta1, sigma2) + logPrior(beta0, beta1, sigma2)\n}\n\n\nBETA0 = NULL\nBETA1 = NULL\nSIGMA2 = NULL\n\naccept1 = 0\naccept2 = 0\naccept3 = 0\n\nS = 10000\n\nbeta0_s = 0.1\nbeta1_s = 10\nsigma2_s = 1\nfor (s in 1:S) {\n  \n  ## propose and update beta0\n  beta0_proposal = rnorm(1, mean = beta0_s, .5)\n   log.r = logPosterior(beta0_proposal, beta1_s, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta0_s = beta0_proposal\n    accept1 = accept1 + 1 \n   }\n   \n   BETA0 = c(BETA0, beta0_s)\n   \n   ## propose and update beta1\n    beta1_proposal = rnorm(1, mean = beta1_s, .5)\n   log.r = logPosterior(beta0_s, beta1_proposal, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta1_s = beta1_proposal\n    accept2 = accept2 + 1 \n   }\n   \n   BETA1 = c(BETA1, beta1_s)\n   \n   ## propose and update sigma2\n   ### note: sigma2 is positive only, we want to only propose positive values\n   sigma2_proposal = 1 / rgamma(1, shape = 1, sigma2_s)\n   log.r = logPosterior(beta0_s, beta1_s, sigma2_proposal) - \n     logPosterior(beta0_s, beta1_s, sigma2_s) + \n     dinvgamma(sigma2_proposal, 1, sigma2_s, log = TRUE) - \n     dinvgamma(sigma2_s, 1, sigma2_proposal, log = TRUE) \n   \n   if(log(runif(1)) < log.r)  {\n    sigma2_s = sigma2_proposal\n    accept3 = accept3 + 1 \n   }\n   \n   SIGMA2 = c(SIGMA2, sigma2_s)\n   \n}\n\nparameterDF = data.frame(BETA0, BETA1, SIGMA2)\napply(parameterDF, 2, effectiveSize)\n\n   BETA0    BETA1   SIGMA2 \n405.7465 241.8751 826.7580 \n\n\n\n\n\n\n\n\n\n\npost. median\nCI\n\n\n\n\nbeta0\n0.5\n(-1.1, 1.9)\n\n\nbeta1\n2\n(-0.1, 2.5)\n\n\nsigma2\n13.4\n(7.3, 2537295.4)"
  },
  {
    "objectID": "slides/lab-conf-band.html#solution-3",
    "href": "slides/lab-conf-band.html#solution-3",
    "title": "MCMC & confidence bands",
    "section": "Solution 3",
    "text": "Solution 3\n\nplotcode\n\n\n\n\n\n\n\nThe red line shows our posterior expectation of \\(\\theta(x)\\) for each \\(x\\). The black bands show our 95% confidence interval \\(\\theta(x)\\).\n\n\n\nget_theta_CI = function(X) {\n     f = BETA0 + (BETA1 * X)\n     return(quantile(f, c(0.025, 0.975)))\n}\n\nget_theta_mean = function(X) {\n  f = BETA0 + (BETA1 * X)\n  return(mean(f))\n}\n\nxlo = min(x)\nxhi = max(x)\nxVal = seq(xlo, xhi, by = 0.01)\nlower = NULL\nupper = NULL\nM = NULL\n   \nfor (i in seq_along(xVal)) {\n  theta_CI = get_theta_CI(xVal[i])\n  lower = c(lower, theta_CI[[1]])\n  upper = c(upper, theta_CI[[2]])\n  M = c(M, get_theta_mean(xVal[i]))\n}\n\ndf = data.frame(xVal, lower, upper, M)\ndf %>%\n  ggplot(aes(x = xVal)) +\n  geom_line(aes(y = lower)) +\n  geom_line(aes(y = upper)) +\n  geom_line(aes(y = M), col = \"red\") +\n  theme_bw() +\n  labs(y = \"theta\",\n       x = \"x\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "notes/lec10-gibbs.html",
    "href": "notes/lec10-gibbs.html",
    "title": "Gibbs sampling",
    "section": "",
    "text": "Definition\n\n\n\nA semiconjugate or conditionally conjugate prior is a prior that is conjugate to the full conditional posterior.\n\n\nNote: the idea of a semiconjugate prior only makes sense when making inferences about two or more parameters.\nExample:\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\n\\theta & \\sim N(\\mu_0, \\tau_0^2)\\\\\n\\frac{1}{\\sigma^2} &\\sim gamma(\\nu_0/2, \\nu_0\\sigma_0^2/2)\n\\end{aligned}\n\\]\nIn this case, \\(\\tau_0^2\\) is not a function of \\(\\sigma^2\\) and thus \\(p(\\theta, \\sigma^2) = p(\\theta) p(\\sigma^2)\\).\nEach prior is ‚Äúsemiconjugate‚Äù since \\(p(\\theta| \\sigma^2, y_1,\\ldots y_n)\\) is normal and \\(p(\\frac{1}{\\sigma^2} | \\theta, y_1,\\ldots y_n)\\) is gamma but \\(p(\\theta, \\sigma^2)\\) is not conjugate to \\(p(\\theta, \\sigma^2 | y_1,\\ldots y_n)\\).\n\n\n\n\n\n\nDefinition\n\n\n\nA proper prior is a density function that does not depend on data and integrates to 1. If a prior integrates to a positive finite value, it is an unnormalized density that can be renormalized by being multiplied by a constant to integrate to 1. If a prior is not proper, we call the prior improper.\n\n\nExample:\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\np(\\theta, \\sigma^2) &= \\frac{1}{\\sigma^2}\n\\end{aligned}\n\\]\n\\(p(\\theta, \\sigma^2)\\) is an improper prior. \\(p(\\theta, \\sigma^2)\\) does not integrate to a finite value and thus cannot be renormalized. It is not a probability density. However, it yields a tractable posterior for \\(\\theta\\) and \\(\\sigma^2\\) (see p 79 of Hoff).\n\n\n\nPriors are meant to describe our state of knowledge before examining data. In some cases we may wish to describe our ignorance a priori using a vague prior that plays a minimal role in the posterior distribution.\nA common trap is to imagine that a flat, or uniform prior is uninformative. Previously, on homework 3 you showed a uniform prior on binary probability of success \\(\\theta\\) is informative on the log-odds. Additionally, an improper flat prior may carry a lot of information, since most of the mass is infinitely far away.\n\n\n\n\n\n\nDefinition\n\n\n\nThe Jeffreys prior\n\\[\nJ(\\theta) \\propto \\sqrt{I(\\theta)}\n\\]\nwhere \\(I(\\theta) = -E[\\frac{\\partial}{\\partial\\theta^2} \\log p(Y|\\theta) | \\theta]\\).\n\n\nThe defining feature of Jeffreys prior is that it will yield an equivalent result if applied to a transformed parameter. This principle of invariance is one approach to non-informative priors that works well for single parameter priors. Multiparameter extensions are often less useful."
  },
  {
    "objectID": "notes/lec10-gibbs.html#gibbs-sampler",
    "href": "notes/lec10-gibbs.html#gibbs-sampler",
    "title": "Gibbs sampling",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nWhat if we have a non-conjugate prior? How can we can we look at \\(p(\\theta, \\sigma^2 | y_1,\\ldots y_n)\\)?\nIn general, suppose we don‚Äôt know\n\\[\np(\\theta, \\sigma^2 | y_1,\\ldots y_n)\n\\]\nbut we do know the full conditional posteriors\n\\[\n\\begin{aligned}\np(\\theta | \\sigma^2, y_1, \\ldots y_n)\\\\\np(\\sigma^2 | \\theta, y_1,\\ldots y_n)\n\\end{aligned}\n\\]\nwe can generate sample \\(\\theta^{(s)}, \\sigma^{2(s)}\\) from the joint posterior by the following algorithm:\n\nsample \\(\\theta^{(s+1)}\\) from \\(p(\\theta | \\sigma^{2(s)}, y_1,\\ldots y_n)\\)\nsample \\(\\sigma^{2(s+1)}\\) from \\(p(\\sigma^2|\\theta^{(s+1)}, y_1,\\ldots, y_n)\\)\nlet \\(\\phi^{(s+1)} = \\{ \\theta^{(s+1)}, \\sigma^{2(s+1)} \\}\\)\n\niterate steps 1-3 \\(S\\) times.\nThis algorithm is called the Gibbs sampler,\n\nit creates a dependent set of values \\(\\phi^{(1)} \\ldots \\phi^{(S)}\\),\nthe sequence is called a Markov chain,\nthe samples let us approximate the posterior i.e.¬†the histogram of \\((\\phi^{(1)},\\ldots \\phi^{(S)})\\) is a Markov chain Monte Carlo approximation to \\(p(\\phi | y_1,\\ldots y_n)\\).\n\nExample: in the semiconjugate normal model described above, the resulting posteriors are:\n\\[\n\\theta | \\sigma^2, y_1,\\ldots y_n \\sim N(\\mu_n, \\tau_n^2),\n\\]\nwhere \\(\\mu_n = \\frac{\\mu_0/\\tau_0^2 + n\\bar{y} /\\sigma^2}{1/{\\tau_0^2} + n/\\sigma^2}\\) and \\(\\tau_n^2 = \\left( \\frac{1}{\\tau_0^2 }+ \\frac{n}{\\sigma^2} \\right)^{-1}\\) and\n\\[\n\\sigma^2 | \\theta, y_1, \\ldots y_n \\sim invgamma(\\nu_n/2, \\nu_n \\sigma^2_n / 2)\n\\]\nwhere \\(\\nu_n = \\nu_0 + n\\), \\(\\sigma_n^2 = \\frac{1}{\\nu_n} [\\nu_0 \\sigma_0^2 + n s^2_n(\\theta)]\\) and \\(s^2_n(\\theta) = \\frac{1}{n}\\sum (y_i - \\theta)^2\\).\n\n##########################\n# example from Hoff ch6 #\n##########################\n\n# data\ny = c(1.64, 1.70, 1.72, 1.74, 1.82, 1.82, 1.82, 1.90, 2.08)\nmean.y = mean(y) ; var.y = var(y) ; n = length(y)\n\n# priors\nmu0 = 0\nt20 = 100\nnu0 = 1\ns20 = 2\n\n# starting point\nS = 1000\nPHI = matrix(nrow = S, ncol = 2)\nphi = c(mean.y, var(y))\nPHI[1, ] = phi\n\n# Gibbs sampling\nset.seed(360)\nfor(s in 2:S) { \n\n## generate theta from sigma2\nmun = (mu0 / t20 + n * mean.y * phi[2]) / (1 / t20 + n * phi[2])\nt2n = 1 / (1 / t20 + n * phi[2])\nphi[1] = rnorm(1, mun, sqrt(t2n))\n\n## generate 1/sigma2 from theta\nnun = nu0 + n\ns2n = (nu0 * s20 + (n - 1) * var.y + n * (mean.y - phi[1])^2 ) / nun\nphi[2] = rgamma(1, nun/2, nun * s2n / 2)\n\n## update chain\nPHI[s,] = phi\n}\n\nNote: in this code we use the identity \\(n s_n^2(\\theta) = (n-1)s^2 + n (\\bar{y} - \\theta)^2\\).\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(latex2exp)\n# plotting the joint posterior\ndf = as.data.frame(PHI)\nnames(df) = c(\"theta\", \"prec\")\ndf %>%\n  ggplot(aes(x = theta, y = prec)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  labs(x = TeX(\"$\\\\theta$\"),\n       y = TeX(\"$1/\\\\sigma^2$\"),\n       fill = TeX(\"$p(\\\\theta, 1/\\\\sigma^2 | y_1, \\\\ldots y_n)$\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nSince the sequence \\(\\{\\phi^{(s)} \\}\\) depends on \\(\\phi^{(0)}, \\ldots \\phi^{(s-1)}\\) only through \\(\\phi^{(s-1)}\\) we say the sequence is memoryless. This is called the Markov property, and so the sequence is a Markov chain.\n‚ÄúWhat happens next depends only on the state of affairs now‚Äù\n\n\nUnder some conditions,\n\\[\np(\\phi^{(s)} \\in A) \\rightarrow \\int_A p(\\phi) d\\phi \\ \\ \\text{ as } s \\rightarrow \\infty\n\\]\ni.e.¬†the sampling distribution of \\(\\phi^{(s)}\\) approaches the target distribution as \\(s \\rightarrow \\infty\\) regardless of \\(\\phi^{(0)}\\).\nFurthermore,\n\\[\n\\frac{1}{S} \\sum_{s=1}^S g(\\phi^{(s)})  \\rightarrow E[g(\\phi)]\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nBig take-away: if we can sample from the full conditional posteriors, we can construct a Markov chain with samples from the joint posterior! We can then use Monte Carlo approximation to use the samples to summarize aspects of the posterior."
  },
  {
    "objectID": "notes/lec10-mvn-gibbs.html",
    "href": "notes/lec10-mvn-gibbs.html",
    "title": "Gibbs sampling the MVN",
    "section": "",
    "text": "Gibbs sampler\nGibbs sampling is a special case of Metropolis-Hastings that proceeds as follows:\n\nsample \\(\\theta_1^{(s+1)}\\) from \\(p(\\theta_1 | \\theta_2^{(s)}, \\mathbf{y})\\)\nsample \\(\\theta_2^{(s+1)}\\) from \\(p(\\theta_2|\\theta_1^{(s+1)}, \\mathbf{y})\\)\n\niterate steps 1-2 a total of \\(S\\) times.\n\nExercise\n\n\nProve that the algorithm described above is in fact a Metropolis-Hastings algorithm where \\(J(\\theta_i | \\theta_i^{(s)}) = p(\\theta_i | \\theta_{j \\neq i}, \\mathbf{y})\\) with acceptance probability 1.\nHint: recall that the Metropolis-Hastings acceptance ratio \\(r\\) is computed as follows:\n\\[\nr = \\frac{\\pi(\\theta^*)}{\\pi(\\theta^{(s)})} \\times\n\\frac{J(\\theta^{(s)}| \\theta^*)}{\nJ(\\theta^{*}| \\theta^{(s)})\n}\n\\]\n\n\n\n\n\nExample\nImagine the following target distribution (the joint probability distribution of two variables, \\(\\theta\\) and \\(\\delta\\)).\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(latex2exp)\nset.seed(360)\n\n## fixed values ## \nmu = c(-3, 0, 3) # conditional means\nsd = rep(sqrt(1 / 3), 3) # conditional sds\nd = c(1, 2, 3) # sample space of delta\nN = 1000 # number of samples\n\ndelta = sample(d, size = N, prob = c(.45, .1, .4), replace = TRUE)\ntheta = rnorm(N, mean = mu[delta], sd = sd[delta])\n\ndf = data.frame(delta, theta)\ndf %>%\n  ggplot(aes(x = theta, y = delta)) + \n  geom_bin2d(bins = 25) +\n  theme_bw() + \n  labs(y = TeX(\"\\\\delta\"), \n       x = TeX(\"\\\\theta\"))\n\n\n\n\nIn this example,\n\\[\n\\begin{aligned}\np(\\delta = d) = \\begin{cases}\n&.45 &\\text{ if } d = 1\\\\\n&.10 &\\text{ if } d = 2\\\\\n&.45 &\\text{ if } d = 3\n\\end{cases}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\{\\theta | \\delta = d\\} \\sim\n\\begin{cases}\n&N(-3, 1/3) &\\text{ if } d = 1\\\\\n&N(0, 1/3) &\\text{ if } d = 2\\\\\n&N(3, 1/3) &\\text{ if } d = 3\n\\end{cases}\n\\end{aligned}\n\\]\n\nExerciseSolution\n\n\nWrite down the full conditional distributions necessary to Gibbs sample the target.\nNote: this is a toy example. We can sample from the target distribution directly as seen above. However, we will construct a Gibbs sampler for pedagogical purposes that will become apparent momentarily.\n\n\n\nTo construct a Gibbs sampler, we need the full conditional distributions.\n\n\\(p(\\theta | \\delta)\\) is given.\n\\(p(\\delta| \\theta) = \\frac{p(\\theta | \\delta = d) p(\\delta = d)}{ \\sum_{d=1}^3p(\\theta | \\delta = d)p(\\delta = d)}\\), for \\(d \\in \\{1, 2, 3\\}\\). \n\n\n\n\nBelow the sampler is implemented and a trace plot for \\(\\theta\\) reported.\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n## fixed values ## \nmu = c(-3, 0, 3) # conditional means\ns2 = rep(1 / 3, 3) # conditional sds\nd = c(1, 2, 3) # sample space of delta\nN = 1000 # chain length\nw = c(.45, .1, .4) # delta probabilities\n\n## Gibbs sampler ##\nset.seed(360)\nN = 1000 # number of Gibbs samples\n\ntheta = 0 # initial theta value\nthd.mcmc = NULL\nfor(i in 1:N) {\nd = sample(1:3 , 1, prob = w * dnorm(theta, mu, sqrt(s2))) \ntheta = rnorm(1, mu[d], sqrt(s2[d]))\nthd.mcmc = rbind(thd.mcmc, c(theta,d))\n}\n# note we take advantage that sample() in R does not require the probability\n# to add up to 1\n\ndf = data.frame(theta = thd.mcmc[,1],\n                delta = thd.mcmc[,2])\n\ndf %>%\n  ggplot(aes(x = seq(1, nrow(df)), y = theta)) +\n  geom_line() +\n  theme_bw() +\n  labs(y = TeX(\"\\\\theta\"),\n       x = \"iteration\",\n       title = \"Traceplot of 1000 Gibbs samples\")\n\n\n\n\n\nExercise\n\n\n\ndescribe how we implement the conditional update for delta in the code above\nwhat do you notice from the traceplot above? Hint: you can imagine hopping from delta islands in the first figure of the joint target over parameter space.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe picture to visualize is that of a particle moving through parameter space.\n\n\n\n\n\nLet‚Äôs see how well our samples of \\(\\theta\\) approximate the true marginal \\(p(\\theta)\\)."
  },
  {
    "objectID": "notes/lec10-mvn-gibbs.html#matrix-algebra-fundamentals",
    "href": "notes/lec10-mvn-gibbs.html#matrix-algebra-fundamentals",
    "title": "Gibbs sampling the MVN",
    "section": "Matrix algebra fundamentals",
    "text": "Matrix algebra fundamentals\n\nmatrix facts\n\nmatrix multiplication proceeds row \\(\\times\\) column, so if we have the product \\(AB\\), \\(A\\) must have the same number of ___ as B has ___.\nthe determinant of a matrix, \\(|A|\\), measures the size of the matrix\nthe identity matrix is the matrix multiplicative identity. It is represented by \\(\\boldsymbol{I}\\), in general \\(\\boldsymbol{I}_p\\) is a \\(p \\times p\\) matrix with 1 on each diagonal and 0 on every off-diagonal. \\(\\boldsymbol{I}A = A \\boldsymbol{I}= A\\).\nthe inverse of a matrix \\(A^{-1}\\) works as follows: \\(A A^{-1} = A^{-1}A = \\boldsymbol{I}\\).\nthe trace of a matrix, tr(A), is the sum of its diagonal elements\norder matters: \\(AB \\neq BA\\) in general.\n\\(\\Sigma > 0\\) is shorthand for saying the matrix is positive definite. This means that for all vectors \\(\\boldsymbol{x}\\), the quadratic form \\(\\boldsymbol{x}^T \\Sigma \\boldsymbol{x} > 0\\). \\(Sigma > 0 \\iff\\) all eigenvalues of \\(\\Sigma\\) are positive.\n\nExercise:\n\n\\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{b}\\) are \\(p \\times 1\\) vectors, \\(A\\) is a symmetric matrix. Simplify \\(\\boldsymbol{b}^T A \\boldsymbol{\\theta}+ \\boldsymbol{\\theta}^T A \\boldsymbol{b}\\) what is the dimension of the result?\nwhat‚Äôs the dimension of \\(V[\\boldsymbol{y}]\\)?\n\n\n\nmatrix operations in R\n\n# make a matrix A\nA = matrix(c(1,.2, .2, 2), ncol = 2)\nA\n\n     [,1] [,2]\n[1,]  1.0  0.2\n[2,]  0.2  2.0\n\n# invert A (expensive for large matrices)\nAinv = solve(A)\n\n# matrix multiplication\nAinv %*% A\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n# determinant of A\ndet(A)\n\n[1] 1.96\n\n# trace of A\nsum(diag(A))\n\n[1] 3\n\n# create a vector b\nb = matrix(c(1, 2), ncol = 1)\nb\n\n     [,1]\n[1,]    1\n[2,]    2\n\n# transpose the vector b\nt(b)\n\n     [,1] [,2]\n[1,]    1    2\n\n\n\nb %*% A\n\nError in b %*% A: non-conformable arguments\n\n\n\nWhat went wrong in the code above?"
  },
  {
    "objectID": "notes/lec10-mvn-gibbs.html#semiconjugate-priors",
    "href": "notes/lec10-mvn-gibbs.html#semiconjugate-priors",
    "title": "Gibbs sampling the MVN",
    "section": "Semiconjugate priors",
    "text": "Semiconjugate priors\n\nsemiconjugate prior for \\(\\boldsymbol{\\theta}\\)\nIf\n\\[\n\\begin{aligned}\n\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma &\\sim MVN(\\boldsymbol{\\theta}, \\Sigma),\\\\\n\\boldsymbol{\\theta}&\\sim MVN(\\mu_0, \\Lambda_0),\n\\end{aligned}\n\\]\nthen\n\\[\n\\boldsymbol{\\theta}| \\boldsymbol{y}, \\Sigma \\sim MVN(\\boldsymbol{\\mu_n}, \\Lambda_n),\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\Lambda_n &= (\\Lambda_0^{-1} + n \\Sigma^{-1} )^{-1},\\\\\n\\boldsymbol{\\mu_n} &= (\\Lambda_0^{-1} + n \\Sigma^{-1} )^{-1}(\\Lambda_0^{-1} \\boldsymbol{\\mu}_0 + n \\Sigma^{-1} \\bar{\\boldsymbol{y}}).\n\\end{aligned}\n\\]\nExercise: interpret \\(E[\\boldsymbol{\\theta}| \\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n, \\Sigma]\\) and \\(Cov[\\boldsymbol{\\theta}| \\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n, \\Sigma]\\).\n\n\nsemiconjugate prior for \\(\\Sigma\\)\nIf\n\\[\n\\begin{aligned}\n\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma &\\sim MVN(\\boldsymbol{\\theta}, \\Sigma),\\\\\n\\Sigma &\\sim \\text{inverse-Wishart}(\\nu_0, S_0^{-1}),\n\\end{aligned}\n\\]\nthen\n\\[\n\\Sigma | \\boldsymbol{y}, \\boldsymbol{\\theta}\\sim \\text{inverse-Wishart} (\\nu_0 + n, (S_0 + S_{\\theta})^{-1}),\n\\]\nwhere \\(S_\\theta = \\sum_{i=1}^n (\\boldsymbol{y}_i - \\boldsymbol{\\theta})(\\boldsymbol{y}_i - \\boldsymbol{\\theta})^T\\) is the residual sum of squares matrix for the vectors \\(\\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n\\) if the population mean is \\(\\boldsymbol{\\theta}\\).\n\n\n\nthe inverse-Wishart\nthe inverse-Wishart\\((\\nu_0, S_0^{-1})\\) density is given by\n\\[\n\\begin{aligned}\np(\\Sigma | \\nu_0, S_0^{-1}) = \\left[\n2^{\\nu_0 p / 2} \\pi^{{p \\choose 2}/2} |S_0|^{-\\nu_0/2} \\prod_{j = 1}^p \\Gamma([\\nu_0 + 1 - j]/2)\n\\right]^{-1} \\times\\\\\n|\\Sigma|^{-(\\nu_0 + p + 1)/2} \\times \\exp \\{ -\\frac{1}{2}tr(S_0 \\Sigma^{-1})\\}.\n\\end{aligned}\n\\]\n\nKey facts\n\nnotice that the first line is the normalizing constant of the density\nthe support is \\(\\Sigma > 0\\) and \\(\\Sigma\\) symmetric \\(p \\times p\\) matrix. \\(\\nu_0 \\in \\mathbb{N}^+\\) and \\(\\nu_0 \\geq p\\). \\(S_0\\) is a \\(p \\times p\\) symmetric positive definite matrix.\nif \\(\\Sigma\\) is inv-Wishart\\((\\nu_0, S_0^{-1})\\) then \\(\\Sigma^{-1}\\) is Wishart\\((\\nu_0, S_0^{-1})\\).\n\\(E[\\Sigma^{-1}] = \\nu_0 S_0^{-1}\\) and \\(E[\\Sigma] = \\frac{1}{\\nu_0 - p - 1} S_0\\).\nintuition: \\(\\nu_0\\) is prior sample size. \\(S_0\\) is a prior guess of the covariance matrix.\n\n\n\nsampling from the inverse-Wishart\n\npick \\(\\nu_0 > p\\), pick \\(S_0\\)\nsample \\(\\boldsymbol{z}_1, \\ldots \\boldsymbol{z}_{\\nu_0} \\sim \\text{ i.i.d. } MVN(\\boldsymbol{0}, S_0^{-1})\\)\ncalculate \\(\\boldsymbol{Z}^T \\boldsymbol{Z} = \\sum_{i = 1}^{\\nu_0} \\boldsymbol{z}_i \\boldsymbol{z}^T\\)\nset \\(\\Sigma = (\\boldsymbol{Z}^T \\boldsymbol{Z})^{-1}\\)\n\n\nlibrary(mvtnorm) # contains function rmvnorm\n\n# 2x2 example: generating 1 sample from an inv-Wishart\nset.seed(360)\np = 2\nnu0 = 3\nS0 = matrix(c(1, .1, .1, 1), ncol = 2)\nS0inv = solve(S0)\nZ = rmvnorm(n = nu0, # number of observations of the 2D vector Z\n        mean = rep(0, p), # mean 0\n        sigma = S0inv) # prior variance\nSigma = solve(t(Z) %*% Z)\neigen(Sigma)$values\n\n[1] 0.7821737 0.4174527\n\nSigma\n\n           [,1]      [,2]\n[1,]  0.5271834 -0.167273\n[2,] -0.1672730  0.672443\n\n\nExercise/show offline: why does this work? Hint: what is \\(cov[\\boldsymbol{z}]\\)?\nWe can also use the monomvn package to simulate from a Wishart more succinctly,\n\nlibrary(monomvn)\n\nset.seed(360)\nSigma = solve(rwish(nu0, S0inv))\neigen(Sigma)$values\n\n[1] 0.692529 0.160428\n\nSigma\n\n          [,1]      [,2]\n[1,] 0.1899212 0.1217519\n[2,] 0.1217519 0.6630358"
  },
  {
    "objectID": "notes/old-gibbs.html",
    "href": "notes/old-gibbs.html",
    "title": "Gibbs sampling",
    "section": "",
    "text": "Definition\n\n\n\nA semiconjugate or conditionally conjugate prior is a prior that is conjugate to the full conditional posterior.\n\n\nNote: the idea of a semiconjugate prior only makes sense when making inferences about two or more parameters.\nExample:\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\n\\theta & \\sim N(\\mu_0, \\tau_0^2)\\\\\n\\frac{1}{\\sigma^2} &\\sim gamma(\\nu_0/2, \\nu_0\\sigma_0^2/2)\n\\end{aligned}\n\\]\nIn this case, \\(\\tau_0^2\\) is not a function of \\(\\sigma^2\\) and thus \\(p(\\theta, \\sigma^2) = p(\\theta) p(\\sigma^2)\\).\nEach prior is ‚Äúsemiconjugate‚Äù since \\(p(\\theta| \\sigma^2, y_1,\\ldots y_n)\\) is normal and \\(p(\\frac{1}{\\sigma^2} | \\theta, y_1,\\ldots y_n)\\) is gamma but \\(p(\\theta, \\sigma^2)\\) is not conjugate to \\(p(\\theta, \\sigma^2 | y_1,\\ldots y_n)\\).\n\n\n\n\n\n\nDefinition\n\n\n\nA proper prior is a density function that does not depend on data and integrates to 1. If a prior integrates to a positive finite value, it is an unnormalized density that can be renormalized by being multiplied by a constant to integrate to 1. If a prior is not proper, we call the prior improper.\n\n\nExample:\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\np(\\theta, \\sigma^2) &= \\frac{1}{\\sigma^2}\n\\end{aligned}\n\\]\n\\(p(\\theta, \\sigma^2)\\) is an improper prior. \\(p(\\theta, \\sigma^2)\\) does not integrate to a finite value and thus cannot be renormalized. It is not a probability density. However, it yields a tractable posterior for \\(\\theta\\) and \\(\\sigma^2\\) (see p 79 of Hoff).\n\n\n\nPriors are meant to describe our state of knowledge before examining data. In some cases we may wish to describe our ignorance a priori using a vague prior that plays a minimal role in the posterior distribution.\nA common trap is to imagine that a flat, or uniform prior is uninformative. Previously, on homework 3 you showed a uniform prior on binary probability of success \\(\\theta\\) is informative on the log-odds. Additionally, an improper flat prior may carry a lot of information, since most of the mass is infinitely far away.\n\n\n\n\n\n\nDefinition\n\n\n\nThe Jeffreys prior\n\\[\nJ(\\theta) \\propto \\sqrt{I(\\theta)}\n\\]\nwhere \\(I(\\theta) = -E[\\frac{\\partial}{\\partial\\theta^2} \\log p(Y|\\theta) | \\theta]\\).\n\n\nThe defining feature of Jeffreys prior is that it will yield an equivalent result if applied to a transformed parameter. This principle of invariance is one approach to non-informative priors that works well for single parameter priors. Multiparameter extensions are often less useful."
  },
  {
    "objectID": "notes/old-gibbs.html#gibbs-sampler",
    "href": "notes/old-gibbs.html#gibbs-sampler",
    "title": "Gibbs sampling",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nWhat if we have a non-conjugate prior? How can we can we look at \\(p(\\theta, \\sigma^2 | y_1,\\ldots y_n)\\)?\nIn general, suppose we don‚Äôt know\n\\[\np(\\theta, \\sigma^2 | y_1,\\ldots y_n)\n\\]\nbut we do know the full conditional posteriors\n\\[\n\\begin{aligned}\np(\\theta | \\sigma^2, y_1, \\ldots y_n)\\\\\np(\\sigma^2 | \\theta, y_1,\\ldots y_n)\n\\end{aligned}\n\\]\nwe can generate sample \\(\\theta^{(s)}, \\sigma^{2(s)}\\) from the joint posterior by the following algorithm:\n\nsample \\(\\theta^{(s+1)}\\) from \\(p(\\theta | \\sigma^{2(s)}, y_1,\\ldots y_n)\\)\nsample \\(\\sigma^{2(s+1)}\\) from \\(p(\\sigma^2|\\theta^{(s+1)}, y_1,\\ldots, y_n)\\)\nlet \\(\\phi^{(s+1)} = \\{ \\theta^{(s+1)}, \\sigma^{2(s+1)} \\}\\)\n\niterate steps 1-3 \\(S\\) times.\nThis algorithm is called the Gibbs sampler,\n\nit creates a dependent set of values \\(\\phi^{(1)} \\ldots \\phi^{(S)}\\),\nthe sequence is called a Markov chain,\nthe samples let us approximate the posterior i.e.¬†the histogram of \\((\\phi^{(1)},\\ldots \\phi^{(S)})\\) is a Markov chain Monte Carlo approximation to \\(p(\\phi | y_1,\\ldots y_n)\\).\n\nExample: in the semiconjugate normal model described above, the resulting posteriors are:\n\\[\n\\theta | \\sigma^2, y_1,\\ldots y_n \\sim N(\\mu_n, \\tau_n^2),\n\\]\nwhere \\(\\mu_n = \\frac{\\mu_0/\\tau_0^2 + n\\bar{y} /\\sigma^2}{1/{\\tau_0^2} + n/\\sigma^2}\\) and \\(\\tau_n^2 = \\left( \\frac{1}{\\tau_0^2 }+ \\frac{n}{\\sigma^2} \\right)^{-1}\\) and\n\\[\n\\sigma^2 | \\theta, y_1, \\ldots y_n \\sim invgamma(\\nu_n/2, \\nu_n \\sigma^2_n / 2)\n\\]\nwhere \\(\\nu_n = \\nu_0 + n\\), \\(\\sigma_n^2 = \\frac{1}{\\nu_n} [\\nu_0 \\sigma_0^2 + n s^2_n(\\theta)]\\) and \\(s^2_n(\\theta) = \\frac{1}{n}\\sum (y_i - \\theta)^2\\).\n\n##########################\n# example from Hoff ch6 #\n##########################\n\n# data\ny = c(1.64, 1.70, 1.72, 1.74, 1.82, 1.82, 1.82, 1.90, 2.08)\nmean.y = mean(y) ; var.y = var(y) ; n = length(y)\n\n# priors\nmu0 = 0\nt20 = 100\nnu0 = 1\ns20 = 2\n\n# starting point\nS = 1000\nPHI = matrix(nrow = S, ncol = 2)\nphi = c(mean.y, var(y))\nPHI[1, ] = phi\n\n# Gibbs sampling\nset.seed(360)\nfor(s in 2:S) { \n\n## generate theta from sigma2\nmun = (mu0 / t20 + n * mean.y * phi[2]) / (1 / t20 + n * phi[2])\nt2n = 1 / (1 / t20 + n * phi[2])\nphi[1] = rnorm(1, mun, sqrt(t2n))\n\n## generate 1/sigma2 from theta\nnun = nu0 + n\ns2n = (nu0 * s20 + (n - 1) * var.y + n * (mean.y - phi[1])^2 ) / nun\nphi[2] = rgamma(1, nun/2, nun * s2n / 2)\n\n## update chain\nPHI[s,] = phi\n}\n\nNote: in this code we use the identity \\(n s_n^2(\\theta) = (n-1)s^2 + n (\\bar{y} - \\theta)^2\\).\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(latex2exp)\n# plotting the joint posterior\ndf = as.data.frame(PHI)\nnames(df) = c(\"theta\", \"prec\")\ndf %>%\n  ggplot(aes(x = theta, y = prec)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  labs(x = TeX(\"$\\\\theta$\"),\n       y = TeX(\"$1/\\\\sigma^2$\"),\n       fill = TeX(\"$p(\\\\theta, 1/\\\\sigma^2 | y_1, \\\\ldots y_n)$\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nSince the sequence \\(\\{\\phi^{(s)} \\}\\) depends on \\(\\phi^{(0)}, \\ldots \\phi^{(s-1)}\\) only through \\(\\phi^{(s-1)}\\) we say the sequence is memoryless. This is called the Markov property, and so the sequence is a Markov chain.\n‚ÄúWhat happens next depends only on the state of affairs now‚Äù\n\n\nUnder some conditions,\n\\[\np(\\phi^{(s)} \\in A) \\rightarrow \\int_A p(\\phi) d\\phi \\ \\ \\text{ as } s \\rightarrow \\infty\n\\]\ni.e.¬†the sampling distribution of \\(\\phi^{(s)}\\) approaches the target distribution as \\(s \\rightarrow \\infty\\) regardless of \\(\\phi^{(0)}\\).\nFurthermore,\n\\[\n\\frac{1}{S} \\sum_{s=1}^S g(\\phi^{(s)})  \\rightarrow E[g(\\phi)]\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nBig take-away: if we can sample from the full conditional posteriors, we can construct a Markov chain with samples from the joint posterior! We can then use Monte Carlo approximation to use the samples to summarize aspects of the posterior."
  },
  {
    "objectID": "notes/lec10-mvn-gibbs.html#non-conjugate-priors",
    "href": "notes/lec10-mvn-gibbs.html#non-conjugate-priors",
    "title": "Gibbs sampling the MVN",
    "section": "Non-conjugate priors",
    "text": "Non-conjugate priors\n\nDefinitions\n\n\n\n\n\n\nDefinition\n\n\n\nA semiconjugate or conditionally conjugate prior is a prior that is conjugate to the full conditional posterior.\n\n\nNote: the idea of a semiconjugate prior only makes sense when making inferences about two or more parameters.\nExample:\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\n\\theta & \\sim N(\\mu_0, \\tau_0^2)\\\\\n\\frac{1}{\\sigma^2} &\\sim gamma(\\nu_0/2, \\nu_0\\sigma_0^2/2)\n\\end{aligned}\n\\]\nIn this case, \\(\\tau_0^2\\) is not a function of \\(\\sigma^2\\) and thus \\(p(\\theta, \\sigma^2) = p(\\theta) p(\\sigma^2)\\).\nEach prior is ‚Äúsemiconjugate‚Äù since \\(p(\\theta| \\sigma^2, y_1,\\ldots y_n)\\) is normal and \\(p(\\frac{1}{\\sigma^2} | \\theta, y_1,\\ldots y_n)\\) is gamma but \\(p(\\theta, \\sigma^2)\\) is not conjugate to \\(p(\\theta, \\sigma^2 | y_1,\\ldots y_n)\\).\n\n\n\n\n\n\nDefinition\n\n\n\nA proper prior is a density function that does not depend on data and integrates to 1. If a prior integrates to a positive finite value, it is an unnormalized density that can be renormalized by being multiplied by a constant to integrate to 1. If a prior is not proper, we call the prior improper.\n\n\nExample:\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\np(\\theta, \\sigma^2) &= \\frac{1}{\\sigma^2}\n\\end{aligned}\n\\]\n\\(p(\\theta, \\sigma^2)\\) is an improper prior. \\(p(\\theta, \\sigma^2)\\) does not integrate to a finite value and thus cannot be renormalized. It is not a probability density. However, it yields a tractable posterior for \\(\\theta\\) and \\(\\sigma^2\\) (see p 79 of Hoff).\n\n\nNoninformative priors\nPriors are meant to describe our state of knowledge before examining data. In some cases we may wish to describe our ignorance a priori using a vague prior that plays a minimal role in the posterior distribution.\nA common trap is to imagine that a flat, or uniform prior is uninformative. Previously, on homework 3 you showed a uniform prior on binary probability of success \\(\\theta\\) is informative on the log-odds. Additionally, an improper flat prior may carry a lot of information, since most of the mass is infinitely far away.\n\n\n\n\n\n\nDefinition\n\n\n\nThe Jeffreys prior\n\\[\nJ(\\theta) \\propto \\sqrt{I(\\theta)}\n\\]\nwhere \\(I(\\theta) = -E[\\frac{\\partial}{\\partial\\theta^2} \\log p(Y|\\theta) | \\theta]\\).\n\n\nThe defining feature of Jeffreys prior is that it will yield an equivalent result if applied to a transformed parameter. This principle of invariance is one approach to non-informative priors that works well for single parameter priors. Multiparameter extensions are often less useful."
  },
  {
    "objectID": "notes/lec10-mvn-gibbs.html#gibbs-sampler-1",
    "href": "notes/lec10-mvn-gibbs.html#gibbs-sampler-1",
    "title": "Gibbs sampling the MVN",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nWhat if we have a non-conjugate prior? How can we can we look at \\(p(\\theta, \\sigma^2 | y_1,\\ldots y_n)\\)?\nIn general, suppose we don‚Äôt know\n\\[\np(\\theta, \\sigma^2 | y_1,\\ldots y_n)\n\\]\nbut we do know the full conditional posteriors\n\\[\n\\begin{aligned}\np(\\theta | \\sigma^2, y_1, \\ldots y_n)\\\\\np(\\sigma^2 | \\theta, y_1,\\ldots y_n)\n\\end{aligned}\n\\]\nwe can generate sample \\(\\theta^{(s)}, \\sigma^{2(s)}\\) from the joint posterior by the following algorithm:\n\nsample \\(\\theta^{(s+1)}\\) from \\(p(\\theta | \\sigma^{2(s)}, y_1,\\ldots y_n)\\)\nsample \\(\\sigma^{2(s+1)}\\) from \\(p(\\sigma^2|\\theta^{(s+1)}, y_1,\\ldots, y_n)\\)\nlet \\(\\phi^{(s+1)} = \\{ \\theta^{(s+1)}, \\sigma^{2(s+1)} \\}\\)\n\niterate steps 1-3 \\(S\\) times.\nThis algorithm is called the Gibbs sampler,\n\nit creates a dependent set of values \\(\\phi^{(1)} \\ldots \\phi^{(S)}\\),\nthe sequence is called a Markov chain,\nthe samples let us approximate the posterior i.e.¬†the histogram of \\((\\phi^{(1)},\\ldots \\phi^{(S)})\\) is a Markov chain Monte Carlo approximation to \\(p(\\phi | y_1,\\ldots y_n)\\).\n\nExample: in the semiconjugate normal model described above, the resulting posteriors are:\n\\[\n\\theta | \\sigma^2, y_1,\\ldots y_n \\sim N(\\mu_n, \\tau_n^2),\n\\]\nwhere \\(\\mu_n = \\frac{\\mu_0/\\tau_0^2 + n\\bar{y} /\\sigma^2}{1/{\\tau_0^2} + n/\\sigma^2}\\) and \\(\\tau_n^2 = \\left( \\frac{1}{\\tau_0^2 }+ \\frac{n}{\\sigma^2} \\right)^{-1}\\) and\n\\[\n\\sigma^2 | \\theta, y_1, \\ldots y_n \\sim invgamma(\\nu_n/2, \\nu_n \\sigma^2_n / 2)\n\\]\nwhere \\(\\nu_n = \\nu_0 + n\\), \\(\\sigma_n^2 = \\frac{1}{\\nu_n} [\\nu_0 \\sigma_0^2 + n s^2_n(\\theta)]\\) and \\(s^2_n(\\theta) = \\frac{1}{n}\\sum (y_i - \\theta)^2\\).\n\n##########################\n# example from Hoff ch6 #\n##########################\n\n# data\ny = c(1.64, 1.70, 1.72, 1.74, 1.82, 1.82, 1.82, 1.90, 2.08)\nmean.y = mean(y) ; var.y = var(y) ; n = length(y)\n\n# priors\nmu0 = 0\nt20 = 100\nnu0 = 1\ns20 = 2\n\n# starting point\nS = 1000\nPHI = matrix(nrow = S, ncol = 2)\nphi = c(mean.y, var(y))\nPHI[1, ] = phi\n\n# Gibbs sampling\nset.seed(360)\nfor(s in 2:S) { \n\n## generate theta from sigma2\nmun = (mu0 / t20 + n * mean.y * phi[2]) / (1 / t20 + n * phi[2])\nt2n = 1 / (1 / t20 + n * phi[2])\nphi[1] = rnorm(1, mun, sqrt(t2n))\n\n## generate 1/sigma2 from theta\nnun = nu0 + n\ns2n = (nu0 * s20 + (n - 1) * var.y + n * (mean.y - phi[1])^2 ) / nun\nphi[2] = rgamma(1, nun/2, nun * s2n / 2)\n\n## update chain\nPHI[s,] = phi\n}\n\nNote: in this code we use the identity \\(n s_n^2(\\theta) = (n-1)s^2 + n (\\bar{y} - \\theta)^2\\).\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(latex2exp)\n# plotting the joint posterior\ndf = as.data.frame(PHI)\nnames(df) = c(\"theta\", \"prec\")\ndf %>%\n  ggplot(aes(x = theta, y = prec)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  labs(x = TeX(\"$\\\\theta$\"),\n       y = TeX(\"$1/\\\\sigma^2$\"),\n       fill = TeX(\"$p(\\\\theta, 1/\\\\sigma^2 | y_1, \\\\ldots y_n)$\")) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nSince the sequence \\(\\{\\phi^{(s)} \\}\\) depends on \\(\\phi^{(0)}, \\ldots \\phi^{(s-1)}\\) only through \\(\\phi^{(s-1)}\\) we say the sequence is memoryless. This is called the Markov property, and so the sequence is a Markov chain.\n‚ÄúWhat happens next depends only on the state of affairs now‚Äù\n\n\nUnder some conditions,\n\\[\np(\\phi^{(s)} \\in A) \\rightarrow \\int_A p(\\phi) d\\phi \\ \\ \\text{ as } s \\rightarrow \\infty\n\\]\ni.e.¬†the sampling distribution of \\(\\phi^{(s)}\\) approaches the target distribution as \\(s \\rightarrow \\infty\\) regardless of \\(\\phi^{(0)}\\).\nFurthermore,\n\\[\n\\frac{1}{S} \\sum_{s=1}^S g(\\phi^{(s)})  \\rightarrow E[g(\\phi)]\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nBig take-away: if we can sample from the full conditional posteriors, we can construct a Markov chain with samples from the joint posterior! We can then use Monte Carlo approximation to use the samples to summarize aspects of the posterior."
  },
  {
    "objectID": "notes/lec10-gibbs-sampling.html",
    "href": "notes/lec10-gibbs-sampling.html",
    "title": "Gibbs sampling",
    "section": "",
    "text": "Gibbs sampler\nGibbs sampling is a special case of Metropolis-Hastings that proceeds as follows:\n\nsample \\(\\theta_1^{(s+1)}\\) from \\(p(\\theta_1 | \\theta_2^{(s)}, \\mathbf{y})\\)\nsample \\(\\theta_2^{(s+1)}\\) from \\(p(\\theta_2|\\theta_1^{(s+1)}, \\mathbf{y})\\)\n\niterate steps 1-2 a total of \\(S\\) times.\n\nExercise\n\n\nProve that the algorithm described above is in fact a Metropolis-Hastings algorithm where \\(J(\\theta_i | \\theta_i^{(s)}) = p(\\theta_i | \\theta_{j \\neq i}, \\mathbf{y})\\) with acceptance probability 1.\nHint: recall that the Metropolis-Hastings acceptance ratio \\(r\\) is computed as follows:\n\\[\nr = \\frac{\\pi(\\theta^*)}{\\pi(\\theta^{(s)})} \\times\n\\frac{J(\\theta^{(s)}| \\theta^*)}{\nJ(\\theta^{*}| \\theta^{(s)})\n}\n\\]\n\n\n\n\n\nExample\nImagine the following target distribution (the joint probability distribution of two variables, \\(\\theta\\) and \\(\\delta\\)).\n\nplotcode\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(latex2exp)\nset.seed(360)\n\n## fixed values ## \nmu = c(-3, 0, 3) # conditional means\nsd = rep(sqrt(1 / 3), 3) # conditional sds\nd = c(1, 2, 3) # sample space of delta\nN = 1000 # number of samples\n\ndelta = sample(d, size = N, prob = c(.45, .1, .4), replace = TRUE)\ntheta = rnorm(N, mean = mu[delta], sd = sd[delta])\n\ndf = data.frame(delta, theta)\ndf %>%\n  ggplot(aes(x = theta, y = delta)) + \n  geom_bin2d(bins = 25) +\n  theme_bw() + \n  labs(y = TeX(\"\\\\delta\"), \n       x = TeX(\"\\\\theta\"))\n\n\n\n\nIn this example,\n\\[\n\\begin{aligned}\np(\\delta = d) = \\begin{cases}\n&.45 &\\text{ if } d = 1\\\\\n&.10 &\\text{ if } d = 2\\\\\n&.45 &\\text{ if } d = 3\n\\end{cases}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\{\\theta | \\delta = d\\} \\sim\n\\begin{cases}\n&N(-3, 1/3) &\\text{ if } d = 1\\\\\n&N(0, 1/3) &\\text{ if } d = 2\\\\\n&N(3, 1/3) &\\text{ if } d = 3\n\\end{cases}\n\\end{aligned}\n\\]\n\nExerciseSolution\n\n\nWrite down the full conditional distributions necessary to Gibbs sample the target.\nNote: this is a toy example. We can sample from the target distribution directly as seen above. However, we will construct a Gibbs sampler for pedagogical purposes that will become apparent momentarily.\n\n\n\nTo construct a Gibbs sampler, we need the full conditional distributions.\n\n\\(p(\\theta | \\delta)\\) is given.\n\\(p(\\delta| \\theta) = \\frac{p(\\theta | \\delta = d) p(\\delta = d)}{ \\sum_{d=1}^3p(\\theta | \\delta = d)p(\\delta = d)}\\), for \\(d \\in \\{1, 2, 3\\}\\). \n\n\n\n\nBelow the sampler is implemented and a trace plot for \\(\\theta\\) reported.\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n## fixed values ## \nmu = c(-3, 0, 3) # conditional means\ns2 = rep(1 / 3, 3) # conditional sds\nd = c(1, 2, 3) # sample space of delta\nN = 1000 # chain length\nw = c(.45, .1, .4) # delta probabilities\n\n## Gibbs sampler ##\nset.seed(360)\nN = 1000 # number of Gibbs samples\n\ntheta = 0 # initial theta value\nthd.mcmc = NULL\nfor(i in 1:N) {\nd = sample(1:3 , 1, prob = w * dnorm(theta, mu, sqrt(s2))) \ntheta = rnorm(1, mu[d], sqrt(s2[d]))\nthd.mcmc = rbind(thd.mcmc, c(theta,d))\n}\n# note we take advantage that sample() in R does not require the probability\n# to add up to 1\n\ndf = data.frame(theta = thd.mcmc[,1],\n                delta = thd.mcmc[,2])\n\ndf %>%\n  ggplot(aes(x = seq(1, nrow(df)), y = theta)) +\n  geom_line() +\n  theme_bw() +\n  labs(y = TeX(\"\\\\theta\"),\n       x = \"iteration\",\n       title = \"Traceplot of 1000 Gibbs samples\")\n\n\n\n\n\nExercise\n\n\n\ndescribe how we implement the conditional update for delta in the code above\nwhat do you notice from the traceplot above? Hint: you can imagine hopping from delta islands in the first figure of the joint target over parameter space.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe picture to visualize is that of a particle moving through parameter space.\n\n\n\n\n\nLet‚Äôs see how well our samples of \\(\\theta\\) approximate the true marginal \\(p(\\theta)\\)."
  },
  {
    "objectID": "notes/lec11-mvn.html",
    "href": "notes/lec11-mvn.html",
    "title": "Multivariate normal",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mvtnorm) \nlibrary(patchwork)"
  },
  {
    "objectID": "notes/lec11-mvn.html#matrix-algebra-fundamentals",
    "href": "notes/lec11-mvn.html#matrix-algebra-fundamentals",
    "title": "Multivariate normal",
    "section": "Matrix algebra fundamentals",
    "text": "Matrix algebra fundamentals\n\nmatrix facts\n\nmatrix multiplication proceeds row \\(\\times\\) column, so if we have the product \\(AB\\), \\(A\\) must have the same number of ___ as B has ___.\nthe determinant of a matrix, \\(|A|\\), measures the size of the matrix\nthe identity matrix is the matrix multiplicative identity. It is represented by \\(\\boldsymbol{I}\\), in general \\(\\boldsymbol{I}_p\\) is a \\(p \\times p\\) matrix with 1 on each diagonal and 0 on every off-diagonal. \\(\\boldsymbol{I}A = A \\boldsymbol{I}= A\\).\nthe inverse of a matrix \\(A^{-1}\\) works as follows: \\(A A^{-1} = A^{-1}A = \\boldsymbol{I}\\).\nthe trace of a matrix, tr(A), is the sum of its diagonal elements\norder matters: \\(AB \\neq BA\\) in general.\n\\(\\Sigma > 0\\) is shorthand for saying the matrix is positive definite. This means that for all vectors \\(\\boldsymbol{x}\\), the quadratic form \\(\\boldsymbol{x}^T \\Sigma \\boldsymbol{x} > 0\\). \\(\\Sigma > 0 \\iff\\) all eigenvalues of \\(\\Sigma\\) are positive.\n\n\nExercise\n\n\n\n\\(\\boldsymbol{\\theta}\\) and \\(\\boldsymbol{b}\\) are \\(p \\times 1\\) vectors, \\(A\\) is a symmetric matrix. Simplify \\(\\boldsymbol{b}^T A \\boldsymbol{\\theta}+ \\boldsymbol{\\theta}^T A \\boldsymbol{b}\\) what is the dimension of the result?\n\\(\\boldsymbol{y}\\) is a \\(p \\times 1\\) vector. What‚Äôs the dimension of \\(V[\\boldsymbol{y}]\\)?\n\n\n\n\n\n\nmatrix operations in R\n\n# make a matrix A\nA = matrix(c(1,.2, .2, 2), ncol = 2)\nA\n\n     [,1] [,2]\n[1,]  1.0  0.2\n[2,]  0.2  2.0\n\n# invert A (expensive for large matrices)\nAinv = solve(A)\n\n# matrix multiplication\nAinv %*% A\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n# determinant of A\ndet(A)\n\n[1] 1.96\n\n# trace of A\nsum(diag(A))\n\n[1] 3\n\n# create a vector b\nb = matrix(c(1, 2), ncol = 1)\nb\n\n     [,1]\n[1,]    1\n[2,]    2\n\n# transpose the vector b\nt(b)\n\n     [,1] [,2]\n[1,]    1    2\n\n\n\nb %*% A\n\nError in b %*% A: non-conformable arguments\n\n\n\nWhat went wrong in the code above?"
  },
  {
    "objectID": "notes/lec11-mvn.html#semiconjugate-priors",
    "href": "notes/lec11-mvn.html#semiconjugate-priors",
    "title": "Multivariate normal",
    "section": "Semiconjugate priors",
    "text": "Semiconjugate priors\n\n\n\n\n\n\nDefinition\n\n\n\nA semiconjugate or conditionally conjugate prior is a prior that is conjugate to the full conditional posterior.\n\n\n\nsemiconjugate prior for \\(\\boldsymbol{\\theta}\\)\nIf\n\\[\n\\begin{aligned}\n\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma &\\sim MVN(\\boldsymbol{\\theta}, \\Sigma),\\\\\n\\boldsymbol{\\theta}&\\sim MVN(\\mu_0, \\Lambda_0),\n\\end{aligned}\n\\]\nthen\n\\[\n\\boldsymbol{\\theta}| \\boldsymbol{y}, \\Sigma \\sim MVN(\\boldsymbol{\\mu_n}, \\Lambda_n),\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\Lambda_n &= (\\Lambda_0^{-1} + n \\Sigma^{-1} )^{-1},\\\\\n\\boldsymbol{\\mu_n} &= (\\Lambda_0^{-1} + n \\Sigma^{-1} )^{-1}(\\Lambda_0^{-1} \\boldsymbol{\\mu}_0 + n \\Sigma^{-1} \\bar{\\boldsymbol{y}}).\n\\end{aligned}\n\\]\n\nExercise\n\n\nInterpret \\(E[\\boldsymbol{\\theta}| \\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n, \\Sigma]\\) and \\(Cov[\\boldsymbol{\\theta}| \\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n, \\Sigma]\\).\n\n\n\n\n\nsemiconjugate prior for \\(\\Sigma\\)\nIf\n\\[\n\\begin{aligned}\n\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma &\\sim MVN(\\boldsymbol{\\theta}, \\Sigma),\\\\\n\\Sigma &\\sim \\text{inverse-Wishart}(\\nu_0, S_0^{-1}),\n\\end{aligned}\n\\]\nthen\n\\[\n\\Sigma | \\boldsymbol{y}, \\boldsymbol{\\theta}\\sim \\text{inverse-Wishart} (\\nu_0 + n, (S_0 + S_{\\theta})^{-1}),\n\\]\nwhere \\(S_\\theta = \\sum_{i=1}^n (\\boldsymbol{y}_i - \\boldsymbol{\\theta})(\\boldsymbol{y}_i - \\boldsymbol{\\theta})^T\\) is the residual sum of squares matrix for the vectors \\(\\boldsymbol{y}_1, \\ldots \\boldsymbol{y}_n\\) if the population mean is \\(\\boldsymbol{\\theta}\\).\n\n\n\nthe inverse-Wishart\nthe inverse-Wishart\\((\\nu_0, S_0^{-1})\\) density is given by\n\\[\n\\begin{aligned}\np(\\Sigma | \\nu_0, S_0^{-1}) = \\left[\n2^{\\nu_0 p / 2} \\pi^{{p \\choose 2}/2} |S_0|^{-\\nu_0/2} \\prod_{j = 1}^p \\Gamma([\\nu_0 + 1 - j]/2)\n\\right]^{-1} \\times\\\\\n|\\Sigma|^{-(\\nu_0 + p + 1)/2} \\times \\exp \\{ -\\frac{1}{2}tr(S_0 \\Sigma^{-1})\\}.\n\\end{aligned}\n\\]\n\nKey facts\n\nnotice that the first line is the normalizing constant of the density\nthe support is \\(\\Sigma > 0\\) and \\(\\Sigma\\) symmetric \\(p \\times p\\) matrix. \\(\\nu_0 \\in \\mathbb{N}^+\\) and \\(\\nu_0 \\geq p\\). \\(S_0\\) is a \\(p \\times p\\) symmetric positive definite matrix.\nif \\(\\Sigma\\) is inv-Wishart\\((\\nu_0, S_0^{-1})\\) then \\(\\Sigma^{-1}\\) is Wishart\\((\\nu_0, S_0^{-1})\\).\n\\(E[\\Sigma^{-1}] = \\nu_0 S_0^{-1}\\) and \\(E[\\Sigma] = \\frac{1}{\\nu_0 - p - 1} S_0\\).\nintuition: \\(\\nu_0\\) is prior sample size. \\(S_0\\) is a prior guess of the covariance matrix.\n\n\n\nsampling from the inverse-Wishart\n\npick \\(\\nu_0 > p\\), pick \\(S_0\\)\nsample \\(\\boldsymbol{z}_1, \\ldots \\boldsymbol{z}_{\\nu_0} \\sim \\text{ i.i.d. } MVN(\\boldsymbol{0}, S_0^{-1})\\)\ncalculate \\(\\boldsymbol{Z}^T \\boldsymbol{Z} = \\sum_{i = 1}^{\\nu_0} \\boldsymbol{z}_i \\boldsymbol{z}^T\\)\nset \\(\\Sigma = (\\boldsymbol{Z}^T \\boldsymbol{Z})^{-1}\\)\n\n\nlibrary(mvtnorm) # contains function rmvnorm\n\n# 2x2 example: generating 1 sample from an inv-Wishart\nset.seed(360)\np = 2\nnu0 = 3\nS0 = matrix(c(1, .1, .1, 1), ncol = 2)\nS0inv = solve(S0)\nZ = rmvnorm(n = nu0, # number of observations of the 2D vector Z\n        mean = rep(0, p), # mean 0\n        sigma = S0inv) # prior variance\nSigma = solve(t(Z) %*% Z)\neigen(Sigma)$values\n\n[1] 0.7821737 0.4174527\n\nSigma\n\n           [,1]      [,2]\n[1,]  0.5271834 -0.167273\n[2,] -0.1672730  0.672443\n\n\n\nExercise\n\n\nWhy does this work? Hint: what is \\(Var[\\boldsymbol{z}]\\)?\n\n\n\nWe can also use the monomvn package to simulate from a Wishart more succinctly,\n\nlibrary(monomvn)\n\nset.seed(360)\nSigma = solve(rwish(nu0, S0inv))\neigen(Sigma)$values\n\n[1] 0.692529 0.160428\n\nSigma\n\n          [,1]      [,2]\n[1,] 0.1899212 0.1217519\n[2,] 0.1217519 0.6630358"
  },
  {
    "objectID": "quizzes/quiz05-old.html",
    "href": "quizzes/quiz05-old.html",
    "title": "Quiz 5",
    "section": "",
    "text": "Exercise 1\nWhat is the purpose of Gibbs sampling?\n\n\nExercise 2\n\\(\\mathbf{x}\\) is a \\(p \\times 1\\) vector.\nWhat is the dimension of \\(\\mathbf{x}^T \\Sigma \\mathbf{x}\\)?\n\n\nExercise 3\nLet \\(\\mathbf{y} = \\left[ {\\begin{array}{cc}  y_1 \\\\  y_2  \\end{array} } \\right]\\), \\(\\boldsymbol{\\theta} = \\left[ {\\begin{array}{cc}  4 \\\\  8  \\end{array} } \\right]\\), \\(\\Sigma = \\left[ {\\begin{array}{cc}  1 & .2 \\\\  .2 & 1.3\\\\  \\end{array} } \\right]\\).\nIf \\(\\mathbf{y} | \\boldsymbol{\\theta}, \\Sigma \\sim MVN(\\boldsymbol{\\theta}, \\Sigma)\\), then \\(y_1 \\sim N(a, b)\\). What is \\(a\\) and \\(b\\)?\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "hw/hw06.html",
    "href": "hw/hw06.html",
    "title": "Homework 6",
    "section": "",
    "text": "6.2 from Hoff. Note the typo: \\(1/\\sigma_j^2\\) is gamma, not \\(1/\\sigma_j\\). Use the code below to load the data.\n\nglucose = readr::read_csv(\"https://sta360-fa24.github.io/data/glucose.csv\")"
  },
  {
    "objectID": "hw/hw06.html#exercise-2",
    "href": "hw/hw06.html#exercise-2",
    "title": "Homework 6",
    "section": "Exercise 2",
    "text": "Exercise 2\nRecall that if \\(W \\sim \\text{Wishart}_p(m, S)\\) then \\(W = \\sum_{i=1}^m z_i z_i^T\\) where \\(z_1, \\ldots, z_m \\sim \\text{i.i.d. } N_p(0, S)\\)\n\nShow that \\(E[W] = mS\\).\nShow that \\(W\\) is positive definite if \\(m \\geq p\\)."
  },
  {
    "objectID": "hw/hw06.html#exercise-3",
    "href": "hw/hw06.html#exercise-3",
    "title": "Homework 6",
    "section": "Exercise 3",
    "text": "Exercise 3\nSuppose \\(Y\\) is a random normal vector \\(Y \\sim N_p(\\theta, \\Sigma)\\). Let \\(Y_A\\) be the first \\(p_1\\) elements of \\(Y\\) and \\(Y_B\\) be the last \\(p_2 = p - p_1\\) elements, so that \\(Y = (Y_A, Y_B)\\). Similarly, write \\(\\theta = (\\theta_A, \\theta_B)\\). Finally, let\n\\[\n\\Sigma^{-} \\equiv \\Psi = \\left[ {\\begin{array}{cc}\n   \\Psi_{AA} & \\Psi_{AB} \\\\\n   \\Psi_{BA} & \\Psi_{BB} \\\\\n  \\end{array} } \\right]\n\\]\nand note that \\(\\Psi_{AB} = \\Psi_{BA}^T\\). Find the conditional distribution of \\(Y_B\\) given \\(Y_A\\) in terms of \\(\\theta_A\\), \\(\\theta_B\\) and components of \\(\\Psi\\). Try to interpret how \\(E[Y_B|Y_A]\\) differs from \\(E[Y_B]\\) and how \\(V[Y_B|Y_A]\\) differs from \\(V[Y_B]\\).\n\nIdentities for exercise 3\nSome of the following identities will be helpful for interpretation.\nLet\n\\[\n\\Sigma = \\left[ {\\begin{array}{cc}\n   \\Sigma_{AA} & \\Sigma_{AB} \\\\\n   \\Sigma_{BA} & \\Sigma_{BB} \\\\\n  \\end{array} } \\right]\n\\]\nand\n\\[\n\\Psi = \\left[ {\\begin{array}{cc}\n   \\Psi_{AA} & \\Psi_{AB} \\\\\n   \\Psi_{BA} & \\Psi_{BB} \\\\\n  \\end{array} } \\right].\n\\]\nThen\n\\[\n\\begin{aligned}\n\\Psi_{AA}^- &= \\Sigma_{AA} - \\Sigma_{AB} \\Sigma_{BB}^- \\Sigma_{BA}\\\\\n\\Psi_{BB}^- &= \\Sigma_{BB} - \\Sigma_{BA} \\Sigma_{AA}^- \\Sigma_{AB}\\\\\n\\Psi_{AB} &= -\\Psi_{AA} \\Sigma_{AB} \\Sigma_{BB}^-\\\\\n\\Psi_{BA} &= -\\Psi_{BB} \\Sigma_{BA} \\Sigma_{AA}^-,\n\\end{aligned}\n\\]\nand note that \\(\\Sigma_{AB} = \\Sigma_{BA}^T\\) and \\(\\Psi_{AB} = \\Psi_{BA}^T\\)."
  },
  {
    "objectID": "notes/lec11-mvn.html#visualizing-a-two-dimensional-normal",
    "href": "notes/lec11-mvn.html#visualizing-a-two-dimensional-normal",
    "title": "Multivariate normal",
    "section": "Visualizing a two-dimensional normal",
    "text": "Visualizing a two-dimensional normal\n\nparameterssimulate mvn datascatterplotdensity plot3dmarginals\n\n\n\nSigma = matrix(c(2, 0.7, 0.7, 1), \n               nrow = 2, ncol = 2)\nmu = c(0, 0)\n\n\nmu\n\n[1] 0 0\n\nSigma\n\n     [,1] [,2]\n[1,]  2.0  0.7\n[2,]  0.7  1.0\n\n\n\n\n\nset.seed(360)\nY = rmvnorm(n = 500, \n        mean = mu,\n        sigma = Sigma)\n\nY[1:5,]\n\n           [,1]       [,2]\n[1,]  2.0834251  0.7384787\n[2,] -0.5817255 -1.0144046\n[3,] -0.2769858 -0.7281417\n[4,] -0.9446903 -0.3398126\n[5,] -0.9972539  0.2615113\n\n\n\n\n\ndf = data.frame(y1 = Y[,1], y2 = Y[,2]) \nplot1 = df %>%\n  ggplot(aes(x = y1, y = y2)) +\n  geom_point() +\n  theme_bw()\nplot1\n\n\n\n\n\n\n\nplot1 +\n  geom_density_2d()\n\n\n\n\n\n\n\nx = seq(-5, 5, 0.25) \ny = seq(-5, 5, 0.25)\nf  = function(x, y) dmvnorm(cbind(x, y), mu, Sigma)\nz = outer(x, y, f)\npersp(x, y, z, theta = -30, phi = 25, \n      shade = 0.75, col = \"steelblue\", expand = 0.5, r = 2, \n      ltheta = 25, ticktype = \"detailed\", xlab =\"y1\",\n      ylab = \"y2\",\n      zlab = \"\")\n\n\n\n\n\n\n\np1 = df %>%\n  ggplot(aes(x = y1)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density() +\n  theme_bw()\n\np2 = df %>%\n  ggplot(aes(x = y2)) +\n  geom_histogram(aes(y = ..density..)) +\n  geom_density() +\n  theme_bw()\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDensity\nWe say a \\(p\\) dimensional vector \\(\\boldsymbol{Y}\\) has a multivariate normal distribution if its sampling density is given by\n\\[\np(\\boldsymbol{y}| \\boldsymbol{\\theta}, \\Sigma) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp\\{\n-\\frac{1}{2}(\\boldsymbol{y}-\\boldsymbol{\\theta})^T \\Sigma^{-1} (\\boldsymbol{y}- \\boldsymbol{\\theta})\n\\}\n\\]\nwhere\n\\[\n\\boldsymbol{y}=  \\left[ {\\begin{array}{cc}\n   y_1 \\\\\n   y_2\\\\\n   \\vdots\\\\\n   y_p\n  \\end{array} } \\right]\n  ~~~\n   \\boldsymbol{\\theta}= \\left[ {\\begin{array}{cc}\n   \\theta_1 \\\\\n   \\theta_2\\\\\n   \\vdots\\\\\n   \\theta_p\n  \\end{array} } \\right]\n  ~~~\n  \\Sigma =\n  \\left[ {\\begin{array}{cc}\n   \\sigma_1^2 & \\sigma_{12}& \\ldots & \\sigma_{1p}\\\\\n   \\sigma_{12} & \\sigma_2^2 &\\ldots & \\sigma_{2p}\\\\\n   \\vdots & \\vdots & & \\vdots\\\\\n   \\sigma_{1p} & \\ldots & \\ldots & \\sigma_p^2\n  \\end{array} } \\right].\n\\]\n\n\nKey facts\n\n\\(\\boldsymbol{y}\\in \\mathbb{R}^p\\) ; \\(\\boldsymbol{\\theta}\\in \\mathbb{R}^p\\); \\(\\Sigma > 0\\)\n\\(E[\\boldsymbol{y}] = \\boldsymbol{\\theta}\\)\n\\(V[\\boldsymbol{y}] = E[(\\boldsymbol{y}- \\boldsymbol{\\theta})(\\boldsymbol{y}- \\boldsymbol{\\theta})^T] = \\Sigma\\)\nMarginally, \\(y_i \\sim N(\\theta_i, \\sigma_i^2)\\).\nIf \\(\\boldsymbol{\\theta}\\) is a MVN random vector, then the kernel is \\(\\exp\\{-\\frac{1}{2} \\boldsymbol{\\theta}^T A \\boldsymbol{\\theta}+ \\boldsymbol{\\theta}^T \\boldsymbol{b} \\}\\). The mean is \\(A^{-1}\\boldsymbol{b}\\) and the covariance is \\(A^{-1}\\).\n\n\nsampling from a mvt norm\nlibrary(mvtnorm) contains functions we need.\n\nrmvnorm() to sample from a multivariate normal\ndmvnorm() to compute the density\npmvnorm() to compute the distribution function\nqmvnorm() to compute quantiles of the multivariate normal"
  },
  {
    "objectID": "slides/lab6-mcmc-d-practice.html#solution-1",
    "href": "slides/lab6-mcmc-d-practice.html#solution-1",
    "title": "MCMC diagnostics practice",
    "section": "Solution 1",
    "text": "Solution 1\n\nThe chain has not reached stationarity for either parameter. Different parts of the chain do not look the same.\nSampler appears to be mixing poorly (at least when run for only 1000 iterations) since we jump from one mode to another only once.\nThe parameters look correlated since jumps happen at the same (or nearly the same) iteration."
  },
  {
    "objectID": "slides/lab6-mcmc-d-practice.html#solution-2",
    "href": "slides/lab6-mcmc-d-practice.html#solution-2",
    "title": "MCMC diagnostics practice",
    "section": "Solution 2",
    "text": "Solution 2\nWe start around (10, 45) and up sampling around (50, 53). This rules out (A) and perhaps also (B). Notice the axes.\nPlot (C) looks like uncorrelated parameters.\nThe only plot with islands of high posterior density at these regions is plot (D), which shows an additional mode that we haven‚Äôt sampled at all.\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "notes/dynamicalSystems2.html",
    "href": "notes/dynamicalSystems2.html",
    "title": "Parameter Estimation in Dynamical Systems",
    "section": "",
    "text": "library(tidyverse)"
  },
  {
    "objectID": "notes/dynamicalSystems2.html#example-hares-lynx-population-dynamics",
    "href": "notes/dynamicalSystems2.html#example-hares-lynx-population-dynamics",
    "title": "Parameter Estimation in Dynamical Systems",
    "section": "Example: Hares-Lynx population dynamics",
    "text": "Example: Hares-Lynx population dynamics\nA Canadian business, Hudson‚Äôs Bay company is North America‚Äôs longest continually operating company (founded in 1670). During the 19th and 20th century, the company bought animal pelts of both snowshoe hares and lynx from trappers. Lynx are the most predominant natural predator of snowshoe hares. The counts of pelts provide an approximation to the population size of each animal. Data sourced from here.\n\nhares_lynx = read_csv(\"../data/hares-lynx.csv\")\nglimpse(hares_lynx)\n\nRows: 28\nColumns: 3\n$ year  <dbl> 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910‚Ä¶\n$ hares <dbl> 30.0, 47.2, 70.2, 77.4, 36.3, 20.6, 18.1, 21.4, 22.0, 25.4, 27.1‚Ä¶\n$ lynx  <dbl> 4.0, 6.1, 9.8, 35.2, 59.4, 41.7, 19.0, 13.0, 8.3, 9.1, 7.4, 8.0,‚Ä¶\n\n\n\nyear is the year of record\nhares is the number of hare pelts (thousands)\nlynx is the number of lynx pelts (thousands)\n\n\nDynamical System\nThe following system is an example known as ‚ÄúLotka-Volterra‚Äù dynamics which describe an interactive model of the population of predator and prey.\n\n\n\n\n\n\n\n\\(H(t)\\)\nNumber of hares at time t\n\n\n\\(H(0) = H_0\\)\nNumber of hares at time 0\n\n\n\\(L(t)\\)\nNumber of lynx at time t\n\n\n\\(L(0) = L_0\\)\nNumber of lynx at time 0\n\n\n\\(a_1\\)\nPer capita birth rate of hares\n\n\n\\(b_1\\)\nPer capita death rate of lynx\n\n\n\\(a_2\\)\nRate at which lynx eat hares\n\n\n\\(b_2\\)\nRate at which prey affects predator growth\n\n\n\nSome assumptions:\n\nThe prey population finds ample food at all times.\nThe food supply of the predator population depends entirely on the size of the prey population.\nThe rate of change of population is proportional to its size.\nDuring the process, the environment does not change in favor of one species, and genetic adaptation is inconsequential.\nPredators have limitless appetite.\nBoth populations can be described by a single variable. This amounts to assuming that the populations do not have a spatial or age distribution that contributes to the dynamics.\n\n\\[\n\\begin{aligned}\n\\frac{dH(t)}{dt} &= a_1 H(t) - a_2 H(t)L(t)\\\\\n\\frac{dL(t)}{dt} &= - b_1 L(t) + b_2 H(t)L(t)\n\\end{aligned}\n\\]\n\n\nStatistical model\nOne more assumption: we will assume multivariate normal noise in our observations \\(H(t), L(t)\\).\n\\[\n  \\begin{align}\n   \\begin{bmatrix}\n           H(t) \\\\\n           L(t)\n         \\end{bmatrix}\n         &\\sim\n         MVN \\left(\n         \\begin{bmatrix}\n           \\tilde{H}(t) \\\\\n           \\tilde{L}(t)\n         \\end{bmatrix},\n         \\Sigma\n          \\right)\n  \\end{align}\n\\]\nwhere \\(\\tilde{H}(t), \\tilde{L}(t)\\) is the solution of the system of differential equations and is a function of \\(H(0), L(0), a_1, a_2, b_1, b_2\\) as well as \\(t\\).\n\\(\\Sigma\\) is a \\(2 \\times 2\\) matrix\nWe place priors on the unknowns\n\\[\n\\begin{aligned}\na_1 &\\sim N()\\\\\na_2 &\\sim \\text{log-normal}\\\\\nb_1 &\\sim N()\\\\\nb_2 &\\sim \\text{gamma}()\n\\end{aligned}\n\\]\n\n\nParameter estimation"
  },
  {
    "objectID": "quizzes/quiz06-old.html",
    "href": "quizzes/quiz06-old.html",
    "title": "Quiz 6",
    "section": "",
    "text": "Exercise 1\n\\[\nY = X\\beta\n\\]\nWrite down \\(\\hat{\\beta}_{OLS}\\) as a function of \\(X\\) and \\(Y\\). Assume \\(X\\) is full rank.\n\n\nExercise 2\nTRUE or FALSE\n\\(\\hat{\\beta}_{OLS}\\) is biased.\n\n\nExercise 3\nTRUE or FALSE\n\\(y_i = \\beta_0 + \\beta_1 x_i^2\\) is a linear model.\n\n\n\n‚àí+\n03:00"
  },
  {
    "objectID": "notes/lec12-regresion.intro.html",
    "href": "notes/lec12-regresion.intro.html",
    "title": "Intro to regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(scatterplot3d)"
  },
  {
    "objectID": "notes/lec12-regresion.intro.html#background-review",
    "href": "notes/lec12-regresion.intro.html#background-review",
    "title": "Intro to regression",
    "section": "Background review",
    "text": "Background review\n\nLinear modeling and linear regression\n\n\n\n\n\n\nA generalized linear model states \\(E[Y|X] = g(X\\beta)\\), for some invertible ‚Äúlink‚Äù function \\(g\\).\nLinear regression is but a special case, \\(E[Y|X] = X \\beta\\). This is what we will focus on today.\nLeast squares regression (otherwise termed ‚Äúordinary least squares‚Äù or ‚ÄúOLS‚Äù) refers to a particular method of estimating \\(\\beta\\): minimize the sum of squared residuals.\n\n\n\nNotation\n\n\\(\\mathbf{y} = \\{y_1, \\ldots y_n\\}\\) is an \\(n \\times 1\\) vector outcomes. Also called the ‚Äúresponse‚Äù or ‚Äúdependent variable‚Äù. \\(y_i\\) is an individual observed outcome.\n\\(\\mathbf{x}_i\\) is a \\(p \\times 1\\) vector of predictors also called ‚Äúregressors‚Äù, ‚Äúindependent variables‚Äù, ‚Äúcovariates‚Äù, or ‚Äúfeatures‚Äù.\n\\(X\\) is a \\(n \\times p\\) matrix of all covariates. This is often referred to as ‚Äúthe data matrix‚Äù.\n\\(\\beta\\) is a \\(p \\times 1\\) vector of constants. These are referred to as parameters. These are fixed, but unknown numbers. Being Bayesian, we will describe our uncertainty about this population parameter vector using probability statements.\n\n\n\nCommon convention\nThe linear model\n\\[\nE[Y | X\\beta] = X \\beta\n\\]\noften has the hidden convention that the first column of \\(X\\) is all 1s and \\(\\beta_1\\) is understood to be the intercept term. E.g.\n\\[\nE [ Y | X ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_1\\\\\n    \\beta_2\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix}\n\\]\n\n\nIllustration of a linear model\n\nExample\nImagine we‚Äôve collected 3 measurements on a number of penguins:\n\nbody mass (g)\nbill length (mm)\nflipper length (mm)\n\nThe first five entries of our data set are given below:\n\n\n# A tibble: 5 √ó 3\n  body_mass_g bill_length_mm flipper_length_mm\n        <int>          <dbl>             <int>\n1        3750           39.1               181\n2        3800           39.5               186\n3        3250           40.3               195\n4        3450           36.7               193\n5        3650           39.3               190\n\n\nIn all, our data set contains the measurements of 342 penguins. Because we‚Äôve collected three measurements, each individual penguin can be represented as a point in three dimensional space:\n\n\n\n\n\nNow, imagine it‚Äôs hard to measure a penguin‚Äôs bodymass because it‚Äôs difficult to get them onto a scale. We wish to develop a linear model that uses bill length and flipper length to predict body mass,\n\\[\nE[Y|X] = X \\beta,\n\\]\nwhere\n\n\\(Y\\) is the body mass of the penguins and\n\\(X\\) contains covariates bill length and flipper length.\n\nWhat does our linear model look like?\n\n\n\n\n\nIn general, for \\(D\\) measurements, a linear model is a \\(D-1\\) dimensional hyperplane!\n\n\n\nTraditional way to find the hyperplane\nTo ‚Äúfit‚Äù a linear regression model means to estimate \\(\\beta\\). One way to do this is to minimize some objective function. A really common function to minimize is the sum of square residuals SSR.\nA residual is defined as the distance our mean is from the true value:\n\\[\n\\begin{aligned}\nr_i &= y_i - E[y_i | \\mathbf{x}\\beta]\\\\\n&= y_i - \\beta^T\\mathbf{x}_i\n\\end{aligned}\n\\]\nThus the sum of square residuals is:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n r_i^2 &= \\sum_{i=1}^n (y_i - \\beta^T \\mathbf{x}_i)^2\\\\\n&= (\\mathbf{y} -X\\beta)^T(\\mathbf{y} -X\\beta)\\\\\n&= \\mathbf{y}^T \\mathbf{y} - 2\\beta^T X^T\\mathbf{y} + \\beta^TX^TX\\beta)\n\\end{aligned}\n\\]\nExercise 1: what other objective functions could we optimize?\nThe ordinary least squares (OLS) estimate is\n\\[\n\\hat{\\beta}_{OLS} = (X^TX)^{-1}X^T \\mathbf{y}\n\\]\n\nSee the matrix cookbook by Petersen and Petersen for all your matrix algebra needs.\n\nExercise 2: is \\(\\hat{\\beta}_{OLS}\\) biased?\n\n\nNormal linear regression model\nSo far, we had not made any distributional assumptions, we only made an assumption about the expectation. Now for the normal linear regression model,\n\\[\nY = X \\beta + \\epsilon\n\\]\nand \\(\\epsilon \\sim N(0, \\sigma^2 I)\\) where \\(I\\) is a \\(n \\times n\\) identity matrix. This is a way of saying \\(\\epsilon_i \\sim_{iid} N(0, \\sigma^2)\\).\nTherefore,\n\\[\n\\mathbf{y} | X, \\beta, \\sigma^2 \\sim MVN(X\\beta, \\sigma^2 I)\n\\]\nExercise 3: What is \\(Var(\\hat{\\beta}_{OLS})\\) under the normal model?\n\n\n\n\n\n\nHint\n\n\n\nLet \\(z\\) be a random vector. \\(Var[Az] = A Var(z) A^T\\).\n\n\nExercise 4: What is \\(\\hat{\\beta}_{MLE}\\)?\n\n\nAssumptions\nA brief reminder about the flexibility and limitations of classical linear regression.\n\nLimitations so far:\n\nthe mean may not be a good summary of the conditional relationship, e.g.¬†if \\(p(y|x)\\) is skewed, multimodal, or has heavy tails.\nerror may not be iid. In other words, the conditional variance of \\(Y\\) may change with the \\(\\mathbf{x}\\)s.\n\n\n\nFlexibility\nIs this an example of linear regression?\n\\[\ny_i = \\beta_1 + \\beta_2x_1 + \\beta_3 x_1^2 + \\beta_4 \\log x_1 + \\beta_5 x_2 + \\beta_6 x_1 x_2 + \\epsilon_i\n\\]\nWhat‚Äôs linear about linear regression? The parameters!\nThis is powerful, because nonlinear relationships between \\(X\\) and \\(\\mathbf{y}\\) can often be corrected by a power transformation of \\(X\\), \\(\\mathbf{y}\\) or of both variables."
  },
  {
    "objectID": "notes/lec12-regresion.intro.html#bayesian-regression",
    "href": "notes/lec12-regresion.intro.html#bayesian-regression",
    "title": "Intro to regression",
    "section": "Bayesian regression",
    "text": "Bayesian regression\nLet‚Äôs assume the normal sampling model (i.e.¬†normal data generative process, aka normal likelihood),\n\\[\n\\mathbf{y} | X, \\beta, \\sigma^2 \\sim MVN(X\\beta, \\sigma^2I).\n\\]\nTo make inference about our model parameters, we will construct a posterior distribution,\n\\[\np(\\beta, \\sigma^2 | \\mathbf{y}, X) \\propto \\underbrace{ p(\\mathbf{y}|X, \\beta, \\sigma^2)}_{likelihood} \\underbrace{p(\\beta, \\sigma^2)}_{prior}\n\\]\n\nsemi-conjugate prior specification\nTo setup Gibbs sampling, let‚Äôs consider independent semi-conjugate priors, i.e.¬†assume \\(p(\\beta, \\sigma^2) = p(\\beta) p(\\sigma^2)\\)\nBefore reading ahead, what do you think semi-conjugate priors will be for the parameters under the normal linear regression model?\n\nsemi-conjuate prior on \\(\\beta\\)\nIf\n\\[\n\\beta \\sim MVN(\\beta_0, \\Sigma_0)\n\\]\nthen\n\\[\n\\begin{aligned}\np(\\beta|\\mathbf{y}, X, \\sigma^2) &\\propto\np( \\mathbf{y} | X, \\beta, \\sigma^2) p(\\beta)\\\\\n&\\propto MVN(\\mathbf{m}, V)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\nV = Var[\\beta | \\mathbf{y}, X, \\sigma^2] &=\n(\\Sigma_0^{-1} + X^TX / \\sigma^2)^{-1}\\\\\n\\mathbf{m} = E[\\beta | \\mathbf{y}, X, \\sigma^2] &= (\\Sigma_0^{-1} + X^T X/ \\sigma^2)^{-1}(\\Sigma_0^{-1} \\beta_0 + X^T\\mathbf{y} / \\sigma^2)\n\\end{aligned}\n\\]\n\n\nsemi-conjugate prior on \\(\\sigma^2\\)\nLet‚Äôs re-parameterize. Let \\(\\gamma = 1/\\sigma^2\\).\nIf\n\\[\n\\gamma \\sim \\text{gamma}(\\nu_0 /2, \\nu_0 \\sigma_0^2 / 2)\n\\]\nthen\n\\[\n\\begin{aligned}\np(\\gamma | \\mathbf{y}, X, \\beta) &\\propto p( \\mathbf{y} | X, \\beta, \\sigma^2) p(\\gamma)\\\\\n&\\propto\n\\text{gamma}([\\nu_0 + n]/2, [\\nu_0 \\sigma_0^2 + SSR(\\beta)]/2)\n\\end{aligned}\n\\]\nExercise 5: write out the pseudo-code of a Gibbs sampler that samples from \\(p(\\beta, \\sigma^2| \\mathbf{y}, X)\\)."
  },
  {
    "objectID": "notes/lec12-regresion-intro.html",
    "href": "notes/lec12-regresion-intro.html",
    "title": "Intro to regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(scatterplot3d)"
  },
  {
    "objectID": "notes/lec12-regresion-intro.html#background-review",
    "href": "notes/lec12-regresion-intro.html#background-review",
    "title": "Intro to regression",
    "section": "Background review",
    "text": "Background review\n\nLinear modeling and linear regression\n\n\n\n\n\n\nA generalized linear model states \\(E[Y|X] = g(X\\beta)\\), for some invertible ‚Äúlink‚Äù function \\(g\\).\nLinear regression is but a special case, \\(E[Y|X] = X \\beta\\). This is what we will focus on today.\nLeast squares regression (otherwise termed ‚Äúordinary least squares‚Äù or ‚ÄúOLS‚Äù) refers to a particular method of estimating \\(\\beta\\): minimize the sum of squared residuals.\n\n\n\nNotation\n\n\\(\\mathbf{y} = \\{y_1, \\ldots y_n\\}\\) is an \\(n \\times 1\\) vector outcomes. Also called the ‚Äúresponse‚Äù or ‚Äúdependent variable‚Äù. \\(y_i\\) is an individual observed outcome.\n\\(\\mathbf{x}_i\\) is a \\(p \\times 1\\) vector of predictors also called ‚Äúregressors‚Äù, ‚Äúindependent variables‚Äù, ‚Äúcovariates‚Äù, or ‚Äúfeatures‚Äù.\n\\(X\\) is a \\(n \\times p\\) matrix of all covariates. This is often referred to as ‚Äúthe data matrix‚Äù.\n\\(\\beta\\) is a \\(p \\times 1\\) vector of constants. These are referred to as parameters. These are fixed, but unknown numbers. Being Bayesian, we will describe our uncertainty about this population parameter vector using probability statements.\n\n\n\nCommon convention\nThe linear model\n\\[\nE[Y | X\\beta] = X \\beta\n\\]\noften has the hidden convention that the first column of \\(X\\) is all 1s and \\(\\beta_1\\) is understood to be the intercept term. E.g.\n\\[\nE [ Y | X ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_1\\\\\n    \\beta_2\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix}\n\\]\n\n\nIllustration of a linear model\n\nExample\nImagine we‚Äôve collected 3 measurements on a number of penguins:\n\nbody mass (g)\nbill length (mm)\nflipper length (mm)\n\nThe first five entries of our data set are given below:\n\n\n# A tibble: 5 √ó 3\n  body_mass_g bill_length_mm flipper_length_mm\n        <int>          <dbl>             <int>\n1        3750           39.1               181\n2        3800           39.5               186\n3        3250           40.3               195\n4        3450           36.7               193\n5        3650           39.3               190\n\n\nIn all, our data set contains the measurements of 342 penguins. Because we‚Äôve collected three measurements, each individual penguin can be represented as a point in three dimensional space:\n\n\n\n\n\nNow, imagine it‚Äôs hard to measure a penguin‚Äôs bodymass because it‚Äôs difficult to get them onto a scale. We wish to develop a linear model that uses bill length and flipper length to predict body mass,\n\\[\nE[Y|X] = X \\beta,\n\\]\nwhere\n\n\\(Y\\) is the body mass of the penguins and\n\\(X\\) contains covariates bill length and flipper length.\n\nWhat does our linear model look like?\n\n\n\n\n\nIn general, for \\(D\\) measurements, a linear model is a \\(D-1\\) dimensional hyperplane!\n\n\n\nTraditional way to find the hyperplane\nTo ‚Äúfit‚Äù a linear regression model means to estimate \\(\\beta\\). One way to do this is to minimize some objective function. A really common function to minimize is the sum of square residuals SSR.\nA residual is defined as the distance our mean is from the true value:\n\\[\n\\begin{aligned}\nr_i &= y_i - E[y_i | \\mathbf{x}\\beta]\\\\\n&= y_i - \\beta^T\\mathbf{x}_i\n\\end{aligned}\n\\]\nThus the sum of square residuals is:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n r_i^2 &= \\sum_{i=1}^n (y_i - \\beta^T \\mathbf{x}_i)^2\\\\\n&= (\\mathbf{y} -X\\beta)^T(\\mathbf{y} -X\\beta)\\\\\n&= \\mathbf{y}^T \\mathbf{y} - 2\\beta^T X^T\\mathbf{y} + \\beta^TX^TX\\beta)\n\\end{aligned}\n\\]\nExercise 1: what other objective functions could we optimize?\nThe ordinary least squares (OLS) estimate is\n\\[\n\\hat{\\beta}_{OLS} = (X^TX)^{-1}X^T \\mathbf{y}\n\\]\n\nSee the matrix cookbook by Petersen and Petersen for all your matrix algebra needs.\n\nExercise 2: is \\(\\hat{\\beta}_{OLS}\\) biased?\n\n\nNormal linear regression model\nSo far, we had not made any distributional assumptions, we only made an assumption about the expectation. Now for the normal linear regression model,\n\\[\nY = X \\beta + \\epsilon\n\\]\nand \\(\\epsilon \\sim N(0, \\sigma^2 I)\\) where \\(I\\) is a \\(n \\times n\\) identity matrix. This is a way of saying \\(\\epsilon_i \\sim_{iid} N(0, \\sigma^2)\\).\nTherefore,\n\\[\n\\mathbf{y} | X, \\beta, \\sigma^2 \\sim MVN(X\\beta, \\sigma^2 I)\n\\]\nExercise 3: What is \\(Var(\\hat{\\beta}_{OLS})\\) under the normal model?\n\n\n\n\n\n\nHint\n\n\n\nLet \\(z\\) be a random vector. \\(Var[Az] = A Var(z) A^T\\).\n\n\nExercise 4: What is \\(\\hat{\\beta}_{MLE}\\)?\n\n\nAssumptions\nA brief reminder about the flexibility and limitations of classical linear regression.\n\nLimitations so far:\n\nthe mean may not be a good summary of the conditional relationship, e.g.¬†if \\(p(y|x)\\) is skewed, multimodal, or has heavy tails.\nerror may not be iid. In other words, the conditional variance of \\(Y\\) may change with the \\(\\mathbf{x}\\)s.\n\n\n\nFlexibility\nIs this an example of linear regression?\n\\[\ny_i = \\beta_1 + \\beta_2x_1 + \\beta_3 x_1^2 + \\beta_4 \\log x_1 + \\beta_5 x_2 + \\beta_6 x_1 x_2 + \\epsilon_i\n\\]\nWhat‚Äôs linear about linear regression? The parameters!\nThis is powerful, because nonlinear relationships between \\(X\\) and \\(\\mathbf{y}\\) can often be corrected by a power transformation of \\(X\\), \\(\\mathbf{y}\\) or of both variables."
  },
  {
    "objectID": "notes/lec12-regresion-intro.html#bayesian-regression",
    "href": "notes/lec12-regresion-intro.html#bayesian-regression",
    "title": "Intro to regression",
    "section": "Bayesian regression",
    "text": "Bayesian regression\nLet‚Äôs assume the normal sampling model (i.e.¬†normal data generative process, aka normal likelihood),\n\\[\n\\mathbf{y} | X, \\beta, \\sigma^2 \\sim MVN(X\\beta, \\sigma^2I).\n\\]\nTo make inference about our model parameters, we will construct a posterior distribution,\n\\[\np(\\beta, \\sigma^2 | \\mathbf{y}, X) \\propto \\underbrace{ p(\\mathbf{y}|X, \\beta, \\sigma^2)}_{likelihood} \\underbrace{p(\\beta, \\sigma^2)}_{prior}\n\\]\n\nsemi-conjugate prior specification\nTo setup Gibbs sampling, let‚Äôs consider independent semi-conjugate priors, i.e.¬†assume \\(p(\\beta, \\sigma^2) = p(\\beta) p(\\sigma^2)\\)\nBefore reading ahead, what do you think semi-conjugate priors will be for the parameters under the normal linear regression model?\n\nsemi-conjuate prior on \\(\\beta\\)\nIf\n\\[\n\\beta \\sim MVN(\\beta_0, \\Sigma_0)\n\\]\nthen\n\\[\n\\begin{aligned}\np(\\beta|\\mathbf{y}, X, \\sigma^2) &\\propto\np( \\mathbf{y} | X, \\beta, \\sigma^2) p(\\beta)\\\\\n&\\propto MVN(\\mathbf{m}, V)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\nV = Var[\\beta | \\mathbf{y}, X, \\sigma^2] &=\n(\\Sigma_0^{-1} + X^TX / \\sigma^2)^{-1}\\\\\n\\mathbf{m} = E[\\beta | \\mathbf{y}, X, \\sigma^2] &= (\\Sigma_0^{-1} + X^T X/ \\sigma^2)^{-1}(\\Sigma_0^{-1} \\beta_0 + X^T\\mathbf{y} / \\sigma^2)\n\\end{aligned}\n\\]\n\n\nsemi-conjugate prior on \\(\\sigma^2\\)\nLet‚Äôs re-parameterize. Let \\(\\gamma = 1/\\sigma^2\\).\nIf\n\\[\n\\gamma \\sim \\text{gamma}(\\nu_0 /2, \\nu_0 \\sigma_0^2 / 2)\n\\]\nthen\n\\[\n\\begin{aligned}\np(\\gamma | \\mathbf{y}, X, \\beta) &\\propto p( \\mathbf{y} | X, \\beta, \\sigma^2) p(\\gamma)\\\\\n&\\propto\n\\text{gamma}([\\nu_0 + n]/2, [\\nu_0 \\sigma_0^2 + SSR(\\beta)]/2)\n\\end{aligned}\n\\]\nExercise 5: write out the pseudo-code of a Gibbs sampler that samples from \\(p(\\beta, \\sigma^2| \\mathbf{y}, X)\\)."
  },
  {
    "objectID": "notes/lec12-regression-intro.html",
    "href": "notes/lec12-regression-intro.html",
    "title": "Intro to regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(scatterplot3d)"
  },
  {
    "objectID": "notes/lec12-regression-intro.html#background-review",
    "href": "notes/lec12-regression-intro.html#background-review",
    "title": "Intro to regression",
    "section": "Background review",
    "text": "Background review\n\nLinear modeling and linear regression\n\n\n\n\n\n\nLinear regression is but a special case of modeling the conditional expectation of one variable \\(Y\\) given other variables \\(X\\) where \\(E[Y|X] = X \\beta\\). This is what we will focus on today.\nA generalized linear model states \\(E[Y|X] = g(X\\beta)\\), for some invertible ‚Äúlink‚Äù function \\(g\\).\nLeast squares regression (otherwise termed ‚Äúordinary least squares‚Äù or ‚ÄúOLS‚Äù) refers to a particular method of estimating \\(\\beta\\): minimize the sum of squared residuals.\n\n\n\nNotation\n\n\\(\\mathbf{y} = \\{y_1, \\ldots y_n\\}\\) is an \\(n \\times 1\\) vector outcomes. Also called the ‚Äúresponse‚Äù or ‚Äúdependent variable‚Äù. \\(y_i\\) is an individual observed outcome.\n\\(\\mathbf{x}_i\\) is a \\(p \\times 1\\) vector of predictors also called ‚Äúregressors‚Äù, ‚Äúindependent variables‚Äù, ‚Äúcovariates‚Äù, or ‚Äúfeatures‚Äù.\n\\(X\\) is a \\(n \\times p\\) matrix of all covariates. This is often referred to as ‚Äúthe data matrix‚Äù.\n\\(\\beta\\) is a \\(p \\times 1\\) vector of constants. These are referred to as parameters. These are fixed, but unknown numbers. Being Bayesian, we will describe our uncertainty about this population parameter vector using probability statements.\n\n\n\nCommon convention\nThe linear model\n\\[\nE[Y | X\\beta] = X \\beta\n\\]\noften has the hidden convention that the first column of \\(X\\) is all 1s and \\(\\beta_1\\) is understood to be the intercept term. E.g.\n\\[\nE [ Y | X ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_1\\\\\n    \\beta_2\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix}\n\\]\n\n\nIllustration of a linear model\n\nExample\nImagine we‚Äôve collected 3 measurements on a number of penguins:\n\nbody mass (g)\nbill length (mm)\nflipper length (mm)\n\nThe first five entries of our data set are given below:\n\n\n# A tibble: 5 √ó 3\n  body_mass_g bill_length_mm flipper_length_mm\n        <int>          <dbl>             <int>\n1        3750           39.1               181\n2        3800           39.5               186\n3        3250           40.3               195\n4        3450           36.7               193\n5        3650           39.3               190\n\n\nIn all, our data set contains the measurements of 342 penguins. Because we‚Äôve collected three measurements, each individual penguin can be represented as a point in three dimensional space:\n\n\n\n\n\nNow, imagine it‚Äôs hard to measure a penguin‚Äôs bodymass because it‚Äôs difficult to get them onto a scale. We wish to develop a linear model that uses bill length and flipper length to predict body mass,\n\\[\nE[Y|X] = X \\beta,\n\\]\nwhere\n\n\\(Y\\) is the body mass of the penguins and\n\\(X\\) contains covariates bill length and flipper length.\n\nWhat does our linear model look like?\n\n\n\n\n\nIn general, for \\(D\\) measurements, a linear model is a \\(D-1\\) dimensional hyperplane!\n\n\n\nTraditional way to find the hyperplane\nTo ‚Äúfit‚Äù a linear regression model means to estimate \\(\\beta\\). One way to do this is to minimize some objective function. A really common function to minimize is the sum of square residuals SSR.\nA residual is defined as the distance our mean is from the true value:\n\\[\n\\begin{aligned}\nr_i &= y_i - E[y_i | \\mathbf{x}\\beta]\\\\\n&= y_i - \\beta^T\\mathbf{x}_i\n\\end{aligned}\n\\]\nThus the sum of square residuals is:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n r_i^2 &= \\sum_{i=1}^n (y_i - \\beta^T \\mathbf{x}_i)^2\\\\\n&= (\\mathbf{y} -X\\beta)^T(\\mathbf{y} -X\\beta)\\\\\n&= \\mathbf{y}^T \\mathbf{y} - 2\\beta^T X^T\\mathbf{y} + \\beta^TX^TX\\beta)\n\\end{aligned}\n\\]\nThe ordinary least squares (OLS) estimate is\n\\[\n\\hat{\\beta}_{OLS} = (X^TX)^{-1}X^T \\mathbf{y}\n\\]\n\nSee the matrix cookbook by Petersen and Petersen for all your matrix algebra needs.\n\n\nExercise 1Exercise 2\n\n\nWhat other objective functions could we optimize?\n\n\nShow that \\(\\hat{\\beta}_{OLS}\\) is un-biased. In other words, show that \\(E[\\hat{\\beta}_{OLS} | \\beta] = \\beta\\).\n\n\n\n\n\nNormal linear regression model\nSo far, we had not made any distributional assumptions, we only made an assumption about the expectation. Now for the normal linear regression model,\n\\[\nY = X \\beta + \\epsilon\n\\]\nand \\(\\epsilon \\sim N(0, \\sigma^2 I)\\) where \\(I\\) is a \\(n \\times n\\) identity matrix. This is a way of saying \\(\\epsilon_i \\sim_{iid} N(0, \\sigma^2)\\).\nTherefore,\n\\[\n\\mathbf{y} | X, \\beta, \\sigma^2 \\sim MVN(X\\beta, \\sigma^2 I)\n\\]\n\nExercise 3\n\n\nShow that \\(Var(\\hat{\\beta}_{OLS} | \\beta, \\sigma^2) = \\sigma^2(X^TX)^{-1}\\) under the normal model above.\n\n\n\n\n\n\n\n\n\nHint\n\n\n\nLet \\(z\\) be a random vector. \\(Var[Az] = A Var(z) A^T\\).\n\n\n\nExercise 4\n\n\nWhat is \\(\\hat{\\beta}_{MLE}\\)?\n\n\n\n\n\nAssumptions\nA brief reminder about the flexibility and limitations of classical linear regression.\n\nLimitations so far:\n\nthe mean may not be a good summary of the conditional relationship, e.g.¬†if \\(p(y|x)\\) is skewed, multimodal, or has heavy tails.\nerror may not be iid. In other words, the conditional variance of \\(Y\\) may change with the \\(\\mathbf{x}\\)s.\n\n\n\nFlexibility\nIs this an example of linear regression?\n\\[\ny_i = \\beta_1 + \\beta_2x_1 + \\beta_3 x_1^2 + \\beta_4 \\log x_1 + \\beta_5 x_2 + \\beta_6 x_1 x_2 + \\epsilon_i\n\\]\nWhat‚Äôs linear about linear regression? The parameters!\nThis is powerful, because nonlinear relationships between \\(X\\) and \\(\\mathbf{y}\\) can often be corrected by a power transformation of \\(X\\), \\(\\mathbf{y}\\) or of both variables."
  },
  {
    "objectID": "notes/lec12-regression-intro.html#bayesian-regression",
    "href": "notes/lec12-regression-intro.html#bayesian-regression",
    "title": "Intro to regression",
    "section": "Bayesian regression",
    "text": "Bayesian regression\nLet‚Äôs assume the normal sampling model (i.e.¬†normal data generative process, aka normal likelihood),\n\\[\n\\mathbf{y} | X, \\beta, \\sigma^2 \\sim MVN(X\\beta, \\sigma^2I).\n\\]\nTo make inference about our model parameters, we will construct a posterior distribution,\n\\[\np(\\beta, \\sigma^2 | \\mathbf{y}, X) \\propto \\underbrace{ p(\\mathbf{y}|X, \\beta, \\sigma^2)}_{likelihood} \\underbrace{p(\\beta, \\sigma^2)}_{prior}\n\\]\n\nsemi-conjugate prior specification\nTo setup Gibbs sampling, let‚Äôs consider independent semi-conjugate priors, i.e.¬†assume \\(p(\\beta, \\sigma^2) = p(\\beta) p(\\sigma^2)\\)\nBefore reading ahead, what do you think semi-conjugate priors will be for the parameters under the normal linear regression model?\n\nsemi-conjuate prior on \\(\\beta\\)\nIf\n\\[\n\\beta \\sim MVN(\\beta_0, \\Sigma_0)\n\\]\nthen\n\\[\n\\begin{aligned}\np(\\beta|\\mathbf{y}, X, \\sigma^2) &\\propto\np( \\mathbf{y} | X, \\beta, \\sigma^2) p(\\beta)\\\\\n&\\propto MVN(\\mathbf{m}, V)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\nV = Var[\\beta | \\mathbf{y}, X, \\sigma^2] &=\n(\\Sigma_0^{-1} + X^TX / \\sigma^2)^{-1}\\\\\n\\mathbf{m} = E[\\beta | \\mathbf{y}, X, \\sigma^2] &= (\\Sigma_0^{-1} + X^T X/ \\sigma^2)^{-1}(\\Sigma_0^{-1} \\beta_0 + X^T\\mathbf{y} / \\sigma^2)\n\\end{aligned}\n\\]\n\n\nsemi-conjugate prior on \\(\\sigma^2\\)\nLet‚Äôs re-parameterize. Let \\(\\gamma = 1/\\sigma^2\\).\nIf\n\\[\n\\gamma \\sim \\text{gamma}(\\nu_0 /2, \\nu_0 \\sigma_0^2 / 2)\n\\]\nthen\n\\[\n\\begin{aligned}\np(\\gamma | \\mathbf{y}, X, \\beta) &\\propto p( \\mathbf{y} | X, \\beta, \\sigma^2) p(\\gamma)\\\\\n&\\propto\n\\text{gamma}([\\nu_0 + n]/2, [\\nu_0 \\sigma_0^2 + SSR(\\beta)]/2)\n\\end{aligned}\n\\]\n\nExercise 5\n\n\nWrite out the pseudo-code of a Gibbs sampler that samples from \\(p(\\beta, \\sigma^2| \\mathbf{y}, X)\\)."
  },
  {
    "objectID": "hw/hw07.html",
    "href": "hw/hw07.html",
    "title": "Homework 7",
    "section": "",
    "text": "Ridge regression theory: Let \\(y \\sim N_n(X \\beta, \\sigma^2 I)\\). Consider estimating \\(\\beta\\) with the prior distribution \\(\\beta | \\sigma^2 \\sim N_p(0, \\sigma^2 I / \\lambda)\\), where \\(\\lambda\\) is known and \\(\\beta\\) and \\(\\sigma^2\\) are unknown.\n\nDerive the conditional distribution of \\(\\beta | y, \\sigma^2\\) and, in particular, show that \\(E[\\beta | y] = (X^TX + I \\lambda)^{-1} X^Ty\\). Denote this expectation \\(\\hat{\\beta}_\\lambda\\), which we can use as an estimator of \\(\\beta\\). What happens to \\(\\hat{\\beta}_\\lambda\\) as \\(\\lambda \\rightarrow 0\\)?\nConsider the special case that \\(X^TX\\) is a diagonal matrix with entries \\(x_1^T x_1, \\ldots, x_p^T x_p\\). Write down the mathematical expression for an individual entry of the estimator, i.e.¬†\\(\\hat{\\beta}_{\\lambda~i}\\) (where \\(i\\) is the \\(i\\)th entry of the vector). Further write down the mathematical expression for individual entries of the OLS estimator \\(\\hat{\\beta}_{OLS~i}\\). Compare the two and explain, in words, the effect of \\(\\lambda\\)."
  },
  {
    "objectID": "hw/hw07.html#exercise-2",
    "href": "hw/hw07.html#exercise-2",
    "title": "Homework 7",
    "section": "Exercise 2",
    "text": "Exercise 2\nRidge regression application: The data set yX.diabetes.train contains data on diabetes progression (first column) and 64 predictor variables. These data can be loaded with with command\n\nyX<-dget(url(\"https://sta360-fa24.github.io/data/yX.diabetes.train\"))\n\n\nFor each value of \\(\\lambda \\in \\{0, 1, \\ldots, 99, 100 \\}\\) compute the estimator \\(\\hat{\\beta}_{\\lambda}\\) based on exercise 1 above. Visualize each \\(\\hat{\\beta}\\) as a function of \\(\\lambda\\) (maybe using matplot). Describe any trend(s) you notice in the plots.\nLoad the data set yX.diabetes.test using the code below\n\n\nyX.diabetes.test<-dget(\n  url(\"https://sta360-fa24.github.io/data/yX.diabetes.test\"))\n\nUse yX.diabetes.test to evaluate the predictive performance of each estimate you obtained in part a. Specifically, compute the predictive error sum of squares \\(PSS(\\lambda) = ||y_{test} - X_{test} \\hat{\\beta}_{\\lambda}||^2\\) for each value of \\(\\lambda\\) (IMPORTANT: \\(\\hat{\\beta}_{\\lambda}\\) is obtained from the training data in part a, not the test data). Make a plot of PSS versus \\(\\lambda\\). How good is the unbiased OLS estimate for prediction, relative to the other estimates?\n\nIdentify the value of \\(\\lambda\\) that has the best predictive performance. For this best value of \\(\\lambda\\), report which x-variables have the largest effects."
  },
  {
    "objectID": "hw/hw07.html#exercise-3",
    "href": "hw/hw07.html#exercise-3",
    "title": "Homework 7",
    "section": "Exercise 3",
    "text": "Exercise 3\nFor this exercise, use the code below to load the data\n\nyX = readRDS(\n  url(\"https://sta360-fa24.github.io/data/yXspectroscopy.rds\"))\n\nSource separation: The first column y of the dataset yXSS.rds is the vectorization of a spectroscopy image of a water sample taken from the Neuse River in North Carolina. You can view the image with the following code: y<-yX[,1] ; image(matrix(y,151,43)). The water sample is of unknown origin, but it is assumed that it is a mix of water from 9 different categories, whose average spectroscopy images are given by the remaining 9 columns \\(X\\) of yX. You can view these images with the same code above, applied to each column of \\(X\\).\n\nFrom \\(y\\) and \\(X\\), infer the sources of the water sample using the linear model \\(E[y|X, \\beta] = X\\beta\\). Assuming the normal linear model and with priors \\(\\beta \\sim N_9(1/9, \\ldots 1/9), I_9)\\), \\(1/\\sigma^2 \\sim \\text{gamma}(1, 1)\\), use a Gibbs sampler to obtain a posterior distribution of \\(\\beta\\) and \\(\\sigma^2\\) given \\(y\\). Plot the posterior density of \\(\\sigma^2\\), and obtain posterior 95% confidence intervals for each element of \\(\\beta\\). Which of the nine categories are the main sources of the water sample?\nEvaluate the assumptions of the normal linear model using some residual plots, addressing the assumption that the entries of \\(y\\) have constant variance, are uncorrelated, and are normally distributed. Hint: plots residuals vs yhat and a QQ plot.\nFor this problem it doesn‚Äôt make sense for the coefficients of \\(\\beta\\) to be negative. Think of a modification to the prior distribution for \\(\\beta\\) that takes this fact into account, and describe how you could sample \\(\\beta\\) under this updated model (you don‚Äôt have to implement it)."
  },
  {
    "objectID": "notes/waterOnMars.html",
    "href": "notes/waterOnMars.html",
    "title": "Water on Mars",
    "section": "",
    "text": "load libraries\nlibrary(truncnorm)\nlibrary(tidyverse)\nlibrary(coda)"
  },
  {
    "objectID": "notes/lec13-BayesianRegression2.html",
    "href": "notes/lec13-BayesianRegression2.html",
    "title": "Bayesian regression II",
    "section": "",
    "text": "See libraries used in these notes\nlibrary(tidyverse)\nlibrary(latex2exp)\nlibrary(patchwork)\nlibrary(tidymodels)\nlibrary(scatterplot3d)\nlibrary(palmerpenguins)\nlibrary(mvtnorm)\nlibrary(coda)\nlibrary(animation)"
  },
  {
    "objectID": "notes/lec13-BayesianRegression2.html#gibbs-sampler",
    "href": "notes/lec13-BayesianRegression2.html#gibbs-sampler",
    "title": "Bayesian regression II",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\n\nLast time\nWe set up the model,\n\\[\n\\begin{aligned}\n\\mathbf{y} | X, \\beta, \\sigma^2 &\\sim MVN(X\\beta, \\sigma^2 I)\\\\\n\\beta &\\sim MVN(\\beta_0, \\Sigma_0)\\\\\n1/\\sigma^2 &\\sim \\text{gamma}(\\nu_0/2, \\nu_0 \\sigma_0^2/2)\n\\end{aligned}\n\\]\nand derived the full conditionals\n\\[\n\\begin{aligned}\n\\beta | \\mathbf{y}, X, \\sigma^2 &\\sim MVN(\\mathbf{m}, V),\\\\\n1/\\sigma^2 | \\mathbf{y}, X, \\beta & \\sim\n\\text{gamma}([\\nu_0 + n]/2, [\\nu_0 \\sigma_0^2 + SSR(\\beta)]/2),\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\nV = Var[\\beta | \\mathbf{y}, X, \\sigma^2] &=\n(\\Sigma_0^{-1} + X^TX / \\sigma^2)^{-1},\\\\\n\\mathbf{m} = E[\\beta | \\mathbf{y}, X, \\sigma^2] &= (\\Sigma_0^{-1} + X^T X/ \\sigma^2)^{-1}(\\Sigma_0^{-1} \\beta_0 + X^T\\mathbf{y} / \\sigma^2).\n\\end{aligned}\n\\]\n\n\nDiffuse prior\nTo complete model specification, we must choose \\(\\beta_0\\), \\(\\Sigma_0\\), \\(\\sigma_0^2\\) and \\(\\nu_0\\).\nIf we know very little about the relationships between \\(X\\) and \\(\\mathbf{y}\\), we might wish to consider a ‚Äúdiffuse‚Äù prior that prescribes a large mass of uncertainty around each parameter.\n\nmath of priorpicture of prior\n\n\n\\[\n\\begin{aligned}\n\\beta & \\sim MVN(0, 1000 I)\\\\\n1/\\sigma^2 &\\sim \\text{gamma}(1, 10)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSampler pseudo-code\n\n\n\n\n\n\npseudo-code\n\n\n\n\npick a starting \\(\\sigma^{2(0)}\\), set \\(s = 0\\). Now for \\(s\\) in 1:S perform 1-3:\nupdate \\(\\beta\\):\n\n\ncompute \\(V\\) and \\(\\mathbf{m}\\)\nsample \\(\\beta^{(s+1)} \\sim MVN(\\mathbf{m}, V)\\)\n\n\nupdate \\(\\sigma^{2}\\):\n\n\ncompute \\(SSR(\\beta^{(s+1)})\\)\nsample \\(1/\\sigma^{2(s+1)} \\sim \\text{gamma}([\\nu_0 + n]/2, [\\nu_0 \\sigma_0^2 + SSR(\\beta^{s+1})]/2)\\)\n\n\nsave the states of \\(\\beta\\) and \\(\\sigma^2\\).\n\n\n\n\n\nCode\nThe Gibbs sampler for our penguin example:\n\n\nCode to reproduce penguins_subset\n# our example examines just a subset of the penguin data\npenguins_subset = penguins %>%\n  select(body_mass_g, flipper_length_mm, bill_length_mm) %>%\n  drop_na() %>%\n  mutate(body_mass_kg = body_mass_g / 1000) %>%\n  select(-body_mass_g)\n\nX = penguins_subset %>%\n  select(-body_mass_kg) %>%\n  mutate(one = rep(1, nrow(penguins_subset))) %>%\n  relocate(one) %>%\n  as.matrix() \n\ny = select(penguins_subset, \n           body_mass_kg) %>%\n  as.matrix()\n\n\n\nset.seed(360)\n# prior hyperparameters \np = 2 # number of covariates\nSigma0 = 1000 * diag(rep(1, p+1)) # p + 1 for intercept term\nb0 = rep(0, p + 1)\nnu0 = 2\nsigma02 = 10\nn = nrow(y)\n\n# starting values\n## note: gamma = 1 / sigma^2\ngamma = 1 / var(penguins_subset$body_mass_kg)\n\n# values we should compute just once\nSigmaInv = solve(Sigma0)\nX2 = t(X) %*% X\nXy = t(X) %*% y\nSIB0 = SigmaInv %*% b0\na = (nu0 + n) / 2\nnu0s02 = nu0 * sigma02\n\n## empty objects to fill\nBETA = NULL\nGAMMA = NULL\n\nS = 2000\nfor (s in 1:S) {\n  ### UPDATE BETA\n  V = solve(SigmaInv + (gamma * X2))\n  m = V %*% (Xy * gamma) # simplified since b0 = 0\n  beta = rmvnorm(1, mean = m, sigma = V)\n  \n  ### UPDATE SIGMA\n  SSR1 = (y - (X %*% t(beta)))\n  SSRB = t(SSR1) %*% SSR1\n  gamma = rgamma(1, a, ((nu0s02 + SSRB) / 2))\n  \n  ### SAVE STATES\n  GAMMA = c(GAMMA, gamma)\n  BETA = rbind(BETA, beta)\n}\n\n\n\nESS?\neffectiveSize(BETA)\n\n\nvar1 var2 var3 \n2000 2000 2000 \n\n\nESS?\neffectiveSize(GAMMA)\n\n\nvar1 \n2000 \n\n\nHow do posterior mean estimates compare to the OLS estimates?\n\nposteriorMean = apply(BETA, 2, mean)\nOLS = lm(body_mass_kg ~ flipper_length_mm + bill_length_mm, data = penguins_subset) \nOLS = OLS$coefficients\nrbind(OLS, posteriorMean)\n\n              (Intercept) flipper_length_mm bill_length_mm\nOLS             -5.736897        0.04814486    0.006047488\nposteriorMean   -5.723741        0.04801443    0.006336029\n\n\nWe might have figured out this is what we were going to see already based on the the fact that the expressions for \\(E[\\beta | \\mathbf{y}, X]\\) and \\(Var[\\beta | \\mathbf{y}, X]\\) look just like \\(E[\\hat{\\beta}_{OLS} | \\beta]\\) and \\(Var[\\hat{\\beta}_{OLS} | \\beta]\\) when the prior information is diffuse.\nWhat was the point of all that extra work? Well, we don‚Äôt just have a point estimate and a confidence interval, we have a whole posterior! We can quantify uncertainty about \\(\\beta\\) in an easy and intuitive way.\nUsing the posterior, we may find 95% posterior CI, compute \\(p(\\beta_i > 0 | \\mathbf{y}, X)\\), compute \\(p(\\beta_i > \\beta_j | \\mathbf{y}, X)\\), compute the posterior median, and a whole host of additional queries quickly and intuitively.\nLet‚Äôs take a look at the marginal posteriors.\n\n\n\n\n\n\nExercise\n\n\nIs flipper length or bill length a ‚Äúmore important‚Äù predictor of penguin body mass? Why?\n\n\n\n\n\nVisually\nFor each iteration of our Gibbs sampler, we‚Äôre sampling a hyperplane, i.e.¬†a set of \\(\\beta\\)s.\n\n\n\n\n\nExercise\n\n\nDiscuss how autocorrelation of \\(\\beta\\)s would affect our sampler based on the animation above."
  },
  {
    "objectID": "notes/lec13-BayesianRegression2.html#ridge-regression-and-the-normal-prior",
    "href": "notes/lec13-BayesianRegression2.html#ridge-regression-and-the-normal-prior",
    "title": "Bayesian regression II",
    "section": "Ridge regression and the normal prior",
    "text": "Ridge regression and the normal prior\nWhat if \\(p > n\\)? In words: what if we have more predictors than observations? \\(X\\) will be wide and therefore have linearly dependent columns.\nIn this case, \\(X^T X\\) is \\(p \\times p\\) but is of rank \\(n < p\\), i.e.¬†\\(X^TX\\) is not full rank and thus not invertible. Therefore, \\(\\hat{\\beta}_{OLS}\\) satisfying \\((X^T X)\\hat{\\beta}_{OLS} = X^T \\mathbf{y}\\) does not exist uniquely.\nSeparately, in the case of multicollinearity, where the columns of \\(X\\) are highly correlated, some eigenvalues of \\(X^TX\\) will be very small, which means \\((X^TX)^{-1}\\) will have very large eigenvalues, i.e.¬†\\(Var(\\hat{\\beta}_{OLS})\\) will be very large.\n\nIntuitively: we can fix this by shrinking some of the \\(\\beta_i\\) towards zero (reducing \\(p\\)).\nAlgebraically: one way we can fix this is by adding some positive quantity on the diagonals.\n\nFrequentists call this sort of algebraic fix ‚Äúridge regression‚Äù and define the problem thus:\n\\[\n\\hat{\\beta}_{ridge} = \\underset{\\beta}{\\mathrm{argmin}} \\underbrace{(\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X \\beta)}_{\\text{SSR}(\\beta)} + \\underbrace{\\lambda \\beta^T \\beta}_{L_2^2 ~\\text{penalty}}\n\\]\nwhere \\(\\lambda\\) is a tuning parameter called the ‚Äúridge coefficient‚Äù.\nBayesians obtain the same objective via the following prior on \\(\\beta\\),\n\\[\n\\beta | \\sigma^2, \\lambda \\sim MVN(0, \\sigma^2 I /\\lambda)\n\\]\n\nExercise\n\n\nShow that \\(\\hat{\\beta}_{ridge} = E[\\beta | \\mathbf{y}, X, \\sigma^2, \\lambda] = ((X^TX) + \\lambda I)^{-1} X^T \\mathbf{y}\\)."
  },
  {
    "objectID": "slides/lab8-rstan.html#exercise-overview",
    "href": "slides/lab8-rstan.html#exercise-overview",
    "title": "Easy Bayesian linear modeling",
    "section": "Exercise Overview",
    "text": "Exercise Overview\nYou want to build a spam filter that blocks emails that have a high probability of being spam.\nIn statistical terms, your outcome \\(Y\\) is the type of the email (spam or not). The 57 predictors from the data set are contained in \\(x\\) and include: the frequency of certain words, the occurrence of certain symbols and the use of capital letters in each email.\nLet \\(X = \\{x_1, \\ldots, x_n\\}\\) where \\(x_i \\in \\mathbb{R}^{58}\\) (number of predictors + 1 for intercept) and \\(n =\\) the number of emails in the data set. \\(Y \\in \\mathbb{R}^n\\).\n\\[\n\\begin{aligned}\np(y_i =1 | x_i, \\beta) = \\theta_i\\\\\n\\text{logit}(\\theta_i) = X\\beta\n\\end{aligned}\n\\]\nA priori, you believe that many of the predictors included in the data set do not in fact help you predict whether the email is spam. You express your beliefs with the prior designated below:\n\nPrior on interceptPrior on the rest\n\n\nLet \\(\\beta_0\\) be the intercept term.\n\\[\n\\beta_0 \\sim Normal(0, 100)\n\\]\n\n\nFor each \\(\\beta\\) associated with the 57 predictors:\n\\[\n\\beta_i \\sim \\text{iid}~Laplace(0, .5) \\ \\ \\text{for }i \\in \\{1, 57\\}{}\n\\]"
  },
  {
    "objectID": "slides/lab8-rstan.html#prior-on-intercept",
    "href": "slides/lab8-rstan.html#prior-on-intercept",
    "title": "Easy Bayesian linear modeling",
    "section": "Prior on intercept",
    "text": "Prior on intercept\nLet \\(\\beta_0\\) be the intercept term.\n\\[\n\\beta_0 \\sim Normal(0, 100)\n\\]"
  },
  {
    "objectID": "slides/lab8-rstan.html#prior-on-the-rest",
    "href": "slides/lab8-rstan.html#prior-on-the-rest",
    "title": "Easy Bayesian linear modeling",
    "section": "Prior on the rest",
    "text": "Prior on the rest\nFor each \\(\\beta\\) associated with the 57 predictors:\n\\[\n\\beta_i \\sim \\text{iid}~Laplace(0, .5) \\ \\ \\text{for }i \\in \\{1, 57\\}{}\n\\]"
  },
  {
    "objectID": "slides/lab8-rstan.html#standardize-the-data",
    "href": "slides/lab8-rstan.html#standardize-the-data",
    "title": "Easy Bayesian linear modeling",
    "section": "Standardize the data",
    "text": "Standardize the data\n\nStandardize dataTraining set\n\n\n\nBefore we fit our model to the data, we need to standardize the predictors (columns of \\(X\\)). Why is this important? Discuss.\n\n\n# scale functions re-scales columns of a df\nspam2 = cbind(\"type\" = spam$type, \n              scale(select(spam, -\"type\"))) %>%\n  data.frame()\n\n\n\nTo validate our model we will separate it into non-overlapping sets ‚Äì a training set and a testing set.\n\nset.seed(360) # ensures we get the same subset\nN = nrow(spam2)\nindices = sample(N, size = 0.8 * N)\nspam_train = spam2[indices,]\nspam_test = spam2[-indices,]\n# sanity check \nnrow(spam_train) + nrow(spam_test) == N\n\n[1] TRUE"
  },
  {
    "objectID": "slides/lab8-rstan.html#standardize-data",
    "href": "slides/lab8-rstan.html#standardize-data",
    "title": "Easy Bayesian linear modeling",
    "section": "Standardize data",
    "text": "Standardize data\n\nBefore we fit our model to the data, we need to standardize the predictors (columns of \\(X\\)). Why is this important? Discuss.\n\n\n# scale functions re-scales columns of a df\nspam2 = cbind(\"type\" = spam$type, \n              scale(select(spam, -\"type\"))) %>%\n  data.frame()"
  },
  {
    "objectID": "slides/lab8-rstan.html#training-set",
    "href": "slides/lab8-rstan.html#training-set",
    "title": "Easy Bayesian linear modeling",
    "section": "Training set",
    "text": "Training set\nTo validate our model we will separate it into non-overlapping sets ‚Äì a training set and a testing set.\n\nset.seed(360) # ensures we get the same subset\nN = nrow(spam2)\nindices = sample(N, size = 0.8 * N)\nspam_train = spam2[indices,]\nspam_test = spam2[-indices,]\n# sanity check \nnrow(spam_train) + nrow(spam_test) == N\n\n[1] TRUE"
  },
  {
    "objectID": "slides/lab8-rstan.html#exercise-1",
    "href": "slides/lab8-rstan.html#exercise-1",
    "title": "Easy Bayesian linear modeling",
    "section": "Exercise 1",
    "text": "Exercise 1\nRead about how to fit Bayesian logistic regression using rstanarm here: https://mc-stan.org/rstanarm/articles/binomial.html and write code to fit the spam_train data set.\nHint: use the stan_glm function. If you set arguments chains = 1, this will run 1 Markov chain instead of the default 4. You can use the argument iter=2000 to manually set the number of iterations in your Markov chain to 2000. This may take anywhere from 1-4 minutes to run locally on your machine. If you are pressed for time, you can load the resulting object directly from the website using the code below.\n\nfit1 = readRDS(url(\"https://sta360-fa24.github.io/data/spam-train-fit.rds\"))"
  },
  {
    "objectID": "slides/lab8-rstan.html#exercise-2",
    "href": "slides/lab8-rstan.html#exercise-2",
    "title": "Easy Bayesian linear modeling",
    "section": "Exercise 2",
    "text": "Exercise 2\nExamining the output\n\nDid stan_glm do what we think it did? Did the Markov chain converge? Which parameters, if any, have a posterior mean rounded to 0?\n\n\nquick lookcheck priorstrace plotsmarginal posteriorsplotting tipsget chainsummarize\n\n\nNotice sample: 1000 since half get thrown away by stan and is called ‚Äúburn-in‚Äù i.e.¬†a period that the chain spends reaching the target distribution gets discarded.\n\nsummary(fit1)\n\n\nModel Info:\n function:     stan_glm\n family:       binomial [logit]\n formula:      type ~ .\n algorithm:    sampling\n sample:       1000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 3680\n predictors:   58\n\nEstimates:\n                    mean   sd    10%   50%   90%\n(Intercept)        -3.9    0.5  -4.5  -3.9  -3.4\nmake               -0.1    0.1  -0.2  -0.1   0.0\naddress            -0.2    0.1  -0.4  -0.2  -0.1\nall                 0.1    0.1   0.0   0.1   0.1\nnum3d               0.9    0.7   0.2   0.7   1.7\nour                 0.4    0.1   0.3   0.4   0.5\nover                0.2    0.1   0.1   0.2   0.3\nremove              0.9    0.1   0.7   0.9   1.1\ninternet            0.2    0.1   0.1   0.2   0.3\norder               0.1    0.1   0.0   0.1   0.2\nmail                0.1    0.0   0.0   0.1   0.1\nreceive             0.0    0.1  -0.1   0.0   0.0\nwill               -0.2    0.1  -0.3  -0.2  -0.1\npeople             -0.1    0.1  -0.1   0.0   0.0\nreport              0.0    0.1  -0.1   0.0   0.1\naddresses           0.3    0.2   0.1   0.3   0.5\nfree                1.0    0.1   0.8   1.0   1.1\nbusiness            0.5    0.1   0.3   0.5   0.6\nemail               0.1    0.1   0.0   0.1   0.2\nyou                 0.1    0.1   0.0   0.1   0.2\ncredit              0.6    0.2   0.3   0.6   0.9\nyour                0.3    0.1   0.2   0.3   0.4\nfont                0.2    0.2   0.1   0.2   0.4\nnum000              0.7    0.2   0.5   0.7   0.9\nmoney               0.2    0.1   0.1   0.2   0.3\nhp                 -3.1    0.5  -3.8  -3.1  -2.5\nhpl                -1.0    0.4  -1.5  -1.0  -0.5\ngeorge             -8.5    1.8 -10.9  -8.5  -6.3\nnum650              0.2    0.1   0.1   0.2   0.4\nlab                -1.1    0.5  -1.8  -1.0  -0.5\nlabs               -0.1    0.1  -0.3  -0.1   0.0\ntelnet             -0.3    0.3  -0.8  -0.3   0.0\nnum857             -0.2    0.6  -1.0  -0.2   0.4\ndata               -0.7    0.2  -1.0  -0.7  -0.5\nnum415             -0.6    0.8  -1.7  -0.5   0.2\nnum85              -0.8    0.4  -1.3  -0.8  -0.3\ntechnology          0.4    0.1   0.2   0.4   0.6\nnum1999             0.0    0.1  -0.1   0.0   0.1\nparts              -0.2    0.1  -0.4  -0.2  -0.1\npm                 -0.4    0.2  -0.6  -0.4  -0.2\ndirect             -0.2    0.1  -0.3  -0.1   0.0\ncs                 -1.4    0.8  -2.4  -1.2  -0.5\nmeeting            -1.5    0.5  -2.2  -1.5  -1.0\noriginal           -0.4    0.2  -0.8  -0.4  -0.1\nproject            -1.3    0.4  -1.8  -1.3  -0.8\nre                 -0.7    0.2  -0.9  -0.7  -0.5\nedu                -1.5    0.3  -1.9  -1.5  -1.1\ntable              -0.3    0.1  -0.5  -0.3  -0.1\nconference         -1.1    0.4  -1.7  -1.1  -0.6\ncharSemicolon      -0.3    0.1  -0.5  -0.3  -0.2\ncharRoundbracket   -0.1    0.1  -0.2  -0.1   0.0\ncharSquarebracket  -0.1    0.1  -0.3  -0.1   0.0\ncharExclamation     0.2    0.1   0.2   0.2   0.3\ncharDollar          1.2    0.2   1.0   1.3   1.5\ncharHash            0.7    0.4   0.2   0.7   1.2\ncapitalAve          0.0    0.3  -0.3   0.0   0.4\ncapitalLong         0.9    0.4   0.4   0.8   1.3\ncapitalTotal        0.7    0.1   0.5   0.7   0.8\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.4    0.0  0.4   0.4   0.4  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                  mcse Rhat n_eff\n(Intercept)       0.0  1.0   740 \nmake              0.0  1.0  1161 \naddress           0.0  1.0  1156 \nall               0.0  1.0   907 \nnum3d             0.0  1.0   958 \nour               0.0  1.0   973 \nover              0.0  1.0  1022 \nremove            0.0  1.0   909 \ninternet          0.0  1.0  1188 \norder             0.0  1.0  1064 \nmail              0.0  1.0  1022 \nreceive           0.0  1.0  1171 \nwill              0.0  1.0  1022 \npeople            0.0  1.0  1307 \nreport            0.0  1.0   935 \naddresses         0.0  1.0  1115 \nfree              0.0  1.0   830 \nbusiness          0.0  1.0  1059 \nemail             0.0  1.0   895 \nyou               0.0  1.0  1026 \ncredit            0.0  1.0  1328 \nyour              0.0  1.0  1077 \nfont              0.0  1.0  1226 \nnum000            0.0  1.0  1124 \nmoney             0.0  1.0   910 \nhp                0.0  1.0  1161 \nhpl               0.0  1.0  1422 \ngeorge            0.1  1.0   779 \nnum650            0.0  1.0  1017 \nlab               0.0  1.0  1477 \nlabs              0.0  1.0  1317 \ntelnet            0.0  1.0  1002 \nnum857            0.0  1.0  1388 \ndata              0.0  1.0  1265 \nnum415            0.0  1.0  1427 \nnum85             0.0  1.0  1247 \ntechnology        0.0  1.0  1021 \nnum1999           0.0  1.0   834 \nparts             0.0  1.0  1227 \npm                0.0  1.0  1135 \ndirect            0.0  1.0   793 \ncs                0.0  1.0  1293 \nmeeting           0.0  1.0  1547 \noriginal          0.0  1.0  1805 \nproject           0.0  1.0  1432 \nre                0.0  1.0  1027 \nedu               0.0  1.0  1048 \ntable             0.0  1.0  1082 \nconference        0.0  1.0  1492 \ncharSemicolon     0.0  1.0  1308 \ncharRoundbracket  0.0  1.0   906 \ncharSquarebracket 0.0  1.0  1286 \ncharExclamation   0.0  1.0  1070 \ncharDollar        0.0  1.0  1147 \ncharHash          0.0  1.0  1111 \ncapitalAve        0.0  1.0  1165 \ncapitalLong       0.0  1.0  1421 \ncapitalTotal      0.0  1.0  1312 \nmean_PPD          0.0  1.0   909 \nlog-posterior     0.5  1.0   338 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\n\n\n\nprior_summary(fit1)\n\nPriors for model 'fit1' \n------\nIntercept (after predictors centered)\n ~ normal(location = 0, scale = 10)\n\nCoefficients\n ~ laplace(location = [0,0,0,...], scale = [0.5,0.5,0.5,...])\n------\nSee help('prior_summary.stanreg') for more details\n\n\n\n\n\nbetaNames = names(spam_train)[2:7]\nbetaNames\n\n[1] \"make\"    \"address\" \"all\"     \"num3d\"   \"our\"     \"over\"   \n\nmcmc_trace(fit1, pars = betaNames)\n\n\n\n\n\n\n\nbetaNames = names(spam_train)[2:7]\nbetaNames\n\n[1] \"make\"    \"address\" \"all\"     \"num3d\"   \"our\"     \"over\"   \n\nmcmc_hist(fit1, pars = c(betaNames))\n\n\n\n\n\n\nTo plot specific parameters, use the arguemnt pars, e.g.\n\nmcmc_trace(fit1, pars = c(\"internet\", \"george\")\nmcmc_hist(fit1, pars = \"make\")\n\nTo read more about bayesplot functionality, see https://mc-stan.org/bayesplot/articles/plotting-mcmc-draws.html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchain_draws = as_draws(fit1)\nchain_draws$george[1:5] # first 5 samples of the first chain run by stan\n\n[1]  -8.657706  -8.058416  -7.542682  -5.875524 -10.805427\n\n\n\ntry the following command: View(chain_draws)\n\n\n\nReport posterior mean, posterior median and 90% posterior CI.\n\nposteriorMean = apply(chain_draws, 2, mean)\nposteriorMedian = fit1$coefficients\nposteriorCI = posterior_interval(fit1, prob = 0.9)\ncbind(posteriorMean, posteriorMedian, posteriorCI)\n\n                  posteriorMean posteriorMedian            5%          95%\n(Intercept)         -3.92819676     -3.89842329  -4.688486640 -3.214996079\nmake                -0.10511093     -0.10291579  -0.234925630  0.007473911\naddress             -0.23994990     -0.22496984  -0.459838123 -0.079085239\nall                  0.06181680      0.06204777  -0.032348802  0.162371165\nnum3d                0.86239227      0.69068831   0.094477008  2.262920466\nour                  0.36326097      0.36517847   0.252660614  0.482222431\nover                 0.22388788      0.22526813   0.104903413  0.345376589\nremove               0.89046939      0.88519560   0.660560624  1.124715173\ninternet             0.19219955      0.18873972   0.084482068  0.313373050\norder                0.14088547      0.14175852   0.022540540  0.262790934\nmail                 0.07091792      0.07049483  -0.004812217  0.148380741\nreceive             -0.04641076     -0.04704090  -0.155648064  0.056349226\nwill                -0.19669579     -0.19506749  -0.323302246 -0.079179801\npeople              -0.05080340     -0.04939546  -0.170276558  0.067469609\nreport              -0.01152332     -0.01105396  -0.102076144  0.078218538\naddresses            0.27381609      0.26410052   0.016663808  0.554382803\nfree                 0.95909363      0.95496752   0.738679361  1.184988932\nbusiness             0.46684266      0.46497563   0.276894271  0.663670058\nemail                0.10488455      0.10024984   0.001909845  0.222419640\nyou                  0.10146793      0.10210904  -0.005066755  0.212057793\ncredit               0.60260737      0.58770051   0.245466733  1.011651069\nyour                 0.29510195      0.29496631   0.185160138  0.408134909\nfont                 0.23712284      0.22816521   0.014405002  0.514370136\nnum000               0.71052738      0.69647322   0.470113449  0.973663603\nmoney                0.20621767      0.19864778   0.101091758  0.340661983\nhp                  -3.12311740     -3.10880324  -4.016671505 -2.304686191\nhpl                 -1.03485238     -1.03298227  -1.697441864 -0.401214421\ngeorge              -8.54020532     -8.47338329 -11.651960267 -5.719147532\nnum650               0.23079106      0.22722517   0.066179720  0.417323354\nlab                 -1.06217022     -0.98480464  -2.007243002 -0.334531315\nlabs                -0.12552567     -0.11162875  -0.398538494  0.091786582\ntelnet              -0.34336793     -0.26439840  -0.971412117  0.026123689\nnum857              -0.24559769     -0.15437261  -1.410789083  0.608907729\ndata                -0.73133589     -0.71699338  -1.108487928 -0.382954624\nnum415              -0.64101304     -0.47534569  -2.093090996  0.285482781\nnum85               -0.79569146     -0.76847847  -1.439452875 -0.215331103\ntechnology           0.39271940      0.38676108   0.173703662  0.625821033\nnum1999             -0.01626319     -0.01501346  -0.146351106  0.113311375\nparts               -0.19387536     -0.17594695  -0.420298806 -0.034525634\npm                  -0.36307678     -0.35286882  -0.654982493 -0.099008434\ndirect              -0.16079193     -0.14842066  -0.405108058  0.023707001\ncs                  -1.35347826     -1.21771081  -2.748096259 -0.377142268\nmeeting             -1.54394154     -1.48634297  -2.451147222 -0.853955898\noriginal            -0.44492327     -0.43068267  -0.889558420 -0.068304526\nproject             -1.27091449     -1.25700373  -1.901896255 -0.691331691\nre                  -0.68897480     -0.68667471  -0.941480034 -0.439694059\nedu                 -1.52342346     -1.51146322  -2.056088445 -1.041884151\ntable               -0.27888780     -0.25736694  -0.549891396 -0.077155657\nconference          -1.10733935     -1.06265166  -1.864536810 -0.460671052\ncharSemicolon       -0.32588657     -0.31416332  -0.514101364 -0.166688438\ncharRoundbracket    -0.08698799     -0.08292353  -0.212643020  0.023441788\ncharSquarebracket   -0.14980893     -0.13280261  -0.342135625 -0.004178006\ncharExclamation      0.23238206      0.22752584   0.137507390  0.351658871\ncharDollar           1.24928427      1.25351495   0.944702156  1.558125382\ncharHash             0.71795213      0.70213467   0.122922001  1.368832748\ncapitalAve          -0.01033156     -0.03262611  -0.401092859  0.470713740\ncapitalLong          0.85197631      0.82214223   0.303300262  1.493571408\ncapitalTotal         0.67337732      0.67245181   0.471182694  0.893198052"
  },
  {
    "objectID": "slides/lab8-rstan.html#exercise-1-solution",
    "href": "slides/lab8-rstan.html#exercise-1-solution",
    "title": "Easy Bayesian linear modeling",
    "section": "Exercise 1 solution",
    "text": "Exercise 1 solution\n\nfit1 = stan_glm(type ~ ., data = spam_train,\n                 family = binomial(link = \"logit\"),\n                 prior = laplace(0, 0.5),\n                 prior_intercept = normal(0, 10),\n                 cores = 2, seed = 360,\n                chains = 1, iter = 2000)"
  },
  {
    "objectID": "slides/lab8-rstan.html#exercise-3",
    "href": "slides/lab8-rstan.html#exercise-3",
    "title": "Easy Bayesian linear modeling",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nTest your spam filter on the spam_test data set.\nMake a table showing correct and incorrect number of classifications."
  },
  {
    "objectID": "slides/lab8-rstan.html#exercise-2-solution",
    "href": "slides/lab8-rstan.html#exercise-2-solution",
    "title": "Easy Bayesian linear modeling",
    "section": "Exercise 2 Solution",
    "text": "Exercise 2 Solution\n\nTrace plots look good, can look through more by subsetting others.\nESS is high\nreceive, report, num1999, capitalAve"
  },
  {
    "objectID": "slides/lab8-rstan.html#exercise-3-solution",
    "href": "slides/lab8-rstan.html#exercise-3-solution",
    "title": "Easy Bayesian linear modeling",
    "section": "Exercise 3 Solution",
    "text": "Exercise 3 Solution\n\npredicted_spam = posterior_predict(object = fit1, newdata = spam_test)\n\ndata.frame(y = spam_test$type, \n           yhat = predict(object = fit1, newdata = spam_test[,-1], type = \"response\")) %>%\n  mutate(yhat = ifelse(yhat >= 0.5, 1, 0)) %>%\n  count(y, yhat)\n\n  y yhat   n\n1 0    0 529\n2 0    1  26\n3 1    0  31\n4 1    1 335\n\n\n856/921 classifications correct with a cutoff of 0.5.\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "notes/waterOnMars.html#parameter-table",
    "href": "notes/waterOnMars.html#parameter-table",
    "title": "Water on Mars",
    "section": "Parameter Table",
    "text": "Parameter Table\n\n\n\n\\(\\alpha\\)\nPore shape aspect ratio\n\n\n\\(\\phi\\)\nPorosity\n\n\n\\(\\gamma_w\\)\nWater saturation (%)\n\n\n\\(\\kappa_m\\)\nMineral bulk modulus (GPa)\n\n\n\\(\\mu_m\\)\nMineral shear modulus (GPa)\n\n\n\\(\\rho_m\\)\nMineral density (kg/m\\(^3\\))\n\n\n\n\n# Berryman‚Äôs rock physics model \nVp = function(ke, mue, pb) {\n  sqrt((ke + ((4/3) * mue)) / pb)\n}\n\nVs = function(mue, pb) {\n  sqrt(mue/pb)\n}\n\ngetParameters = function(alpha, phi, gw, km, mum, rhom)\n\n\nberry_scm <- function(k, mu, asp, x, ro1, P_water) {\n  # BERRYSCM - Effective elastic moduli for multi-component composite\n  # using Berryman's Self-Consistent (Coherent Potential Approximation) method.\n  # \n  # Arguments:\n  # k: Bulk moduli of the N constituent phases (numeric vector)\n  # mu: Shear moduli of the N constituent phases (numeric vector)\n  # asp: Aspect ratio for the inclusions of the N phases (numeric vector)\n  # x: Fraction of each phase. Solid, then fluid phase (numeric vector)\n  # ro1: Density of the original rock (numeric scalar)\n  # P_water: Proportion of water (numeric scalar)\n  #\n  # Returns:\n  # kbr: Effective bulk modulus (numeric scalar)\n  # mubr: Effective shear modulus (numeric scalar)\n  # vp: P-wave velocity (numeric scalar)\n  # vs: S-wave velocity (numeric scalar)\n  # ro2: Final rock density after fluid substitution (numeric scalar)\n  # k2: Rock bulk modulus after fluid substitution (numeric scalar)\n  \n  # Ensure inputs are column vectors (equivalent to MATLAB's k(:), mu(:), etc.)\n  k <- as.vector(k)\n  mu <- as.vector(mu)\n  asp <- as.vector(asp)\n  x <- as.vector(x)\n  \n  # Modify aspect ratios of inclusions that are equal to 1\n  asp[asp == 1] <- 0.99\n  \n  # Initialize variables\n  theta <- numeric(length(asp))\n  fn <- numeric(length(asp))\n  \n  # Compute theta and fn for oblate and prolate spheroids\n  obdx <- which(asp < 1)\n  prdx <- which(asp > 1)\n  \n  theta[obdx] <- (asp[obdx] / ((1 - asp[obdx]^2)^(3/2))) * (acos(asp[obdx]) - asp[obdx] * sqrt(1 - asp[obdx]^2))\n  fn[obdx] <- (asp[obdx]^2 / (1 - asp[obdx]^2)) * (3 * theta[obdx] - 2)\n  \n  theta[prdx] <- (asp[prdx] / ((asp[prdx]^2 - 1)^(3/2))) * (asp[prdx] * sqrt(asp[prdx]^2 - 1) - acosh(asp[prdx]))\n  fn[prdx] <- (asp[prdx]^2 / (asp[prdx]^2 - 1)) * (2 - 3 * theta[prdx])\n  \n  # Initialize initial bulk and shear moduli\n  ksc <- sum(k * x)\n  musc <- sum(mu * x)\n  \n  # Initialize iteration parameters\n  knew <- 0\n  tol <- 1e-6 * k[1]\n  del <- abs(ksc - knew)\n  niter <- 0\n  \n  # Iterative solution for effective moduli\n  while (del > abs(tol) && niter < 3000) {\n    nusc <- (3 * ksc - 2 * musc) / (2 * (3 * ksc + musc))\n    a <- mu / musc - 1\n    b <- (1 / 3) * (k / ksc - mu / musc)\n    r <- (1 - 2 * nusc) / (2 * (1 - nusc))\n    \n    f1 <- 1 + a * ((3 / 2) * (fn + theta) - r * ((3 / 2) * fn + (5 / 2) * theta - (4 / 3)))\n    f2 <- 1 + a * (1 + (3 / 2) * (fn + theta) - (r / 2) * (3 * fn + 5 * theta)) + b * (3 - 4 * r)\n    f2 <- f2 + (a / 2) * (a + 3 * b) * (3 - 4 * r) * (fn + theta - r * (fn - theta + 2 * theta^2))\n    f3 <- 1 + a * (1 - (fn + (3 / 2) * theta) + r * (fn + theta))\n    f4 <- 1 + (a / 4) * (fn + 3 * theta - r * (fn - theta))\n    f5 <- a * (-fn + r * (fn + theta - (4 / 3))) + b * theta * (3 - 4 * r)\n    f6 <- 1 + a * (1 + fn - r * (fn + theta)) + b * (1 - theta) * (3 - 4 * r)\n    f7 <- 2 + (a / 4) * (3 * fn + 9 * theta - r * (3 * fn + 5 * theta)) + b * theta * (3 - 4 * r)\n    f8 <- a * (1 - 2 * r + (fn / 2) * (r - 1) + (theta / 2) * (5 * r - 3)) + b * (1 - theta) * (3 - 4 * r)\n    f9 <- a * ((r - 1) * fn - r * theta) + b * theta * (3 - 4 * r)\n    \n    p <- 3 * f1 / f2\n    q <- (2 / f3) + (1 / f4) + ((f4 * f5 + f6 * f7 - f8 * f9) / (f2 * f4))\n    \n    p <- p / 3\n    q <- q / 5\n    \n    # Update moduli\n    knew <- sum(x * k * p) / sum(x * p)\n    munew <- sum(x * mu * q) / sum(x * q)\n    \n    del <- abs(ksc - knew)\n    ksc <- knew\n    musc <- munew\n    niter <- niter + 1\n  }\n  \n  kbr <- ksc\n  mubr <- musc\n  \n  # Density and fluid substitution\n  rofl1 <- 0.020    # density of gas\n  kfl1 <- 0         # bulk modulus of gas\n  ro_water <- 1000   # density of water\n  k_water <- 2.2e9  # bulk modulus of water\n  P_gas <- 1 - P_water\n  rofl2 <- P_gas * rofl1 + P_water * ro_water\n  kfl2 <- P_gas * kfl1 + P_water * k_water\n  k0 <- k[1]        # bulk modulus of solid mineral phase\n  phi <- x[2]       # porosity of rock\n  k1 <- kbr         # dry bulk modulus\n  \n  # Perform fluid substitution using Gassmann equation\n  a <- k1 / (k0 - k1) - kfl1 / (phi * (k0 - kfl1)) + kfl2 / (phi * (k0 - kfl2))\n  k2 <- k0 * a / (1 + a)   # bulk modulus after fluid substitution\n  \n  # Compute final density after fluid substitution\n  ro2 <- ro1 - phi * rofl1 + phi * rofl2\n  \n  # Compute seismic velocities after fluid substitution\n  mu2 <- mubr\n  vp <- sqrt((k2 + (4 / 3) * mu2) / ro2)\n  vs <- sqrt(mu2 / ro2)\n  \n  return(list(kbr = kbr, mubr = mubr, vp = vp, vs = vs, ro2 = ro2, k2 = k2))\n}\n\nmyBerry <- function(theta, H = 3) {\n  # Extract parameters from theta\n  asp <- c(1, theta[1])   # Aspect ratio\n  x_phi <- theta[2]       # Proportion of fluid (phi)\n  rock_vol <- 1 - theta[2] # Volume of solid phase (rock)\n  x <- c(rock_vol, x_phi)  # Fraction of phases\n  rock_density <- theta[6] * rock_vol  # Density of solid phase (basalt)\n  gas_density <- 0.020 * x_phi         # Density of fluid phase (gas)\n  rhob1 <- rock_density + gas_density  # Bulk density\n  \n  # Percentage of water in pore space\n  P_water <- theta[3]\n  \n  # Bulk and shear moduli (scaled by 1e9 as per MATLAB code)\n  k <- c(theta[4] * 1e9, 0)  # Bulk modulus (first element)\n  mu <- c(theta[5] * 1e9, 0) # Shear modulus (first element)\n  \n  # Call berryscm function (ensure berryscm function is defined in your R environment)\n  result <- berry_scm(k, mu, asp, x, rhob1, P_water)\n  \n  # Return the required output from berryscm\n  return(result)\n}\n\n\n# myBerry <- function(theta, H = 3) {\n#   asp <- c(1, theta[1])\n#   x_phi <- theta[2]\n#   rock_vol <- 1 - theta[2]\n#   x <- c(rock_vol, x_phi)\n#   \n#   rock_density <- theta[6] * rock_vol  # density of solid phase\n#   gas_density <- 0.020 * x_phi         # density of fluid phase\n#   rhob1 <- rock_density + gas_density   # bulk density\n#   P_water <- theta[3]                   # percentage of water in pore space\n#   k <- c(theta[4] * 1e9, 0)\n#   mu <- c(theta[5] * 1e9, 0)\n# \n#   result <- berry_scm(k, mu, asp, x)\n#   return(c(result, \"rhob\" = rhob1))\n# }"
  },
  {
    "objectID": "notes/waterOnMars.html#code",
    "href": "notes/waterOnMars.html#code",
    "title": "Water on Mars",
    "section": "Code",
    "text": "Code\n\nset.seed(360)\ndtruncnormL = function(x, a, b, mean, sd) {\n  log(dtruncnorm(x, a=a, b=b, mean = mean, sd = sd))\n}\n\nrcnorm<-function(n, mean=0, sd=1, a=-Inf, b=Inf){\n  u = runif(n, pnorm((a - mean) / sd), pnorm((b - mean) / sd))\n  mean + (sd * qnorm(u))\n}\n\nlogPosterior = function(theta) {\n  result = myBerry(theta)\n  y = c(result$vp / 1000, result$vs / 1000, result$ro2)\n  if(result$vp < result$vs) {\n    return(-Inf)\n  }\n  else{\n  return(\n    sum(dnorm(y, mean = seismic_data$y, sd = seismic_data$sd,\n            log = TRUE))\n  )\n  }\n}\n\n# THETA[1] = alpha\n# THETA[2] = porosity\n# THETA[3] = saturation\n# THETA[4] = kappa_m\n# THETA[5] = mu_m\n# THETA[6] = rho_m\n\n# starting point\nalpha = 0.5\nporosity = 0.25 #runif(1, 0.05, 0.50)\nsaturation = 0.05 #0 to 1\nkappa_m = 78\nmu_m = 33\nrhom = 3000\ntheta = c(alpha, porosity, saturation, kappa_m, mu_m, rhom)\n\n# prior\n## indicator that parameters are in proper ranges * constant * indicator that Vp > Vs\n\n# MCMC \nS = 50000\naccept = rep(0, length(theta))\n\nTHETA = NULL\nfor(i in 1:S) {\n  thetaStar = theta \n  thetaStar[1] = rcnorm(1, mean = theta[1], sd = .05,\n                        a = 0.03, b = 0.99)\n   log.r = logPosterior(thetaStar) + \n     dtruncnormL(theta[1], \n                a=0.03, b=0.99, \n                mean = thetaStar[1], sd = .05) - \n     logPosterior(theta) - \n     dtruncnormL(thetaStar[1], \n                a=0.03, b=0.99, \n                mean = theta[1], sd = .05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[1] = accept[1] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[2] = rcnorm(1, mean = theta[2], sd = .05,\n                        a = 0.05, b = 0.5)\n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[2], \n                a=0.05, b=0.5, \n                mean = theta[2], sd = .05) - \n     logPosterior(theta) - \n      dtruncnormL(thetaStar[2], \n                a=0.05, b=0.5, \n                mean = theta[2], sd = .05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[2] = accept[2] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[3] = rcnorm(1, mean = theta[3], sd = 0.05,\n                        a = 0, b = 1)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[3], \n                a=0, b=1, \n                mean = theta[3], sd = 0.05) - \n     logPosterior(theta) - \n    dtruncnormL(thetaStar[3], \n                a=0, b=1, \n                mean = theta[3], sd = 0.05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[3] = accept[3] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[4] = rcnorm(1, mean = theta[4], sd = .5,\n                        a = 76.5, b = 80)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[4], \n                a=76.5, b=80, \n                mean = theta[4], sd = .5) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[4], \n                a=76.5, b=80, \n                mean = theta[4], sd = .5)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[4] = accept[4] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[5] = rcnorm(1, mean = theta[5], sd = 1,\n                        a = 25.6, b = 40)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[5], \n                a=25.6, b=40, \n                mean = theta[5], sd = 1) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[5], \n                a=25.6, b=40, \n                mean = theta[5], sd = 1)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[5] = accept[5] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[6] = rcnorm(1, mean = theta[6], sd = 20,\n                        a = 2689, b = 2900)\n  \n   log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[6], \n                a=2689, b=2900, \n                mean = theta[6], sd = 20) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[6], \n                a=2689, b=2900, \n                mean = theta[6], sd = 20)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[6] = accept[6] + 1 \n   }\n  \n   THETA = rbind(THETA, theta)\n}"
  },
  {
    "objectID": "notes/lec14-estimators.html",
    "href": "notes/lec14-estimators.html",
    "title": "Estimators",
    "section": "",
    "text": "Definition\n\n\n\nA point estimator of an unknown parameter \\(\\theta\\) is a function that converts data into a single element of parameter space \\(\\Theta\\).\n\n\nExample: Let \\(Y_1, \\ldots, Y_n | \\theta, \\sigma^2 \\sim \\text{iid }~N(\\theta, \\sigma^2)\\). Further assume some prior \\(p(\\theta, \\sigma^2)\\). The following are each point estimators of \\(\\theta\\):\n\n\\(\\bar{y}\\)\n\\(y_1\\)\n\\(\\frac{y_1 + y_2}{2}\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is convention to write estimators the same way we write the parameter but with a ‚Äúhat‚Äù. For example, \\(\\theta\\) is the parameter and \\(\\hat{\\theta}\\) is the estimator.\n\n\nSampling properties of a point estimator refer to the estimator‚Äôs behavior under hypothetical repeatable surveys or experiments.\nThree common sampling properties of estimators we will see again and again are:\n\nbias\nvariance\nmean squared error (MSE)\n\n\n\nBefore we discuss bias, variance and mean squared error of an estimator, it‚Äôs important to understand that an estimator is a statistic (function of the data) and therefore a random variable. Because of this, estimator‚Äôs have a sampling distribution.\n\nExercise 1\n\n\nWhat does the example below show? What is x?\n\n\n\n\nset.seed(360)\n\nx = vector()\nfor (i in 1:100) {\n  y = rnorm(10)\n  x = append(min(y), x)\n}\nhist(x, freq = FALSE)\nabline(v = mean(x), col= \"steelblue\", lwd = 4)\n\n\n\ncat(\"The variance of x is \", round(var(x), 3))\n\nThe variance of x is  0.291\n\n\n\nExercise 2\n\n\nImagine \\(\\hat{\\theta}_a\\) and \\(\\hat{\\theta}_b\\) are two different estimators of \\(\\theta\\). The true value of \\(\\theta\\) is \\(\\theta_0 = 0\\). The sampling distributions of the two estimators are given below. Which estimator do you prefer?\n\n\n\n\n\n\n\n\n\n\n\nIn the rest of these notes, let \\(\\theta_0\\) be the true value of the population parameter \\(\\theta\\).\n\n\n\n\n\n\nDefinition\n\n\n\nBias is the the difference between the expected value of the estimator and the true value of the parameter.\n\n\\(E[\\hat{\\theta} | \\theta = \\theta_ 0] - \\theta_0\\) is the bias of \\(\\hat{\\theta}\\).\nIf \\(E[\\hat{\\theta} | \\theta = \\theta_0] = \\theta_0\\), then we say \\(\\hat{\\theta}\\) is an unbiased estimator of \\(\\theta\\).\nIf \\(E[\\hat{\\theta} | \\theta = \\theta_0] \\neq \\theta_0\\), then we say \\(\\hat{\\theta}\\) is a biased estimator of \\(\\theta\\).\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nRecall: variance is average squared distance from the mean. In this context, the variance of an estimator refers to the variance of the sampling distribution of \\(\\hat{\\theta}\\). We write this mathematically,\n\\[\nVar[\\hat{\\theta} | \\theta_0] = E[(\\hat{\\theta} - m)^2 |\\theta_0]\n\\]\nwhere \\(m = E[\\hat{\\theta}|\\theta_0]\\).\n\n\nWhile it may seem desirable to have an estimator with zero bias, the estimator may still be far away from the true parameter value if the variance is too large. The mean squared error quantifies how close an estimator is to the true parameter value.\n\n\n\n\n\n\nDefinition\n\n\n\nMean squared error (MSE) is (as the name suggests) the expected value of the squared difference between the estimator and true parameter value. Equivalently, MSE is the variance plus the square bias of the estimator.\n\\[\n\\begin{aligned}\nMSE[\\hat{\\theta}|\\theta_0] &= E[(\\hat{\\theta} - \\theta_0)^2 | \\theta_0]\\\\\n&= Var[\\hat{\\theta} | \\theta_0] + Bias^2[\\hat{\\theta}|\\theta_0]\n\\end{aligned}\n\\]\n\n\n\nLet‚Äôs show this offline."
  },
  {
    "objectID": "notes/lec14-estimators.html#practice",
    "href": "notes/lec14-estimators.html#practice",
    "title": "Estimators",
    "section": "Practice",
    "text": "Practice\nSuppose you wish to make inference about the average bill length of Chinstrap penguins.\nYou make the modeling assumption that \\(Y\\), the bill length of a penguin is normally distributed, i.e.¬†\\(Y \\sim N(\\theta, \\sigma^2)\\) and you set up a conjugate prior as we‚Äôve done before.\nOne can then show that the posterior mean estimator of \\(\\theta\\) is\n\\[\n\\hat{\\theta}_b = E[\\theta | y_1,\\ldots y_n] = \\frac{n}{\\kappa_0 + n} \\bar{y} + \\frac{\\kappa_0}{\\kappa_0 + n} \\mu_0 = w\\bar{y} + (1-w) \\mu_0\n\\]\n\nExercise 3\n\n\nCompare \\(\\hat{\\theta}_b\\) to the estimator \\(\\hat{\\theta}_e = \\bar{y}\\). Compute the expected value of each estimator, which one is biased? Compute the variance of each estimator. Which has lower variance?\n\n\n\n\nLet‚Äôs compute the MSE and discuss when the Bayesian estimator \\(\\hat{\\theta}_b\\) has lower MSE than the sample mean offline."
  },
  {
    "objectID": "notes/lec14-estimators.html#extra-practice",
    "href": "notes/lec14-estimators.html#extra-practice",
    "title": "Estimators",
    "section": "Extra practice",
    "text": "Extra practice\n\n\n\n\n\n\nExercise 4plotcode\n\n\nSuppose you know that you know Gentoo penguins are closely related to Chinstrap penguins. Previously, you‚Äôve measured the bill length of three Gentoo penguins and found their mean bill length to be 46.2. Accordingly, you set \\(\\mu_0 = 46.2\\).\n\n\n\n\n\n\n\n\nSuppose (for illustrative purposes) that you know the true population mean and variance for Chinstrap penguin bill length,\n\\[\n\\begin{aligned}\n\\theta_0 &= 48.8\\\\\n\\sigma^2 &= 3.3.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nCompute \\(MSE[\\hat{\\theta}_e|\\theta_0]\\) and \\(MSE[\\hat{\\theta_b}|\\theta_0]\\) and plot the ratio \\(MSE[\\hat{\\theta}_b]/MSE[\\hat{\\theta}_e|\\theta_0]\\) as a function of \\(n\\) for \\(\\kappa_0 = 0, 1, 2, 3\\).\n\n\n\n\n\n\n\n\n\n\n\n# MSE of the sample mean = sigma^2 / n \nsigma2 = 3.3 \nMSE_empirical = function(n) {\n  sigma2 / n\n}\n\nMSE_bayesian = function(k0, n) {\n  w = n / (k0 + n)\n  (w^2 * sigma2 / n) + (((1-w) * (46.2 - 48.8))^2)\n}\n\nMSE_ratio = function(k0, n) {\n  MSE_bayesian(k0, n) / MSE_empirical(n)\n}\n\ndata.frame(x = 1:100) %>%\n  ggplot(aes(x = x)) +\n  stat_function(aes(col = '0'), alpha = 0.5, fun = MSE_ratio,\n                args = list(k0 = 0)) +\n  stat_function(aes(col = '1'), alpha = 0.5, fun = MSE_ratio,\n                args = list(k0 = 1)) +\n  stat_function(aes(col = '2'), alpha = 0.5, fun = MSE_ratio,\n                args = list(k0 = 2)) +\n  stat_function(aes(col = '3'), alpha = 0.5, fun = MSE_ratio,\n                args = list(k0 = 3)) +\n  labs(x = \"n\", y = \"MSE ratio\", col = \"k0\") +\n  theme_bw()\n\n\n\n\n\nExercise 5\n\n\nConsider the normal model from the previous lecture. Compare the MSE for the least squares estimator \\(\\hat{\\beta}_{OLS}\\) to the MSE for the ridge estimator \\(\\hat{\\beta}_{ridge}\\)."
  },
  {
    "objectID": "notes/lec15-hierarchical-intro.html",
    "href": "notes/lec15-hierarchical-intro.html",
    "title": "Hierarchical modeling",
    "section": "",
    "text": "view packages used in these notes\n# load packages\nlibrary(tidyverse)\nlibrary(coda)"
  },
  {
    "objectID": "notes/lec15-hierarchical-intro.html#questions-about-the-data",
    "href": "notes/lec15-hierarchical-intro.html#questions-about-the-data",
    "title": "Hierarchical modeling",
    "section": "Questions about the data",
    "text": "Questions about the data\n\nHow are the schools ranked?\nDoes school 51 have a higher average score than school 41?\nWhat is the probability a single student randomly selected from school 51 performs better on the exam than a single student randomly selected from school 41?"
  },
  {
    "objectID": "notes/lec15-hierarchical-intro.html#model",
    "href": "notes/lec15-hierarchical-intro.html#model",
    "title": "Hierarchical modeling",
    "section": "Model",
    "text": "Model\nSuppose students scores at school \\(j\\) are exchangeable for all \\(n_j\\). By de Finetti‚Äôs theorem, this means\n\\[\n\\{y_{1,j}, \\ldots y_{n_j,j} | \\phi_j \\} \\sim \\text{ i.i.d. } p(y|\\phi_j).\n\\]\nThat is, the student‚Äôs scores at school \\(j\\) are conditionally i.i.d. given some school specific parameters \\(\\phi_j\\). This describes our within-group sampling variability.\nNow suppose that all the schools we sampled are similar in some way. Maybe they belong to some larger population of schools across the country i.e.¬†schools in North Carolina are somewhat distinct from schools in South Carolina. We might imagine that the school-specific parameters themselves are exchangeable for all \\(m\\). By de Finetti‚Äôs theorem, this means\n\\[\n\\{\\phi_1, \\ldots \\phi_m\\} \\sim \\text{ i.i.d. } p(\\phi|\\psi).\n\\]\nIn words, school-specific parameters are conditionally i.i.d. given some population specific parameters \\(\\psi\\). This describes our between-group sampling variability.\nFinally, if our hierarchy stops there, then to complete model specification, we may describe our prior beliefs about \\(\\psi\\) according to some prior density \\(p(\\psi)\\).\n\nExerciseSolution\n\n\nImagine variability among scores is the same across all schools, but there does exist heterogeneity in the mean scores of the schools. Write down the mathematical form of a model that describes this using the normal distribution. What are some priors you could pick on relevant parameters to make sure full conditionals are easy to compute for Gibbs sampling? What are the full conditionals?\n\n\n\n\nsampling distributions:\n\n\\[\n\\begin{aligned}\ny_j | \\theta_j, \\sigma^2 &\\sim N(\\theta_j, \\sigma^2)\\\\\n\\theta_j | \\mu, \\tau^2 &\\sim N(\\mu, \\tau^2)\n\\end{aligned}\n\\]\n\npriors distributions:\n\n\\[\n\\begin{aligned}\n1/\\sigma^2 &\\sim \\text{ gamma}(\\nu_0/2, \\nu_0 \\sigma_0^2/2)\\\\\n1/\\tau^2 &\\sim \\text{ gamma}(\\eta_0/2, \\eta_0 \\tau_0^2/2)\\\\\n\\mu &\\sim N(\\mu_0, \\gamma_0^2)\n\\end{aligned}\n\\]\n\n\n\n\nTo facilitate Gibbs sampling, notice\n\\[\np(\\theta_1, \\ldots \\theta_m, \\mu, \\tau^2, \\sigma^2 | \\mathbf{y}_1, \\ldots \\mathbf{y}_m) \\propto\np(\\mu, \\tau^2, \\sigma^2) p(\\theta_1, \\ldots \\theta_m | \\mu, \\tau^2, \\sigma^2) \\times p(\\mathbf{y}_1, \\ldots \\mathbf{y}_m| \\theta_1, \\ldots \\theta_m, \\mu, \\tau^2, \\sigma^2)\n\\]\nIt follows that the full conditionals are:\n\\[\n\\begin{aligned}\np(\\mu | \\cdot) &\\propto p(\\mu) \\prod_{j = 1}^m p(\\theta_j| \\mu, \\tau^2)\\\\\np(\\tau^2 | \\cdot) &\\propto p(\\tau^2) \\prod_{j = 1}^m p(\\theta_j| \\mu, \\tau^2)\\\\\np(\\sigma^2|\\cdot) &\\propto p(\\sigma^2)\\prod_{j =1}^m \\prod_{i = 1}^{n_j} p(y_{i,j}|\\theta_j, \\sigma^2)\\\\\np(\\theta_j | \\cdot) &\\propto p(\\theta_j | \\mu, \\tau^2) \\prod_{i = 1}^{n_j} p(y_{i,j}|\\theta_j, \\sigma^2)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/lec15-hierarchical-intro.html#gibbs-sampling",
    "href": "notes/lec15-hierarchical-intro.html#gibbs-sampling",
    "title": "Hierarchical modeling",
    "section": "Gibbs sampling",
    "text": "Gibbs sampling\n\n#### MCMC approximation to posterior for the hierarchical normal model\n\n## weakly informative priors\nnu0 <- 1; s20 <- 100\neta0 <- 1; t20 <- 100\nmu0 <- 50; g20 <- 25\n\n## starting values\nm <- length(Y)\nn <- sv <- ybar <- rep(NA, m)\nfor (j in 1:m)\n{\n  ybar[j] <- mean(Y[[j]])\n  sv[j] <- var(Y[[j]])\n  n[j] <- length(Y[[j]])\n}\ntheta <- ybar\nsigma2 <- mean(sv)\nmu <- mean(theta)\ntau2 <- var(theta)\n\n## setup MCMC\nset.seed(1)\nS <- 5000\nTHETA <- matrix(nrow = S, ncol = m)\nMST <- matrix(nrow = S, ncol = 3)\npredictiveY = NULL\n\n## MCMC algorithm\nfor (s in 1:S)\n{\n  # sample new values of the thetas\n  for (j in 1:m)\n  {\n    vtheta <- 1 / (n[j] / sigma2 + 1 / tau2)\n    etheta <- vtheta * (ybar[j] * n[j] / sigma2 + mu / tau2)\n    theta[j] <- rnorm(1, etheta, sqrt(vtheta))\n  }\n  \n  #sample new value of sigma2\n  nun <- nu0 + sum(n)\n  ss <- nu0 * s20\n  for (j in 1:m) {\n    ss <- ss + sum((Y[[j]] - theta[j]) ^ 2)\n  }\n  sigma2 <- 1 / rgamma(1, nun / 2, ss / 2)\n  \n  #sample a new value of mu\n  vmu <- 1 / (m / tau2 + 1 / g20)\n  emu <- vmu * (m * mean(theta) / tau2 + mu0 / g20)\n  mu <- rnorm(1, emu, sqrt(vmu))\n  \n  # sample a new value of tau2\n  etam <- eta0 + m\n  ss <- eta0 * t20 + sum((theta - mu) ^ 2)\n  tau2 <- 1 / rgamma(1, etam / 2, ss / 2)\n  \n  #store results\n  THETA[s, ] <- theta\n  MST[s, ] <- c(mu, sigma2, tau2)\n  \n  # predictive sampling\n  y51 = rnorm(1, mean = theta[51], sd = sqrt(sigma2))\n  y41 = rnorm(1, mean = theta[41], sd = sqrt(sigma2))\n  predictiveY = rbind(predictiveY, c(y51, y41))\n  \n}\n\nmcmc1 <- list(THETA = THETA, MST = MST)"
  },
  {
    "objectID": "notes/lec15-hierarchical-intro.html#mcmc-diagnostics",
    "href": "notes/lec15-hierarchical-intro.html#mcmc-diagnostics",
    "title": "Hierarchical modeling",
    "section": "MCMC diagnostics",
    "text": "MCMC diagnostics\n\ntrace plots\n\nplotscode\n\n\n\n\n\n\n\n\n\n\ncolnames(MST) = c(\"mu\", \"sigma2\", \"tau2\")\nMST2 = MST %>%\n  as.data.frame() %>% \n  pivot_longer(cols = 1:3)\n\nMST2 %>%\n  ggplot(aes(x = seq(1, nrow(MST2)), y = value)) +\n  geom_line() +\n  theme_bw() +\n  facet_wrap(~ name, scales = \"free_y\") +\n  labs(y = \"mu\",\n       x = \"iteration\",\n       title = \"Traceplot of 5000 Gibbs samples\")\n\n\n\n\n\n\neffective sample size and autocorrelation\n\neffectiveSize(MST)\n\n      mu    sigma      tau \n3925.336 4461.112 2905.517 \n\npar(mfrow=c(1,3))\nacf(MST[,1])\nacf(MST[,2]) \nacf(MST[,3]) \n\n\n\n\n\n\nposterior means and standard error\n\n# MC error of mu, sigma2, tau2\nMCERR <- apply(MST,2,sd)/sqrt( effectiveSize(MST) )\napply(MST,2,mean)\n\n      mu    sigma      tau \n48.12530 84.82892 24.79410 \n\nMCERR\n\n         mu       sigma         tau \n0.008528321 0.041664073 0.082344432 \n\n\nWe can do the exact same for the thetas, but the output will be 100 lines, so I suppress output below.\n\n# MC error of thetas\neffectiveSize(THETA) -> esTHETA\nTMCERR <- apply(THETA,2,sd)/sqrt( effectiveSize(THETA) )\nTMCERR"
  },
  {
    "objectID": "notes/lec15-hierarchical-intro.html#answers",
    "href": "notes/lec15-hierarchical-intro.html#answers",
    "title": "Hierarchical modeling",
    "section": "Answers",
    "text": "Answers\n\nHow are the schools ranked? How does the ordering compare to just ranking the schools by the sample means?\n\n\n# Ordering E[theta | data] and comparing to ybar\n\nposteriorMean = THETA %>%\n  apply(2, mean)\n\norderedTable = mathScores %>%\n  group_by(school) %>%\n  summarize(ybar = mean(mathscore),\n            n = n()) %>%\n  cbind(posteriorMean) %>%\n  arrange(posteriorMean) %>%\n  relocate(school, n, ybar, posteriorMean) %>%\n  mutate_if(is.numeric, round, digits = 2)\n\nDT::datatable(\n  orderedTable,\n  fillContainer = FALSE, options = list(pageLength = 10)\n)\n\n\n\n\n\n\nHow many of the schools are ranked in the same position in the posterior ordering as the sample mean ordering?\n\noutputcode\n\n\n\n\n[1] 46\n\n\n\n\n\npostOrdering = posteriorMean %>%\n  order()\n\nybarOrdering = mathScores %>%\n  group_by(school) %>%\n  summarize(ybar = mean(mathscore), \n            n = n()) %>%\n  arrange(ybar) %>%\n  pull(school)\n\nsum(postOrdering == ybarOrdering)\n\n\n\n\n\nDoes school 51 have a higher average score than school 41? Re-cast as a Bayesian question: what‚Äôs \\(p(\\theta_{51} > \\theta_{41} | \\text{data})\\)?\n\n\nmean(THETA[,51] > THETA[,41])\n\n[1] 0.9892\n\n\n\nWhat‚Äôs the probability a student randomly selected from school 51 performs better than a student selected randomly from school 41?\n\nBefore looking at the solution below, how would you answer this problem?\n\n\nSolution\nmean(predictiveY[,1] > predictiveY[,2])\n\n# output:\n# [1] 0.685"
  },
  {
    "objectID": "slides/estimator-lab.html#exercise-1",
    "href": "slides/estimator-lab.html#exercise-1",
    "title": "Estimators",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet \\(Y_1,\\ldots Y_n\\) be iid random variables with expectation \\(\\theta\\) and variance \\(\\sigma^2\\).\nShow that \\(\\frac{1}{n} \\sum_{i = 1}^n (Y_i -\\bar{Y})^2\\) is a biased estimator of \\(\\sigma^2\\)."
  },
  {
    "objectID": "slides/estimator-lab.html#exercise-2",
    "href": "slides/estimator-lab.html#exercise-2",
    "title": "Estimators",
    "section": "Exercise 2",
    "text": "Exercise 2"
  },
  {
    "objectID": "slides/estimator-lab.html#solution-1",
    "href": "slides/estimator-lab.html#solution-1",
    "title": "Estimators",
    "section": "Solution 1",
    "text": "Solution 1\nLet \\(\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i = 1}^n (Y_i -\\bar{Y})^2\\).\n\\[\n\\begin{aligned}\nBias(\\hat{\\sigma}^2 | \\sigma^2 = \\sigma_0^2) &= E[\\hat{\\sigma}^2|\\sigma_0^2] - \\sigma_0^2\\\\\n&=  - \\sigma_0^2 + \\frac{1}{n} \\sum_{i = 1}^n E[(Y_i -\\bar{Y})^2|\\sigma_0^2]\\\\\n&= - \\sigma_0^2 + \\frac{1}{n} \\sum_{i=1}^n \\left[\nE[Y_i^2 |\\sigma_0^2] - 2E[Y_i \\bar{Y}|\\sigma_0^2] + E[\\bar{Y}^2 | \\sigma_0^2]\n\\right]\n\\end{aligned}\n\\]\nRecall that for any random variable X, \\(Var(X) = E[X^2] - E[X]^2\\). Using this fact, we continue our proof above:\n\\[\n\\begin{aligned}\n&= -\\sigma_0^2 +(\\sigma_0^2  + \\theta^2)\n-2 \\frac{1}{n} \\sum_{i=1}^n \\left[  E~\\left(Y_i \\frac{1}{n}\\sum_j Y_j\\right) | \\sigma_0^2    \\right]\n+ \\left(\\frac{\\sigma_0^2}{n} + \\theta^2\\right)\\\\\n&= 2\\theta^2 + \\frac{\\sigma_0^2}{n} - \\frac{2}{n}\n\\left(n\\theta^2  - \\sigma_0^2\n\\right)\\\\\n&= \\frac{(n-1)\\sigma_0^2}{n}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/estimator-lab.html#solution-2",
    "href": "slides/estimator-lab.html#solution-2",
    "title": "Estimators",
    "section": "Solution 2",
    "text": "Solution 2\n\\[\n\\begin{aligned}\n\\hat{\\theta}_{MLE} &= \\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i\\\\\n\\hat{\\theta}_B &= \\frac{n\\bar{y}+a}{n+a+b} =\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nMSE(\\hat{\\theta}_{MLE}|\\theta) &= \\frac{\\theta(1-\\theta)}{n}\\\\\nMSE(\\hat{\\theta}_B|\\theta) &= \\frac{n}{n + a + b}\\bar{Y} + \\frac{a + b}{n + a + b} \\frac{a}{a + b} =  w \\bar{Y} + (1-w)\\frac{a}{a+b}\\\\\n&= w^2Var(\\bar{Y} | \\theta) +  (1-w)^2 \\left(\\frac{a}{a+b} - \\theta\\right)^2\\\\\n&= {w^2} \\frac{\\theta(1-\\theta)}{n} + (1-w)^2  \\left(\\frac{a}{a+b} - \\theta\\right)^2\n\\end{aligned}\n\\]\nFor the Bayesian estimator to have smaller MSE than the MLE, we need\n\\[\n\\begin{aligned}\n\\left(\\frac{a}{a+b} - \\theta\\right)^2 &\\leq \\frac{\\theta(1 - \\theta)}{n} \\frac{1 + w}{1 - w}\\\\\n&\\leq \\frac{\\theta(1 - \\theta) (2n + a + b)}{n^2}\n\\end{aligned}\n\\]\nIn words, if our prior guess \\(a / (a+b)\\) is ‚Äúclose enough‚Äù to \\(\\theta\\), where ‚Äúclose enough‚Äù is defined by the inequality above and is proportional to the variance of the estimator, then the MSE of the Bayesian estimator is smaller.\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "hw/hw08.html",
    "href": "hw/hw08.html",
    "title": "Homework 8",
    "section": "",
    "text": "6.3 from Hoff. You can simulate from a constrained normal distribution with mean mean and standard deviation sd, constrained to lie in the interval \\((a,b)\\) using the following function:\n\nrcnorm<-function(n, mean=0, sd=1, a=-Inf, b=Inf){\n  u = runif(n, pnorm((a - mean) / sd), pnorm((b - mean) / sd))\n  mean + (sd * qnorm(u))\n}\n\nNote that you can use this function to simulate a vector of constrained normal random variables, each with a potentially different mean, standard deviation, and constraints.\nTo load the data for this exercise, run the code below\n\ndivorce = readr::read_csv(\"https://sta360-fa24.github.io/data/divorce.csv\")"
  },
  {
    "objectID": "hw/hw08.html#exercise-2",
    "href": "hw/hw08.html#exercise-2",
    "title": "Homework 8",
    "section": "Exercise 2",
    "text": "Exercise 2\nWeighted regression: Suppose \\(y_i \\sim N(\\beta x_i, \\sigma^2 / w_i)\\) independently for \\(i = 1,\\ldots n\\), where \\(x_1, \\ldots, x_n\\) and \\(w_1, \\ldots w_n\\) are known scalars, and \\(\\beta\\) and \\(\\sigma^2\\) are unknown.\n\nFind the formula for the OLS estimator \\(\\hat{\\beta}_{OLS}\\) and compute its variance \\(V[\\hat{\\beta}_{OLS} | \\beta, \\sigma^2]\\).\nWrite out the sampling density \\(p(y_1, \\ldots, y_n | \\sigma^2, \\beta)\\) as a function of \\(\\beta\\) (i.e.¬†the likelihood) and find the value of \\(\\beta\\) that maximizes this function (the MLE). Denote this maximizing value as \\(\\hat{\\beta}_{MLE}\\). Compute \\(V[\\hat{\\beta}_{MLE} | \\beta, \\sigma^2]\\) and compare it to that of \\(\\hat{\\beta}_{OLS}\\).\nUnder the prior distribution \\(\\beta \\sim N(0, \\tau^2)\\), find \\(E[\\beta | y_1, \\ldots, y_n, \\sigma^2]\\). What does this estimator get close to as the prior precision goes to zero (\\(\\tau^2 \\rightarrow \\infty\\))?"
  },
  {
    "objectID": "hw/hw08.html#exercise-3",
    "href": "hw/hw08.html#exercise-3",
    "title": "Homework 8",
    "section": "Exercise 3",
    "text": "Exercise 3\nHandwritten digit classification. Data originally sourced from U.S. postal envelopes.\nExercise inspired by Prof.¬†Hua Zhou‚Äôs Biostat M280.\n\n\n\n\n\nIn this exercise, you will build a very simple Bayesian image classifier. Load the training and test data sets using the code below.\n\nyTrain = readr::read_csv(\n  \"https://sta360-fa24.github.io/data/hw-digits-train.csv\")\n\nyTest = readr::read_csv(\n  \"https://sta360-fa24.github.io/data/hw-digits-test.csv\"\n)\n\nThe training data set contains 3822 images like the ones displayed above. Each image is a 32 x 32 bitmap, i.e.¬†1024 pixels, where each pixel is either black (0) or white (1). The 1024 pixels are divided into 64 blocks of 4 x 4 pixels. Each digit in the data set is represented by a vector of these blocks \\(\\mathbf{x} = (x_1, \\ldots, x_{64})\\) where each element is a count of the white pixels in a block (a number between 0 and 16).\nThe 65th column of the data set (id) is the actual digit label.\n\nHow many of each digit are in the training data set? Create a histogram to show the distribution of block10 white pixels for each digit. What do you notice?\nAssume each digit (i.e.¬†each id) has its own multinomial data generative model. You can read about the multinomial distribution using ?rmultinom in R.\n\n\nWrite down the joint density for images with id ‚Äúj‚Äù, \\(\\prod_{k = 1}^{n_j} p(\\mathbf{x}_k^{(j)} | \\boldsymbol{\\pi}^{(j)})\\). Here \\(n_j\\) is the number of images of type \\(j\\) and \\(\\boldsymbol{\\pi}^{(j)} = \\{\\pi_1^{(j)}, \\ldots, \\pi_{64}^{(j)} \\}\\)\nHow many total unknown parameters are in the complete joint density of all images?\n\n\nThe Dirichlet distribution is the multivariate generalization of the beta distribution. You can read more about it here. Place a Dirichlet prior on the probability parameters for each of your multinomial models in part b. Let the concentration parameters be all identically 1.\n\n\nSample from the posterior using any method you choose and compute the posterior mean \\(\\hat{\\boldsymbol{\\pi}}^{(j)}\\) for each \\(j\\). Hint: you may need to look up how to sample from a Dirichlet distribution in R. You may do this manually or find a package with built-in functions.\n\n\nFor each image \\(\\mathbf{x}\\) in your testing data set, compute your predicted id according to \\(\\text{argmax}_{j}~~p(\\mathbf{x}| \\boldsymbol{\\hat{\\pi}}^{(j)})p(j)\\), where \\(p(j)\\) is the proportion of digit \\(j\\) in the training set. Report the number of correct and incorrect classifications in your testing data set."
  },
  {
    "objectID": "slides/estimator-lab.html#exercise-1-estimators",
    "href": "slides/estimator-lab.html#exercise-1-estimators",
    "title": "Estimators",
    "section": "Exercise 1: estimators",
    "text": "Exercise 1: estimators\nLet \\(Y_1,\\ldots Y_n\\) be iid random variables with expectation \\(\\theta\\) and variance \\(\\sigma^2\\).\nShow that \\(\\frac{1}{n} \\sum_{i = 1}^n (Y_i -\\bar{Y})^2\\) is a biased estimator of \\(\\sigma^2\\)."
  },
  {
    "objectID": "slides/estimator-lab.html#exercise-2-estimators",
    "href": "slides/estimator-lab.html#exercise-2-estimators",
    "title": "Estimators",
    "section": "Exercise 2: estimators",
    "text": "Exercise 2: estimators\n\\[\n\\begin{aligned}\nY_1, \\ldots, Y_n &\\sim \\text{ i.i.d. binary}(\\theta)\\\\\n\\theta &\\sim \\text{beta}(a, b)\n\\end{aligned}\n\\]\n\nCompute \\(\\hat{\\theta}_{MLE}\\)\nCompute \\(\\hat{\\theta}_{B} = E[\\theta | y_1,\\ldots y_n]\\).\nCompare \\(MSE(\\hat{\\theta}_{MLE})\\) to \\(MSE(\\hat{\\theta}_{B})\\)). Under what conditions is the MSE of \\(\\hat{\\theta}_B\\) smaller?"
  },
  {
    "objectID": "slides/estimator-lab.html#exercise-3-mvn",
    "href": "slides/estimator-lab.html#exercise-3-mvn",
    "title": "Exam practice",
    "section": "Exercise 3: MVN",
    "text": "Exercise 3: MVN\nConsider a single observation \\((y_1, y_2)\\) drawn from a bivariate normal distribution with mean \\((\\theta_1, \\theta_2)\\) and fixed, known \\(2 \\times 2\\) covariance matrix \\(\\Sigma = \\left[ {\\begin{array}{cc}  1 & .5 \\\\  .5 & 1  \\end{array} } \\right]\\). Consider a uniform prior on \\(\\theta = (\\theta_1, \\theta_2)\\) : \\(p(\\theta_1, \\theta_2) \\propto 1\\).\n(a.) Derive the joint posterior for \\(\\theta_1, \\theta_2 | y_1, y_2, \\Sigma\\). Describe a direct sampler for this distribution.\n(b.) Write down full conditionals \\(p(\\theta_1 | \\theta_2, y_1, y_2, \\Sigma)\\) and \\(p(\\theta_2 | \\theta_1, y_1, y_2, \\Sigma)\\). Write pseudo-code to describe a Gibbs sampling procedure. Hint: you can use the result from HW6 Ex 3.\n(c.) Will the direct sampler from part (a) or the Gibbs sampler in part (b) have higher ESS? Why?"
  },
  {
    "objectID": "slides/estimator-lab.html#solution-3",
    "href": "slides/estimator-lab.html#solution-3",
    "title": "Exam practice",
    "section": "Solution 3",
    "text": "Solution 3\n\n\nüîó sta360-fa24.github.io"
  },
  {
    "objectID": "notes/lec15-hierarchical-intro.html#data-set-school-test-scores",
    "href": "notes/lec15-hierarchical-intro.html#data-set-school-test-scores",
    "title": "Hierarchical modeling",
    "section": "Data set: school test scores",
    "text": "Data set: school test scores\n\nExample from Hoff Ch. 8\n\nEach year, students across North Carolina take an identical standardized test. In our sample, we observe scores from students at \\(m\\) different schools. At the \\(j\\)th school, \\(n_j\\) students take the exam and \\(j \\in \\{1, \\ldots m\\}\\). The exam is designed to give an average score of 50 on a 0 to 100 scale.\n\n# load data\nmathScores = read_csv(\"https://sta360-fa24.github.io/data/mathScores.csv\")\n\n\nCodebook and glimpseSchool scoresAll mean scores\n\n\n\nschool: which school the math score came from\nmathscore: score from 0 to 100 of an individual student\n\n\n\nRows: 1,993\nColumns: 2\n$ school    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ mathscore <dbl> 52.11, 57.65, 66.44, 44.68, 40.57, 35.04, 50.71, 66.17, 39.4‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvert data to list for downstream processing\nY.school.mathscore <- as.matrix(mathScores)\n#### Put data into list form.\nY <- list()\nJ <- max(Y.school.mathscore[, 1])\nn <- ybar <- ymed <- s2 <- rep(0, J)\nfor (j in 1:J) {\n  Y[[j]] <- Y.school.mathscore[Y.school.mathscore[, 1] == j, 2]\n}"
  },
  {
    "objectID": "hw/hw08.html#exercise-4",
    "href": "hw/hw08.html#exercise-4",
    "title": "Homework 8",
    "section": "Exercise 4",
    "text": "Exercise 4"
  },
  {
    "objectID": "digit-recognition.html",
    "href": "digit-recognition.html",
    "title": "Handwritten digit recognition",
    "section": "",
    "text": "library(tidyverse)\nlibrary(extraDistr)\ninstall.packages(\"extraDistr\")"
  },
  {
    "objectID": "digit-recognition.html#data",
    "href": "digit-recognition.html#data",
    "title": "Handwritten digit recognition",
    "section": "Data",
    "text": "Data\nThere are 3822 images in the training data set. Each image is a 32 x 32 bitmap, i.e.¬†1024 pixels, where each pixel is either black (0) or white (1). The 1024 pixels are divided into 64 blocks of 4 x 4 pixels. Each digit is represented by a vector of these blocks \\(\\mathbf{x} = (x_1, \\ldots, x_{64})\\) where each element is a count between 0 and 16.\n\ny = read_csv(\"~/Downloads/optdigits.tra\")\ndim(y)\n\nThe 65th column of the data set is the digit label. Below we can see the number of each digit in our training set.\n\ny[,65] %>% table()\n\nLet \\(|\\mathbf{x}|\\) be the ‚Äúbatch size‚Äù or the total number of white pixels in an image.\n\nbatchSizes = rowSums(y)\nmean(batchSizes)\nsd(batchSizes)"
  },
  {
    "objectID": "digit-recognition.html#data-generative-model",
    "href": "digit-recognition.html#data-generative-model",
    "title": "Handwritten digit recognition",
    "section": "Data generative model",
    "text": "Data generative model\nLet \\(j\\) be the true digit the image depicts. We write the data generative model for an individual image of type \\(j\\),\n\\[\np(\\mathbf{x} | \\theta_1^{(j)}, \\ldots, \\theta^{(j)}_{64}) = \\frac{|\\mathbf{x}|!}{x_1!\\cdot x_2! \\cdots x_{64}!} \\prod_{i=1}^{64} \\left(\\theta_i^{(j)}\\right)^{x_i}\n\\]"
  },
  {
    "objectID": "digit-recognition.html#prior",
    "href": "digit-recognition.html#prior",
    "title": "Handwritten digit recognition",
    "section": "Prior",
    "text": "Prior\n\n\n\n\n\n\nnames(y) = c(paste0(\"block\", 1:64), \"id\")\ny0 = y %>%\n  filter(id == 0)\n\ntheta0 = c(0, rep(1/64, 63))\ntheta0 = y0[1,-65]\ntheta0 = t(theta0) %>% as.vector()\ndmultinom(y0[1,-65], prob = theta0, log = TRUE)\ndmultinom(y0[10, -65], prob = theta0, log = TRUE)\n\n\nN = 5000\n\nposteriorMeans = NULL\nfor(i in 0:9) {\n  df = y %>% \n    filter(id == i) %>%\n    select(-id)\n  alpha = colSums(df) + 1\n  # assign(paste0(\"posteriorMeans\", i), \n  #        rdirichlet(N, alpha) %>% colMeans())\n  posteriorMeans = cbind(posteriorMeans, \n                           rdirichlet(N, alpha) %>% colMeans())\n}\n\n\n# alpha0 = colSums(y0) + 1\n# posteriorMeans0 = rdirichlet(N, alpha0) %>%\n#   colMeans()\n\ndim(posteriorMeans)\n\n\nytest = read_csv(\"~/Downloads/optdigits.tes\")\nnames(ytest) = c(paste0(\"block\", 1:64), \"id\")\n\n\nprediction = vector(length = nrow(ytest))\nfor(i in 1:nrow(ytest)) {\n  prediction[i] = \napply(posteriorMeans, MARGIN = 2, FUN = function(p) dmultinom(x = ytest[i,-65], prob = p, log = TRUE)) %>%\n  which.max() - 1\n}\n\ndf2 = data.frame(truth = ytest$id, prediction)\ndf2 %>%\n  mutate(correct = truth == prediction) %>%\n  summarize(mean(correct))\n\nIdea\n\nDifferent set of thetas for each digit category.\nLearn thetas.\nCompute posterior mean of theta in each category\nclassify according to which set of thetas yields maximum likelihood‚Ä¶"
  },
  {
    "objectID": "hw/hw09.html#exercise-2",
    "href": "hw/hw09.html#exercise-2",
    "title": "Homework 9",
    "section": "Exercise 2",
    "text": "Exercise 2\nHares-Lynx population dynamics\nA Canadian business, Hudson‚Äôs Bay company is North America‚Äôs longest continually operating company (founded in 1670). During the 19th and 20th century, the company bought animal pelts of both snowshoe hares and lynx from trappers. Lynx are the most predominant natural predator of snowshoe hares. The counts of pelts provide an approximation to the population size of each animal. Data sourced from here.\n\nhares_lynx = read_csv(\"../data/hares-lynx.csv\")\nglimpse(hares_lynx)\n\nRows: 21\nColumns: 3\n$ year  <dbl> 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910‚Ä¶\n$ hares <dbl> 30.0, 47.2, 70.2, 77.4, 36.3, 20.6, 18.1, 21.4, 22.0, 25.4, 27.1‚Ä¶\n$ lynx  <dbl> 4.0, 6.1, 9.8, 35.2, 59.4, 41.7, 19.0, 13.0, 8.3, 9.1, 7.4, 8.0,‚Ä¶\n\n\n\nyear is the year of record\nhares is the number of hare pelts (thousands)\nlynx is the number of lynx pelts (thousands)\n\n\nDynamical System\nThe following system is an example known as ‚ÄúLotka-Volterra‚Äù dynamics which describe an interactive model of the population of predator and prey.\n\n\n\n\n\n\n\n\\(H(t)\\)\nNumber of hares at time t\n\n\n\\(H(0) = H_0\\)\nNumber of hares at time 0\n\n\n\\(L(t)\\)\nNumber of lynx at time t\n\n\n\\(L(0) = L_0\\)\nNumber of lynx at time 0\n\n\n\\(a_1\\)\nPer capita birth rate of hares\n\n\n\\(b_1\\)\nPer capita death rate of lynx\n\n\n\\(a_2\\)\nRate at which lynx eat hares\n\n\n\\(b_2\\)\nRate at which prey affects predator growth\n\n\n\nSome assumptions:\n\nThe prey population finds ample food at all times.\nThe food supply of the predator population depends entirely on the size of the prey population.\nThe rate of change of population is proportional to its size.\nDuring the process, the environment does not change in favor of one species, and genetic adaptation is inconsequential.\nPredators have limitless appetite.\nBoth populations can be described by a single variable. This amounts to assuming that the populations do not have a spatial or age distribution that contributes to the dynamics.\n\n\\[\n\\begin{aligned}\n\\frac{dH(t)}{dt} &= a_1 H(t) - a_2 H(t)L(t)\\\\\n\\frac{dL(t)}{dt} &= - b_1 L(t) + b_2 H(t)L(t)\n\\end{aligned}\n\\]\n\n\nStatistical model\nOne more assumption: we will assume multivariate normal noise in our observations \\(H(t), L(t)\\).\n\\[\n  \\begin{align}\n   \\begin{bmatrix}\n           H(t) \\\\\n           L(t)\n         \\end{bmatrix}\n         &\\sim\n         MVN \\left(\n         \\begin{bmatrix}\n           \\tilde{H}(t) \\\\\n           \\tilde{L}(t)\n         \\end{bmatrix},\n         I_2\n          \\right)\n  \\end{align}\n\\]\nwhere \\(\\tilde{H}(t), \\tilde{L}(t)\\) is the solution of the system of differential equations and is a function of \\(H(0), L(0), a_1, a_2, b_1, b_2\\) as well as \\(t\\).\n\\(I_2\\) is a \\(2 \\times 2\\) identity matrix.\nUse the function below to simulate a \\(\\{H(t), L(t)\\}\\) given a set of parameters.\n\n\n\n\n\n\nImportant\n\n\n\ntodo: add Lotka-Volterra integration via Euler‚Äôs method\n\n\n\nPlace reasonable priors on each unknown. Explain why the prior choices make sense.\nConstruct an MCMC sampler of your choice to sample from the posterior \\(p(a_1, a_2, b_1, b_2, H(0), L(0) | data)\\).\nExamine MCMC diagnostics and discuss whether or not your posterior converged.\nUsing the posterior mean estimates of each unknown, forward simulate a data set \\(\\{H^*(t), L^*(t) \\}\\) for each time 1900, ‚Ä¶, 1920. Plot a time series of the predicted population and compare to the empirical trajectory of the populations."
  },
  {
    "objectID": "hw/hw09.html#exercise-3",
    "href": "hw/hw09.html#exercise-3",
    "title": "Homework 9",
    "section": "Exercise 3",
    "text": "Exercise 3\nTBD"
  },
  {
    "objectID": "notes/waterOnMars.html#bayesian-inverse-problems",
    "href": "notes/waterOnMars.html#bayesian-inverse-problems",
    "title": "Water on Mars",
    "section": "Bayesian inverse problems",
    "text": "Bayesian inverse problems\nIdea: writing the likelihood is difficult or impossible in closed form.\nAs usual, we wish to estimate some parameter vector \\(\\theta\\). However, \\(\\theta\\) relates to our observations \\(y\\) via some function \\(y = f(\\theta)\\). The catch is, we cannot write \\(f(\\theta)\\) directly, nor compute it exactly. Most often, all we can do is simulate from \\(f(\\theta)\\)."
  },
  {
    "objectID": "notes/waterOnMars.html#results",
    "href": "notes/waterOnMars.html#results",
    "title": "Water on Mars",
    "section": "Results",
    "text": "Results\n\nTHETA %>%\n  apply(FUN = effectiveSize, MARGIN = 2)\n\n   theta1    theta2    theta3    theta4    theta5    theta6 iteration \n 114.8943  136.5720  278.0475 2502.2433  475.4239 1036.8308    0.0000 \n\ncat(\"\\n\")\nposteriorMeans = \n  THETA %>%\n  colMeans()\nposteriorMeans\n\n      theta1       theta2       theta3       theta4       theta5       theta6 \n2.083255e-01 1.831104e-01 5.988736e-01 7.820175e+01 3.175727e+01 2.816404e+03 \n   iteration \n2.000100e+04 \n\nmyBerry(posteriorMeans)[c(\"vp\", \"vs\", \"ro2\")]\n\n$vp\n  theta3 \n4602.632 \n\n$vs\n  theta6 \n2596.363 \n\n$ro2\n  theta6 \n2410.352 \n\n\n\nhist(THETA$theta1)\n\n\n\nhist(THETA$theta2)\n\n\n\nhist(THETA$theta3)"
  },
  {
    "objectID": "notes/waterOnMars.html#shameless-code-duplication",
    "href": "notes/waterOnMars.html#shameless-code-duplication",
    "title": "Water on Mars",
    "section": "Shameless code duplication",
    "text": "Shameless code duplication\n\nset.seed(360)\ndtruncnormL = function(x, a, b, mean, sd) {\n  log(dtruncnorm(x, a=a, b=b, mean = mean, sd = sd))\n}\n\nrcnorm<-function(n, mean=0, sd=1, a=-Inf, b=Inf){\n  u = runif(n, pnorm((a - mean) / sd), pnorm((b - mean) / sd))\n  mean + (sd * qnorm(u))\n}\n\nlogPosterior = function(theta) {\n  result = myBerry(theta)\n  y = c(result$vp / 1000, result$vs / 1000, result$ro2)\n  if(result$vp < result$vs) {\n    return(-Inf)\n  }\n  else{\n  return(\n    sum(dnorm(y, mean = seismic_data$y, sd = seismic_data$sd,\n            log = TRUE)) + \n      dbeta(theta[3], .1, .5, log = TRUE)\n  )\n  }\n}\n\n# THETA[1] = alpha\n# THETA[2] = porosity\n# THETA[3] = saturation\n# THETA[4] = kappa_m\n# THETA[5] = mu_m\n# THETA[6] = rho_m\n\n# starting point\nalpha = 0.5\nporosity = 0.25 #runif(1, 0.05, 0.50)\nsaturation = 0.05 #0 to 1\nkappa_m = 78\nmu_m = 33\nrhom = 3000\ntheta = c(alpha, porosity, saturation, kappa_m, mu_m, rhom)\n\n# prior\n## indicator that parameters are in proper ranges * constant * indicator that Vp > Vs\n\n# MCMC \nS = 5000\naccept = rep(0, length(theta))\n\nTHETA = NULL\nfor(i in 1:S) {\n  thetaStar = theta \n  thetaStar[1] = rcnorm(1, mean = theta[1], sd = .05,\n                        a = 0.03, b = 0.99)\n   log.r = logPosterior(thetaStar) + \n     dtruncnormL(theta[1], \n                a=0.03, b=0.99, \n                mean = thetaStar[1], sd = .05) - \n     logPosterior(theta) - \n     dtruncnormL(thetaStar[1], \n                a=0.03, b=0.99, \n                mean = theta[1], sd = .05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[1] = accept[1] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[2] = rcnorm(1, mean = theta[2], sd = .05,\n                        a = 0.05, b = 0.5)\n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[2], \n                a=0.05, b=0.5, \n                mean = theta[2], sd = .05) - \n     logPosterior(theta) - \n      dtruncnormL(thetaStar[2], \n                a=0.05, b=0.5, \n                mean = theta[2], sd = .05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[2] = accept[2] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[3] = rcnorm(1, mean = theta[3], sd = 0.05,\n                        a = 0, b = 1)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[3], \n                a=0, b=1, \n                mean = theta[3], sd = 0.05) - \n     logPosterior(theta) - \n    dtruncnormL(thetaStar[3], \n                a=0, b=1, \n                mean = theta[3], sd = 0.05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[3] = accept[3] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[4] = rcnorm(1, mean = theta[4], sd = .5,\n                        a = 76.5, b = 80)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[4], \n                a=76.5, b=80, \n                mean = theta[4], sd = .5) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[4], \n                a=76.5, b=80, \n                mean = theta[4], sd = .5)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[4] = accept[4] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[5] = rcnorm(1, mean = theta[5], sd = 1,\n                        a = 25.6, b = 40)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[5], \n                a=25.6, b=40, \n                mean = theta[5], sd = 1) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[5], \n                a=25.6, b=40, \n                mean = theta[5], sd = 1)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[5] = accept[5] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[6] = rcnorm(1, mean = theta[6], sd = 20,\n                        a = 2689, b = 2900)\n  \n   log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[6], \n                a=2689, b=2900, \n                mean = theta[6], sd = 20) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[6], \n                a=2689, b=2900, \n                mean = theta[6], sd = 20)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[6] = accept[6] + 1 \n   }\n  \n   THETA = rbind(THETA, theta)\n}\n\n\ndim(THETA)\n\neffectiveSize(THETA[,3])\n\nhist(THETA[,3])\n\n\nxx = apply(THETA2, MARGIN = 2, FUN = median) # THETA2 = og data set\nlogPosterior(xx)\n\nx2 = xx\nx2[3] = .99\nlogPosterior(x2)\n\nx3 = xx\nx3[3] = 0.01\nlogPosterior(x3)"
  },
  {
    "objectID": "notes/HamiltonianMonteCarlo.html",
    "href": "notes/HamiltonianMonteCarlo.html",
    "title": "Hamiltonian Monte Carlo",
    "section": "",
    "text": "See libraries used in these notes\nlibrary(tidyverse)\nlibrary(latex2exp)\nlibrary(mvtnorm)\nlibrary(coda)"
  },
  {
    "objectID": "notes/HamiltonianMonteCarlo.html#hamiltonian-monte-carlo",
    "href": "notes/HamiltonianMonteCarlo.html#hamiltonian-monte-carlo",
    "title": "Hamiltonian Monte Carlo",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\nHamiltonian Monte Carlo (HMC) is a proposal mechanism \\(J(\\theta | \\theta^{(s)})\\), that uses Hamiltonian dynamics to generate proposals that are far away from the current state of the chain with high acceptance probability. These proposals are subsequently accepted or rejected according to the Metropolis-Hastings acceptance ratio.\n\nMotivation: the banana target\n\ntarget distributioncodeplot\n\n\nposterior:\n\\[\np(\\theta | y_1, \\ldots y_n) \\propto \\underbrace{\\prod_{i=1}^n \\text{dnorm}(y_i; \\theta_1 + \\theta_2^2, 1)}_{\\text{likelihood}} \\cdot \\underbrace{\\text{dnorm}(\\theta_1; 0, 1) \\text{dnorm}(\\theta_2; 0, 1)}_{\\text{priors}}\n\\]\n\n\n\nlogPosterior = function(theta) {\n  c = theta[1] + (theta[2] ^ 2)\n  logLikelihood = sum(dnorm(y, mean = c, sd = 1, log = TRUE))\n  logPrior = dnorm(theta[1], 0, 1, log = TRUE) +\n    dnorm(theta[2], 0, 1, log = TRUE)\n    return(logLikelihood + logPrior)\n}\n\n\n# simulated data y\nset.seed(360)\nn = 30\ntheta1 = .75\ntheta2 = .5\ny = rnorm(n, (theta1 + (theta2^2)), 1)\ny\n\n [1]  2.4374945977  1.3225732383  0.7957033706  0.0009050433  0.9624998552\n [6]  0.2485689217  0.3494050797  0.8481528753  0.1619672883  1.5373043843\n[11]  1.9319327323  2.1723549678  0.5916180759  1.5788760946 -0.2521989302\n[16] -0.0956751145  2.1896602700  2.7428271328 -0.8507334992 -0.3434228915\n[21]  0.7158629051  2.9076884521 -0.0258688807  2.7880781640  1.3319085255\n[26]  1.0734242350  1.3910936322  1.8806039555  1.6171004720  1.4077704842\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What about this target distribution could challenge a Metropolis sampler with \\(J(\\theta_i | \\theta_i^{(s)}) = \\text{normal}(\\theta_i, \\delta)\\)?\nLet‚Äôs try it out.\n\nMetropolis samplertrajectoryESSautocorrelationtrace plots\n\n\n\n# sample from posterior\nset.seed(360)\ntheta1 = 0 # starting point\ntheta2 = 0\nTHETA1 = NULL # empty object to save iterations in\nTHETA2 = NULL\nS = 10000 # number of iterations\ndelta = .5 # proposal variance\naccept1 = 0 # keep track of acceptance rate\naccept2 = 0\n\nfor (s in 1:S) {\n  # log everything for numerical stability #\n  \n  ### generate proposal 1 and compute ratio r ###\n  theta1star = rnorm(1, mean = theta1, sd = delta) \n  log.r = logPosterior(c(theta1star, theta2)) - \n    logPosterior(c(theta1, theta2))\n  \n  ### accept or reject proposal and add to chain ###\n  if(log(runif(1)) < log.r)  {\n    theta1 = theta1star\n    accept1 = accept1 + 1 \n  }\n  THETA1 = c(THETA1, theta1)\n  THETA2 = c(THETA2, theta2)\n  \n   ### generate proposal 2 and compute ratio r ###\n  theta2star = rnorm(1, mean = theta2, sd = delta) \n  log.r = logPosterior(c(theta1, theta2star)) - \n    logPosterior(c(theta1, theta2))\n  \n  ### accept or reject proposal and add to chain ###\n  if(log(runif(1)) < log.r)  {\n    theta2 = theta2star\n    accept2 = accept2 + 1 \n  }\n  THETA1 = c(THETA1, theta1)\n  THETA2 = c(THETA2, theta2)\n}\n\n\n\n\n\n\n\n\n\n\n\neffectiveSize(THETA1)\n\n    var1 \n38.28904 \n\neffectiveSize(THETA2)\n\n    var1 \n40.44706 \n\n\n\n\n\npar(mfrow=c(1,2))\nacf(THETA1)\nacf(THETA2)\n\n\n\n\n\n\n\nN = length(THETA1)\ndf = data.frame(theta = c(THETA1, THETA2), \n                theta_id = c(rep(\"theta1\", N), rep(\"theta2\", N)),\n                step = rep(1:N, 2))  \ndf %>%\n  ggplot(aes(x = step, y = theta, col = theta_id)) + \n  geom_line() +\n  theme_bw() +\n  facet_wrap(~ theta_id) +\n  labs(x = \"iteration\", y = \"value\")\n\n\n\n\n\n\n\n\n\nHamiltonian dynamics\nIf we view the state of the Markov chain as the physical location of a particle in parameter space, then what happens if we pretend the laws of physics apply to this physical space? More specifically, let‚Äôs suppose the steps of the Markov chain are akin to a particle moving through Euclidean space and obeying Hamiltonian dynamics. The Hamiltonian of a system specifies its total energy.\nTo be a Hamiltonian system, the particle will have:\n\na location (the position in parameter space)\npotential energy\nkinetic energy\n\nQuestion: we are going to match up the negative log-posterior to either the kinetic energy or the potential energy. Which one do you think makes more sense? Why does the negative sign make sense when we think of what we are trying to do in the context of this as a physical system?\nMathematically, let \\(q\\) be the position of the particle (in parameter space) and let \\(p\\) be the momentum of the particle. So \\(q\\) and \\(p\\) are both vectors of the same dimension (the dimension of parameter space). Then the Hamiltonian, \\(H = U(q) + K(p)\\) where \\(U(q)\\) and \\(K(p)\\) are the potential and kinetic energy respectively. We will let \\(U(q) = - \\log \\pi(q)\\) where \\(\\pi(q)\\) is our target distribution.\nHamilton‚Äôs equations of motion state\n\\[\n\\begin{aligned}\n\\frac{dq_i}{dt} &= \\frac{\\partial{H}}{\\partial p_i}\\\\\n\\frac{dp_i}{dt} &= -\\frac{\\partial{H}}{\\partial q_i}\\\\\n\\end{aligned}\n\\]\nThese equations govern the motion of the particle. They let us map from the state at time \\(t\\) to the state of the system at any future state \\(t + s\\). And it can be shown that \\(\\frac{d}{dt} H = 0\\). In words, energy is conserved.\n\n\n\n\n\n\nImportant\n\n\n\nThe above equations elicit a need to compute \\(-\\frac{\\partial H}{\\partial q_i} = \\frac{\\partial}{\\partial q_i} \\log \\pi(q)\\), i.e.¬†the gradient of the log-posterior.\n\n\nA simple choice of kinetic energy is:\n\\[\nK(p) = \\frac{1}{2} p^T M^{-1}p\n\\]\nwhere \\(M\\) is called the ‚Äúmass matrix‚Äù.\nQuestion: this looks like the log of a kernel you know‚Ä¶ which one?\n\n\nAlgorithm\nFundamentally, HMC is just the Metropolis algorithm with proposals generated via Hamiltonian dynamics. The equations of motion above describe a vector field, and if we integrate them numerically, we can follow the flow through joint space of parameters and momentum.\nLet‚Äôs tackle the banana target from before in an example.\n\ngradientHMC codetrajectoryESSautocorrelationtrace plots\n\n\nWe need the gradient of the log-posterior\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial\\theta_1} \\log \\pi(\\theta_1, \\theta_2 | y_1,\\ldots y_n) &=\n\\frac{\\partial}{\\partial\\theta_1} \\log\\prod_{i=1}^n \\text{dnorm}(y_i; \\theta_1 + \\theta_2^2, 1) \\cdot \\text{dnorm}(\\theta_1; 0, 1) \\text{dnorm}(\\theta_2; 0, 1)\\\\\n&= n \\bar{y} - n \\theta_1 - n\\theta_2^2 - \\theta_1\\\\\n&= n\\bar{y} - n\\theta_2^2 - \\theta_1(n + 1)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial\\theta_2} \\log \\pi(\\theta_1, \\theta_2 | y_1,\\ldots y_n) &=\n\\frac{\\partial}{\\partial\\theta_2} \\log\\prod_{i=1}^n \\text{dnorm}(y_i; \\theta_1 + \\theta_2^2, 1) \\cdot \\text{dnorm}(\\theta_1; 0, 1) \\text{dnorm}(\\theta_2; 0, 1)\\\\\n&= 2n \\bar{y} \\theta_2 - 2 n \\theta_1 \\theta_2 - 2n\\theta_2^3\n\\end{aligned}\n\\]\n\nn = length(y)\nybar = mean(y)\ngradLogPosterior = function(theta) {\n  gradTheta1 = (n*ybar) - (n * (theta[2]^2)) - ((theta[1])*(n+1))\n  gradTheta2 = (2 * n * ybar * theta[2]) - (2 * n * theta[1] * theta[2]) - \n    (2 * n * (theta[2]^3))\n  return(c(gradTheta1, gradTheta2))\n}\n\n\n\nHMC code block below from Neal (2011), see the references\n\nHMC = function (U, grad_U, epsilon, L, current_q) { \n  q = current_q\n  p = rnorm(length(q), 0, 1) # independent standard normal variates\n  current_p = p\n  # Make a half step for momentum at the beginning\n  p = p - epsilon * grad_U(q) / 2\n  # Alternate full steps for position and momentum\n  for (i in 1:L) {\n    # Make a full step for the position\n    q = q + epsilon * p\n    # Make a full step for the momentum, except at end of trajectory\n    if (i != L)\n      p = p - epsilon * grad_U(q)\n  }\n  # Make a half step for momentum at the end.\n  p = p - epsilon * grad_U(q) / 2\n  # Negate momentum at end of trajectory to make the proposal symmetric\n  p = -p\n  # Evaluate potential and kinetic energies at start and end of trajectory\n  current_U = U(current_q)\n  current_K = sum(current_p ^ 2) / 2\n  proposed_U = U(q)\n  proposed_K = sum(p ^ 2) / 2\n  # Accept or reject the state at end of trajectory, returning either\n  # the position at the end of the trajectory or the initial position\n  if (runif(1) < exp(current_U - proposed_U + current_K - proposed_K))\n  {\n    return (q) # accept\n  }\n  else\n  {\n    return (current_q) # reject\n  }\n}\n\n\nset.seed(360)\nS = 10000\nTHETA1 = NULL\nTHETA2 = NULL\ncurrent_q = c(1, 0)\n\nU = function(theta) {\n  return(-1 * logPosterior(theta))\n}\n\ngradU = function(theta) {\n  return(-1 * gradLogPosterior(theta))\n}\n\n\nfor(s in 1:S) {\n  current_q = HMC(U, gradU, epsilon = .05, L = 10, current_q)\n  theta1 = current_q[1]\n  theta2 = current_q[2]\n  THETA1 = c(THETA1, theta1)\n  THETA2 = c(THETA2, theta2)\n}\n\n\n\n\ntrajectoryDF = data.frame(theta1 = THETA1, theta2 = THETA2) %>%\n  head(n = 300)\n\nTHETA %>%\n  ggplot(aes(x = theta1, y = theta2)) +\n  stat_density_2d(aes(fill = ..level..), geom = \"polygon\") +\n  theme_bw() +\n  labs(x = TeX(\"$\\\\theta_1$\"), y = TeX(\"$\\\\theta_2$\"), fill = \"density\",\n       title = \"Trajectory of first 300 steps of HMC\") +\n  geom_path(data = trajectoryDF, color = \"orange\", alpha = 0.6, size=0.5)\n\n\n\n\n\n\n\neffectiveSize(THETA1)\n\n    var1 \n765.6712 \n\neffectiveSize(THETA2)\n\n    var1 \n301.3347 \n\n\n\n\n\npar(mfrow=c(1,2))\nacf(THETA1)\nacf(THETA2)\n\n\n\n\n\n\n\nN = length(THETA1)\ndf = data.frame(theta = c(THETA1, THETA2), \n                theta_id = c(rep(\"theta1\", N), rep(\"theta2\", N)),\n                step = rep(1:N, 2))  \ndf %>%\n  ggplot(aes(x = step, y = theta, col = theta_id)) + \n  geom_line() +\n  theme_bw() +\n  facet_wrap(~ theta_id) +\n  labs(x = \"iteration\", y = \"value\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFurther reading\nGreat it works‚Ä¶ but how do I know this is producing an ergodic Markov chain?\n\nMichael Betancourt‚Äôs conceptual intro\nRadford Neal‚Äôs comprehensive book chapter\nHow is it implemented in stan?"
  },
  {
    "objectID": "notes/waterOnMars.html#example",
    "href": "notes/waterOnMars.html#example",
    "title": "Water on Mars",
    "section": "Example",
    "text": "Example\nToday‚Äôs example comes from Liquid water in the Martian mid-crust by Wright, Morzfeld and Manga.\n\nThe Interior Exploration using Seismic Investigations, Geodesy and Heat Transport (InSight) mission from NASA sent a robotic lander to Mars on May 5, 2018. The goal of the mission, which ended December 2022, was to study the crust, mantle and core of Mars.\n\n\nCode\nlibrary(tidyverse)\n\n\n\nData\nWatch the video here to see the data that InSight collects from earthquakes on Mars.\n\nseismic_data = data.frame(y = c(4.1, 2.5, 2589),\n                          sd = c(0.2, 0.3, 157))\n\n\n\nParameter Table\n\n\n\nSymbol in paper\nCode\nDefinition\n\n\n\n\n\\(\\alpha\\)\ntheta[1]\nPore shape aspect ratio\n\n\n\\(\\phi\\)\ntheta[2]\nPorosity\n\n\n\\(\\gamma_w\\)\ntheta[3]\nWater saturation (%)\n\n\n\\(\\kappa_m\\)\ntheta[4]\nMineral bulk modulus (GPa)\n\n\n\\(\\mu_m\\)\ntheta[5]\nMineral shear modulus (GPa)\n\n\n\\(\\rho_m\\)\ntheta[6]\nMineral density (kg/m\\(^3\\))\n\n\n\n\n\n\n\n\nBerryman self-consistent rock physics model\n\n\nShow Berryman functions\nberry_scm <- function(k, mu, asp, x, ro1, P_water) {\n  # BERRYSCM - Effective elastic moduli for multi-component composite\n  # using Berryman's Self-Consistent (Coherent Potential Approximation) method.\n  # \n  # Arguments:\n  # k: Bulk moduli of the N constituent phases (numeric vector)\n  # mu: Shear moduli of the N constituent phases (numeric vector)\n  # asp: Aspect ratio for the inclusions of the N phases (numeric vector)\n  # x: Fraction of each phase. Solid, then fluid phase (numeric vector)\n  # ro1: Density of the original rock (numeric scalar)\n  # P_water: Proportion of water (numeric scalar)\n  #\n  # Returns:\n  # kbr: Effective bulk modulus (numeric scalar)\n  # mubr: Effective shear modulus (numeric scalar)\n  # vp: P-wave velocity (numeric scalar)\n  # vs: S-wave velocity (numeric scalar)\n  # ro2: Final rock density after fluid substitution (numeric scalar)\n  # k2: Rock bulk modulus after fluid substitution (numeric scalar)\n  \n  # Ensure inputs are column vectors (equivalent to MATLAB's k(:), mu(:), etc.)\n  k <- as.vector(k)\n  mu <- as.vector(mu)\n  asp <- as.vector(asp)\n  x <- as.vector(x)\n  \n  # Modify aspect ratios of inclusions that are equal to 1\n  asp[asp == 1] <- 0.99\n  \n  # Initialize variables\n  theta <- numeric(length(asp))\n  fn <- numeric(length(asp))\n  \n  # Compute theta and fn for oblate and prolate spheroids\n  obdx <- which(asp < 1)\n  prdx <- which(asp > 1)\n  \n  theta[obdx] <- (asp[obdx] / ((1 - asp[obdx]^2)^(3/2))) * (acos(asp[obdx]) - asp[obdx] * sqrt(1 - asp[obdx]^2))\n  fn[obdx] <- (asp[obdx]^2 / (1 - asp[obdx]^2)) * (3 * theta[obdx] - 2)\n  \n  theta[prdx] <- (asp[prdx] / ((asp[prdx]^2 - 1)^(3/2))) * (asp[prdx] * sqrt(asp[prdx]^2 - 1) - acosh(asp[prdx]))\n  fn[prdx] <- (asp[prdx]^2 / (asp[prdx]^2 - 1)) * (2 - 3 * theta[prdx])\n  \n  # Initialize initial bulk and shear moduli\n  ksc <- sum(k * x)\n  musc <- sum(mu * x)\n  \n  # Initialize iteration parameters\n  knew <- 0\n  tol <- 1e-6 * k[1]\n  del <- abs(ksc - knew)\n  niter <- 0\n  \n  # Iterative solution for effective moduli\n  while (del > abs(tol) && niter < 3000) {\n    nusc <- (3 * ksc - 2 * musc) / (2 * (3 * ksc + musc))\n    a <- mu / musc - 1\n    b <- (1 / 3) * (k / ksc - mu / musc)\n    r <- (1 - 2 * nusc) / (2 * (1 - nusc))\n    \n    f1 <- 1 + a * ((3 / 2) * (fn + theta) - r * ((3 / 2) * fn + (5 / 2) * theta - (4 / 3)))\n    f2 <- 1 + a * (1 + (3 / 2) * (fn + theta) - (r / 2) * (3 * fn + 5 * theta)) + b * (3 - 4 * r)\n    f2 <- f2 + (a / 2) * (a + 3 * b) * (3 - 4 * r) * (fn + theta - r * (fn - theta + 2 * theta^2))\n    f3 <- 1 + a * (1 - (fn + (3 / 2) * theta) + r * (fn + theta))\n    f4 <- 1 + (a / 4) * (fn + 3 * theta - r * (fn - theta))\n    f5 <- a * (-fn + r * (fn + theta - (4 / 3))) + b * theta * (3 - 4 * r)\n    f6 <- 1 + a * (1 + fn - r * (fn + theta)) + b * (1 - theta) * (3 - 4 * r)\n    f7 <- 2 + (a / 4) * (3 * fn + 9 * theta - r * (3 * fn + 5 * theta)) + b * theta * (3 - 4 * r)\n    f8 <- a * (1 - 2 * r + (fn / 2) * (r - 1) + (theta / 2) * (5 * r - 3)) + b * (1 - theta) * (3 - 4 * r)\n    f9 <- a * ((r - 1) * fn - r * theta) + b * theta * (3 - 4 * r)\n    \n    p <- 3 * f1 / f2\n    q <- (2 / f3) + (1 / f4) + ((f4 * f5 + f6 * f7 - f8 * f9) / (f2 * f4))\n    \n    p <- p / 3\n    q <- q / 5\n    \n    # Update moduli\n    knew <- sum(x * k * p) / sum(x * p)\n    munew <- sum(x * mu * q) / sum(x * q)\n    \n    del <- abs(ksc - knew)\n    ksc <- knew\n    musc <- munew\n    niter <- niter + 1\n  }\n  \n  kbr <- ksc\n  mubr <- musc\n  \n  # Density and fluid substitution\n  rofl1 <- 0.020    # density of gas\n  kfl1 <- 0         # bulk modulus of gas\n  ro_water <- 1000   # density of water\n  k_water <- 2.2e9  # bulk modulus of water\n  P_gas <- 1 - P_water\n  rofl2 <- P_gas * rofl1 + P_water * ro_water\n  kfl2 <- P_gas * kfl1 + P_water * k_water\n  k0 <- k[1]        # bulk modulus of solid mineral phase\n  phi <- x[2]       # porosity of rock\n  k1 <- kbr         # dry bulk modulus\n  \n  # Perform fluid substitution using Gassmann equation\n  a <- k1 / (k0 - k1) - kfl1 / (phi * (k0 - kfl1)) + kfl2 / (phi * (k0 - kfl2))\n  k2 <- k0 * a / (1 + a)   # bulk modulus after fluid substitution\n  \n  # Compute final density after fluid substitution\n  ro2 <- ro1 - phi * rofl1 + phi * rofl2\n  \n  # Compute seismic velocities after fluid substitution\n  mu2 <- mubr\n  vp <- sqrt((k2 + (4 / 3) * mu2) / ro2)\n  vs <- sqrt(mu2 / ro2)\n  \n  return(list(kbr = kbr, mubr = mubr, vp = vp, vs = vs, ro2 = ro2, k2 = k2))\n}\n\nmyBerry <- function(theta, H = 3) {\n  # Extract parameters from theta\n  asp <- c(1, theta[1])   # Aspect ratio\n  x_phi <- theta[2]       # Proportion of fluid (phi)\n  rock_vol <- 1 - theta[2] # Volume of solid phase (rock)\n  x <- c(rock_vol, x_phi)  # Fraction of phases\n  rock_density <- theta[6] * rock_vol  # Density of solid phase (basalt)\n  gas_density <- 0.020 * x_phi         # Density of fluid phase (gas)\n  rhob1 <- rock_density + gas_density  # Bulk density\n  \n  # Percentage of water in pore space\n  P_water <- theta[3]\n  \n  # Bulk and shear moduli (scaled by 1e9 as per MATLAB code)\n  k <- c(theta[4] * 1e9, 0)  # Bulk modulus (first element)\n  mu <- c(theta[5] * 1e9, 0) # Shear modulus (first element)\n  \n  # Call berryscm function (ensure berryscm function is defined in your R environment)\n  result <- berry_scm(k, mu, asp, x, rhob1, P_water)\n  \n  # Return the required output from berryscm\n  return(result)\n}"
  },
  {
    "objectID": "notes/priors.html",
    "href": "notes/priors.html",
    "title": "Priors",
    "section": "",
    "text": "Definition\n\n\n\nA proper prior is a density function that:\n\ndoes not depend on data and\nintegrates to 1.\n\nIf a prior is not proper, we call the prior improper.\nIf a prior integrates to a positive finite value, it is an un-normalized density. This is different from being an improper prior. An un-normalized density can be normalized by being multiplied by a constant to integrate to 1.\n\n\n\n\n\\[\n\\begin{aligned}\nY | \\theta, \\sigma^2 &\\sim N(\\theta, \\sigma^2)\\\\\np(\\theta, \\sigma^2) &= \\frac{1}{\\sigma^2}\n\\end{aligned}\n\\]\n\\(p(\\theta, \\sigma^2)\\) is an improper prior. \\(p(\\theta, \\sigma^2)\\) does not integrate to a finite value and thus cannot be renormalized. It is not a probability density. However, it yields a tractable posterior for \\(\\theta\\) and \\(\\sigma^2\\) (see p 79 of Hoff)."
  },
  {
    "objectID": "notes/priors.html#reference-prior",
    "href": "notes/priors.html#reference-prior",
    "title": "Improper priors",
    "section": "Reference prior",
    "text": "Reference prior"
  },
  {
    "objectID": "notes/priors.html#unit-information-prior",
    "href": "notes/priors.html#unit-information-prior",
    "title": "Priors",
    "section": "Unit information prior",
    "text": "Unit information prior\nPriors are meant to describe our state of knowledge before examining data.\n\n\n\n\n\n\nDefinition\n\n\n\nA unit information prior is an improper, data-dependent prior that contains the same amount of information as would be contained in a single observation.\n\n\n\nExample:\n\\[\n\\begin{aligned}\nY &\\sim N_n(X \\beta, \\sigma^2 I_n)\\\\\n\\beta &\\sim N_p(\\beta_0, \\Sigma_0)\\\\\n1/\\sigma^2  &\\sim \\text{gamma}(\\nu_0/2, \\nu_0/2 \\sigma_0^2)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\beta_0 &= (X^TX)^{-1}X^Ty\\\\\n\\Sigma_0 &= \\left((X^TX)/n\\sigma^2\\right)^{-1}\\\\\n\\nu_0 &= 1\\\\\n\\sigma_0^2 &= \\text{SSR}(\\hat{\\beta}_{OLS})/ n\n\\end{aligned}\n\\]\nThis is using the MLE (or equivalently OLS estimator) as ‚Äòunit information‚Äô. Notice \\(\\beta_0 = \\hat{\\beta}_{OLS}\\) and the variance \\(\\Sigma_0\\) is just 1/n the precision of the MLE: \\(\\left(Var[\\hat{\\beta}_{OLS}]\\right)^{-1}\\).\nSimilarly, \\(\\nu_0 = 1\\) (implying unit information) and \\(\\sigma_0^2\\) is the MLE of \\(\\sigma^2\\).\n\n\nIn practice\nProcedurally, one can construct a unit information prior in the following way:\nLet \\(Y_1,\\ldots,Y_n \\sim \\text{iid } p(y|\\theta)\\). Let \\(l(\\theta | y_1,\\ldots,y_n) = \\sum_{i=1}^n \\log p(y_i|\\theta)\\).\n\nCompute the MLE, \\(\\hat{\\theta}_{MLE} = \\text{argmax}_{\\theta}~ l(\\theta|y_1,\\ldots,y_n)\\).\nCompute the negative of the curvature of the log-likelihood: \\(J(\\theta) = - \\frac{\\partial^2}{\\partial \\theta^2} l(\\theta | y_1,\\ldots, y_n)\\).\nCenter the prior on the MLE, and let the curvature of the prior be \\(- \\frac{\\partial^2}{\\partial \\theta^2} p(\\theta) = J(\\theta) / n\\).\n\n\nExercise\n\n\n\nLet \\(Y_1,\\ldots,Y_n \\sim \\text{iid binary}(\\theta)\\). Find \\(\\hat{\\theta}_{MLE}\\).\nFind the unit information prior \\(p(\\theta)\\). Hint: find a density \\(p(\\theta)\\) such that \\(\\log p(\\theta) = l(\\theta | y_1,\\ldots,y_n) + c\\) where \\(c\\) is a constant that doesn‚Äôt depend on \\(\\theta\\)."
  },
  {
    "objectID": "notes/priors.html#noninformative-priors",
    "href": "notes/priors.html#noninformative-priors",
    "title": "Priors",
    "section": "Noninformative priors",
    "text": "Noninformative priors\nIn some cases we may wish to describe our ignorance a priori using a vague prior that plays a minimal role in the posterior distribution.\nA common trap is to imagine that a flat, or uniform prior is uninformative. However, we know that uniform priors are often informative. For example, you showed on a previous homework that a uniform prior on binary probability of success \\(\\theta\\) is informative on the log-odds \\(\\log \\left(\\frac{\\theta}{(1-\\theta)}\\right)\\).\nHowever, when a uniform prior is improper, it is informative because it states that most of the mass is infinitely far away.\n\nExample:\n\\[\n\\begin{aligned}\nY |\\theta &\\sim Poisson(\\theta)\\\\\np(\\theta) &\\propto 1 \\text{ for } \\theta \\in (0, \\infty)\n\\end{aligned}\n\\]\n\n\nJeffreys prior\n\n\n\n\n\n\nDefinition\n\n\n\nThe Jeffreys prior\n\\[\nJ(\\theta) \\propto \\sqrt{I(\\theta)}\n\\]\nwhere \\(I(\\theta) = -E[\\frac{\\partial}{\\partial\\theta^2} \\log p(Y|\\theta) | \\theta]\\).\n\n\nThe defining feature of Jeffreys prior is that it will yield an equivalent result if applied to a transformed parameter. This principle of invariance is one approach to non-informative priors that works well for single parameter priors. Multiparameter extensions are often less useful."
  },
  {
    "objectID": "notes/priors.html#unit-information-prior-1",
    "href": "notes/priors.html#unit-information-prior-1",
    "title": "Priors",
    "section": "Unit information prior",
    "text": "Unit information prior"
  },
  {
    "objectID": "notes/priors.html#improper-uniform-priors",
    "href": "notes/priors.html#improper-uniform-priors",
    "title": "Priors",
    "section": "Improper uniform priors",
    "text": "Improper uniform priors\nIn some cases we may wish to describe our ignorance a priori using a vague prior that plays a minimal role in the posterior distribution.\nA common trap is to imagine that a flat, or uniform prior is uninformative. However, we know that uniform priors are often informative. For example, you showed on a previous homework that a uniform prior on binary probability of success \\(\\theta\\) is informative on the log-odds \\(\\log \\left(\\frac{\\theta}{(1-\\theta)}\\right)\\).\nHowever, when a uniform prior is improper, it is informative because it states that most of the mass is infinitely far away.\n\nExample:\n\\[\n\\begin{aligned}\nY |\\theta &\\sim Poisson(\\theta)\\\\\np(\\theta) &\\propto 1 \\text{ for } \\theta \\in (0, \\infty)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/priors.html#jeffreys-prior",
    "href": "notes/priors.html#jeffreys-prior",
    "title": "Priors",
    "section": "Jeffreys prior",
    "text": "Jeffreys prior\n\n\n\n\n\n\nDefinition\n\n\n\nThe Jeffreys prior\n\\[\nJ(\\theta) \\propto \\sqrt{I(\\theta)}\n\\]\nwhere \\(I(\\theta) = -E[\\frac{\\partial^2}{\\partial\\theta^2} \\log p(Y|\\theta) | \\theta]\\) is the Fisher information.\n\n\nThe defining feature of Jeffreys prior is that is invariant under monotonic transformations. This principle of invariance is one approach to non-informative priors that works well for single parameter priors. Multiparameter extensions are often less useful.\n\nExample:\n\\[\n\\begin{aligned}\nY | \\theta &\\sim \\text{Poisson}(\\theta)\\\\\np(\\theta) &\\propto \\theta^{-1/2}\n\\end{aligned}\n\\]\n\nExerciseSolutionBonus\n\n\n\nProve that \\(p(\\theta) \\propto \\theta^{-1/2}\\) is the Jeffreys prior for the Poisson model above.\nAssume you observe \\(\\{y_1,\\ldots, y_n\\}\\). To what family of distributions does the posterior \\(p(\\theta|y_1,\\ldots,y_n)\\) belong? Note: you are assuming \\(Y_i \\sim \\text{iid Poisson}(\\theta)\\). What are the parameters?\n\n\n\n\\[\n\\begin{aligned}\nI(\\theta) = -E\\left[\\frac{\\partial^2}{\\partial\\theta^2} \\log p(Y|\\theta) | \\theta \\right]\n&=\n-E\\left[\\frac{\\partial^2}{\\partial\\theta^2} \\left(y \\log \\theta -\\theta - \\log y! \\right) | \\theta\\right]\\\\\n&= E\\left[ \\frac{y}{\\theta^2} | \\theta\\right]\\\\\n&= \\frac{1}{\\theta}\n\\end{aligned}\n\\]\nTherefore, \\(J(\\theta) \\propto \\theta^{-1/2}\\).\n\nPosterior is \\(\\text{gamma}(\\sum y_i + \\frac{1}{2}, n)\\)\n\n\n\nExamine the prior under re-parameterization.\nLet \\(\\phi = \\log \\theta\\). Then \\(\\log p(y | \\phi) = y \\phi - e^\\phi\\).\n\\[\n\\begin{aligned}\nI(\\phi) = -E\\left[\\frac{\\partial^2}{\\partial\\phi^2} \\log p(Y|\\phi) | \\phi \\right] &= -E\\left[ -e^\\phi |\\phi\\right]\\\\\n&= e^\\phi\n\\end{aligned}\n\\]\nTherefore, \\(J(\\phi) \\propto e^{\\phi/2}\\).\nSeparately, using the Jeffreys prior we obtained in \\(\\theta\\), one can use the change of variables formula: \\(p(\\phi) = p(\\theta) \\left|\\frac{d\\theta}{d\\phi}\\right|\\) to show that \\(J(\\phi) \\propto e^{\\phi/2}\\)."
  },
  {
    "objectID": "hw/hw09.html",
    "href": "hw/hw09.html",
    "title": "Homework 9",
    "section": "",
    "text": "load-libraries\nlibrary(tidyverse)"
  },
  {
    "objectID": "notes/waterOnMars.html#inference",
    "href": "notes/waterOnMars.html#inference",
    "title": "Water on Mars",
    "section": "Inference",
    "text": "Inference\n\n\n\n\nMCMC CodeDiagnosticsMarginal posteriorsResults\n\n\n\n\nCode\nset.seed(360)\ndtruncnormL = function(x, a, b, mean, sd) {\n  log(dtruncnorm(x, a=a, b=b, mean = mean, sd = sd))\n}\n\nrcnorm<-function(n, mean=0, sd=1, a=-Inf, b=Inf){\n  u = runif(n, pnorm((a - mean) / sd), pnorm((b - mean) / sd))\n  mean + (sd * qnorm(u))\n}\n\nlogPosterior = function(theta) {\n  result = myBerry(theta)\n  y = c(result$vp / 1000, result$vs / 1000, result$ro2)\n  if(result$vp < result$vs) {\n    return(-Inf)\n  }\n  else{\n  return(\n    sum(dnorm(y, mean = seismic_data$y, sd = seismic_data$sd,\n            log = TRUE))\n  )\n  }\n}\n\n# THETA[1] = alpha\n# THETA[2] = porosity\n# THETA[3] = saturation\n# THETA[4] = kappa_m\n# THETA[5] = mu_m\n# THETA[6] = rho_m\n\n# starting point\nalpha = 0.5\nporosity = 0.25 #runif(1, 0.05, 0.50)\nsaturation = 0.05 #0 to 1\nkappa_m = 78\nmu_m = 33\nrhom = 3000\ntheta = c(alpha, porosity, saturation, kappa_m, mu_m, rhom)\n\n# prior\n## indicator that parameters are in proper ranges * constant * indicator that Vp > Vs\n\n# MCMC \nS = 50000\naccept = rep(0, length(theta))\n\nTHETA = NULL\nfor(i in 1:S) {\n  thetaStar = theta \n  thetaStar[1] = rcnorm(1, mean = theta[1], sd = .05,\n                        a = 0.03, b = 0.99)\n   log.r = logPosterior(thetaStar) + \n     dtruncnormL(theta[1], \n                a=0.03, b=0.99, \n                mean = thetaStar[1], sd = .05) - \n     logPosterior(theta) - \n     dtruncnormL(thetaStar[1], \n                a=0.03, b=0.99, \n                mean = theta[1], sd = .05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[1] = accept[1] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[2] = rcnorm(1, mean = theta[2], sd = .05,\n                        a = 0.05, b = 0.5)\n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[2], \n                a=0.05, b=0.5, \n                mean = theta[2], sd = .05) - \n     logPosterior(theta) - \n      dtruncnormL(thetaStar[2], \n                a=0.05, b=0.5, \n                mean = theta[2], sd = .05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[2] = accept[2] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[3] = rcnorm(1, mean = theta[3], sd = 0.05,\n                        a = 0, b = 1)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[3], \n                a=0, b=1, \n                mean = theta[3], sd = 0.05) - \n     logPosterior(theta) - \n    dtruncnormL(thetaStar[3], \n                a=0, b=1, \n                mean = theta[3], sd = 0.05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[3] = accept[3] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[4] = rcnorm(1, mean = theta[4], sd = .5,\n                        a = 76.5, b = 80)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[4], \n                a=76.5, b=80, \n                mean = theta[4], sd = .5) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[4], \n                a=76.5, b=80, \n                mean = theta[4], sd = .5)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[4] = accept[4] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[5] = rcnorm(1, mean = theta[5], sd = 1,\n                        a = 25.6, b = 40)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[5], \n                a=25.6, b=40, \n                mean = theta[5], sd = 1) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[5], \n                a=25.6, b=40, \n                mean = theta[5], sd = 1)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[5] = accept[5] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[6] = rcnorm(1, mean = theta[6], sd = 20,\n                        a = 2689, b = 2900)\n  \n   log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[6], \n                a=2689, b=2900, \n                mean = theta[6], sd = 20) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[6], \n                a=2689, b=2900, \n                mean = theta[6], sd = 20)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[6] = accept[6] + 1 \n   }\n  \n   THETA = rbind(THETA, theta)\n}\n\n\n\n\n\n\nCode\nN = nrow(THETA)\nTHETA = THETA %>%\n  mutate(iteration = seq(1,N))\n\nTHETA %>%\n  filter(iteration > 30000) %>%\n  pivot_longer(cols = 1:6) %>%\n  ggplot(aes(x = iteration, y = value)) +\n  geom_line() +\n  facet_wrap(~ name, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nCode\nTHETA %>%\n  filter(iteration > 20000) %>%\n  pivot_longer(cols = 1:6) %>%\n  ggplot(aes(x = value)) +\n  geom_histogram(aes(y = ..density..), alpha = 0.5) +\n  geom_density() +\n  facet_wrap(~ name, scales = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\nlibrary(knitr)\n# posterior means of parameters\nposteriorMeans = \n  THETA %>%\n  select(-iteration) %>%\n  colMeans()\n\nposteriorMeans %>% \n  kable(col.names = c(\"parameter\", \"posteriorMean\"))\n\n\n\n\n\nparameter\nposteriorMean\n\n\n\n\ntheta1\n0.2083255\n\n\ntheta2\n0.1831104\n\n\ntheta3\n0.5988736\n\n\ntheta4\n78.2017496\n\n\ntheta5\n31.7572690\n\n\ntheta6\n2816.4036103\n\n\n\n\n\n\n\nCode\n# generating data from posterior means \nresult = myBerry(posteriorMeans)\ndataFromPostMean = c(result$vp / 1000, result$vs / 1000, result$ro2)\nrbind(dataFromPostMean, seismic_data$y) %>%\n  kable(col.names = c(\"Vp\", \"Vs\", \"Rho\"), digits = 3)\n\n\n\n\n\n\nVp\nVs\nRho\n\n\n\n\ndataFromPostMean\n4.603\n2.596\n2410.352\n\n\n\n4.100\n2.500\n2589.000"
  },
  {
    "objectID": "notes/waterOnMars.html#liquid-water-in-the-martian-mid-crust-by-wright-morzfeld-and-manga.",
    "href": "notes/waterOnMars.html#liquid-water-in-the-martian-mid-crust-by-wright-morzfeld-and-manga.",
    "title": "Water on Mars",
    "section": "Liquid water in the Martian mid-crust by Wright, Morzfeld and Manga.",
    "text": "Liquid water in the Martian mid-crust by Wright, Morzfeld and Manga.\n\nThe Interior Exploration using Seismic Investigations, Geodesy and Heat Transport (InSight) mission from NASA sent a robotic lander to Mars on May 5, 2018. The goal of the mission, which ended December 2022, was to study the crust, mantle and core of Mars.\n\n\nCode\nlibrary(tidyverse)\n\n\n\nData\nWatch the video here to see the data that InSight collects from earthquakes on Mars.\n\nseismic_data = data.frame(y = c(4.1, 2.5, 2589),\n                          sd = c(0.2, 0.3, 157))\n\n\n\nParameter Table\n\n\n\nSymbol in paper\nCode\nDefinition\n\n\n\n\n\\(\\alpha\\)\ntheta[1]\nPore shape aspect ratio\n\n\n\\(\\phi\\)\ntheta[2]\nPorosity\n\n\n\\(\\gamma_w\\)\ntheta[3]\nWater saturation (%)\n\n\n\\(\\kappa_m\\)\ntheta[4]\nMineral bulk modulus (GPa)\n\n\n\\(\\mu_m\\)\ntheta[5]\nMineral shear modulus (GPa)\n\n\n\\(\\rho_m\\)\ntheta[6]\nMineral density (kg/m\\(^3\\))\n\n\n\n\n\n\n\n\nBerryman self-consistent rock physics model\n\n\nShow Berryman functions\nberry_scm <- function(k, mu, asp, x, ro1, P_water) {\n  # BERRYSCM - Effective elastic moduli for multi-component composite\n  # using Berryman's Self-Consistent (Coherent Potential Approximation) method.\n  # \n  # Arguments:\n  # k: Bulk moduli of the N constituent phases (numeric vector)\n  # mu: Shear moduli of the N constituent phases (numeric vector)\n  # asp: Aspect ratio for the inclusions of the N phases (numeric vector)\n  # x: Fraction of each phase. Solid, then fluid phase (numeric vector)\n  # ro1: Density of the original rock (numeric scalar)\n  # P_water: Proportion of water (numeric scalar)\n  #\n  # Returns:\n  # kbr: Effective bulk modulus (numeric scalar)\n  # mubr: Effective shear modulus (numeric scalar)\n  # vp: P-wave velocity (numeric scalar)\n  # vs: S-wave velocity (numeric scalar)\n  # ro2: Final rock density after fluid substitution (numeric scalar)\n  # k2: Rock bulk modulus after fluid substitution (numeric scalar)\n  \n  # Ensure inputs are column vectors (equivalent to MATLAB's k(:), mu(:), etc.)\n  k <- as.vector(k)\n  mu <- as.vector(mu)\n  asp <- as.vector(asp)\n  x <- as.vector(x)\n  \n  # Modify aspect ratios of inclusions that are equal to 1\n  asp[asp == 1] <- 0.99\n  \n  # Initialize variables\n  theta <- numeric(length(asp))\n  fn <- numeric(length(asp))\n  \n  # Compute theta and fn for oblate and prolate spheroids\n  obdx <- which(asp < 1)\n  prdx <- which(asp > 1)\n  \n  theta[obdx] <- (asp[obdx] / ((1 - asp[obdx]^2)^(3/2))) * (acos(asp[obdx]) - asp[obdx] * sqrt(1 - asp[obdx]^2))\n  fn[obdx] <- (asp[obdx]^2 / (1 - asp[obdx]^2)) * (3 * theta[obdx] - 2)\n  \n  theta[prdx] <- (asp[prdx] / ((asp[prdx]^2 - 1)^(3/2))) * (asp[prdx] * sqrt(asp[prdx]^2 - 1) - acosh(asp[prdx]))\n  fn[prdx] <- (asp[prdx]^2 / (asp[prdx]^2 - 1)) * (2 - 3 * theta[prdx])\n  \n  # Initialize initial bulk and shear moduli\n  ksc <- sum(k * x)\n  musc <- sum(mu * x)\n  \n  # Initialize iteration parameters\n  knew <- 0\n  tol <- 1e-6 * k[1]\n  del <- abs(ksc - knew)\n  niter <- 0\n  \n  # Iterative solution for effective moduli\n  while (del > abs(tol) && niter < 3000) {\n    nusc <- (3 * ksc - 2 * musc) / (2 * (3 * ksc + musc))\n    a <- mu / musc - 1\n    b <- (1 / 3) * (k / ksc - mu / musc)\n    r <- (1 - 2 * nusc) / (2 * (1 - nusc))\n    \n    f1 <- 1 + a * ((3 / 2) * (fn + theta) - r * ((3 / 2) * fn + (5 / 2) * theta - (4 / 3)))\n    f2 <- 1 + a * (1 + (3 / 2) * (fn + theta) - (r / 2) * (3 * fn + 5 * theta)) + b * (3 - 4 * r)\n    f2 <- f2 + (a / 2) * (a + 3 * b) * (3 - 4 * r) * (fn + theta - r * (fn - theta + 2 * theta^2))\n    f3 <- 1 + a * (1 - (fn + (3 / 2) * theta) + r * (fn + theta))\n    f4 <- 1 + (a / 4) * (fn + 3 * theta - r * (fn - theta))\n    f5 <- a * (-fn + r * (fn + theta - (4 / 3))) + b * theta * (3 - 4 * r)\n    f6 <- 1 + a * (1 + fn - r * (fn + theta)) + b * (1 - theta) * (3 - 4 * r)\n    f7 <- 2 + (a / 4) * (3 * fn + 9 * theta - r * (3 * fn + 5 * theta)) + b * theta * (3 - 4 * r)\n    f8 <- a * (1 - 2 * r + (fn / 2) * (r - 1) + (theta / 2) * (5 * r - 3)) + b * (1 - theta) * (3 - 4 * r)\n    f9 <- a * ((r - 1) * fn - r * theta) + b * theta * (3 - 4 * r)\n    \n    p <- 3 * f1 / f2\n    q <- (2 / f3) + (1 / f4) + ((f4 * f5 + f6 * f7 - f8 * f9) / (f2 * f4))\n    \n    p <- p / 3\n    q <- q / 5\n    \n    # Update moduli\n    knew <- sum(x * k * p) / sum(x * p)\n    munew <- sum(x * mu * q) / sum(x * q)\n    \n    del <- abs(ksc - knew)\n    ksc <- knew\n    musc <- munew\n    niter <- niter + 1\n  }\n  \n  kbr <- ksc\n  mubr <- musc\n  \n  # Density and fluid substitution\n  rofl1 <- 0.020    # density of gas\n  kfl1 <- 0         # bulk modulus of gas\n  ro_water <- 1000   # density of water\n  k_water <- 2.2e9  # bulk modulus of water\n  P_gas <- 1 - P_water\n  rofl2 <- P_gas * rofl1 + P_water * ro_water\n  kfl2 <- P_gas * kfl1 + P_water * k_water\n  k0 <- k[1]        # bulk modulus of solid mineral phase\n  phi <- x[2]       # porosity of rock\n  k1 <- kbr         # dry bulk modulus\n  \n  # Perform fluid substitution using Gassmann equation\n  a <- k1 / (k0 - k1) - kfl1 / (phi * (k0 - kfl1)) + kfl2 / (phi * (k0 - kfl2))\n  k2 <- k0 * a / (1 + a)   # bulk modulus after fluid substitution\n  \n  # Compute final density after fluid substitution\n  ro2 <- ro1 - phi * rofl1 + phi * rofl2\n  \n  # Compute seismic velocities after fluid substitution\n  mu2 <- mubr\n  vp <- sqrt((k2 + (4 / 3) * mu2) / ro2)\n  vs <- sqrt(mu2 / ro2)\n  \n  return(list(kbr = kbr, mubr = mubr, vp = vp, vs = vs, ro2 = ro2, k2 = k2))\n}\n\nmyBerry <- function(theta, H = 3) {\n  # Extract parameters from theta\n  asp <- c(1, theta[1])   # Aspect ratio\n  x_phi <- theta[2]       # Proportion of fluid (phi)\n  rock_vol <- 1 - theta[2] # Volume of solid phase (rock)\n  x <- c(rock_vol, x_phi)  # Fraction of phases\n  rock_density <- theta[6] * rock_vol  # Density of solid phase (basalt)\n  gas_density <- 0.020 * x_phi         # Density of fluid phase (gas)\n  rhob1 <- rock_density + gas_density  # Bulk density\n  \n  # Percentage of water in pore space\n  P_water <- theta[3]\n  \n  # Bulk and shear moduli (scaled by 1e9 as per MATLAB code)\n  k <- c(theta[4] * 1e9, 0)  # Bulk modulus (first element)\n  mu <- c(theta[5] * 1e9, 0) # Shear modulus (first element)\n  \n  # Call berryscm function (ensure berryscm function is defined in your R environment)\n  result <- berry_scm(k, mu, asp, x, rhob1, P_water)\n  \n  # Return the required output from berryscm\n  return(result)\n}"
  },
  {
    "objectID": "notes/waterOnMars.html#liquid-water-in-the-martian-mid-crust1-wright-vashan-matthias-morzfeld-and-michael-manga.-liquid-water-in-the-martian-mid-crust.-proceedings-of-the-national-academy-of-sciences-121.35-2024-e2409983121.",
    "href": "notes/waterOnMars.html#liquid-water-in-the-martian-mid-crust1-wright-vashan-matthias-morzfeld-and-michael-manga.-liquid-water-in-the-martian-mid-crust.-proceedings-of-the-national-academy-of-sciences-121.35-2024-e2409983121.",
    "title": "Water on Mars",
    "section": "Liquid water in the Martian mid-crust[^1]: Wright, Vashan, Matthias Morzfeld, and Michael Manga. ‚ÄúLiquid water in the Martian mid-crust.‚Äù Proceedings of the National Academy of Sciences 121.35 (2024): e2409983121.",
    "text": "Liquid water in the Martian mid-crust[^1]: Wright, Vashan, Matthias Morzfeld, and Michael Manga. ‚ÄúLiquid water in the Martian mid-crust.‚Äù Proceedings of the National Academy of Sciences 121.35 (2024): e2409983121.\n\nThe Interior Exploration using Seismic Investigations, Geodesy and Heat Transport (InSight) mission from NASA sent a robotic lander to Mars on May 5, 2018. The goal of the mission, which ended December 2022, was to study the crust, mantle and core of Mars.\n\n\nCode\nlibrary(tidyverse)\n\n\n\nData\nWatch the video here to see the data that InSight collects from earthquakes on Mars.\n\nseismic_data = data.frame(y = c(4.1, 2.5, 2589),\n                          sd = c(0.2, 0.3, 157))\n\n\n\nParameter Table\n\n\n\nSymbol in paper\nCode\nDefinition\n\n\n\n\n\\(\\alpha\\)\ntheta[1]\nPore shape aspect ratio\n\n\n\\(\\phi\\)\ntheta[2]\nPorosity\n\n\n\\(\\gamma_w\\)\ntheta[3]\nWater saturation (%)\n\n\n\\(\\kappa_m\\)\ntheta[4]\nMineral bulk modulus (GPa)\n\n\n\\(\\mu_m\\)\ntheta[5]\nMineral shear modulus (GPa)\n\n\n\\(\\rho_m\\)\ntheta[6]\nMineral density (kg/m\\(^3\\))\n\n\n\n\n\n\n\n\nBerryman self-consistent rock physics model\n\n\nShow Berryman functions\nberry_scm <- function(k, mu, asp, x, ro1, P_water) {\n  # BERRYSCM - Effective elastic moduli for multi-component composite\n  # using Berryman's Self-Consistent (Coherent Potential Approximation) method.\n  # \n  # Arguments:\n  # k: Bulk moduli of the N constituent phases (numeric vector)\n  # mu: Shear moduli of the N constituent phases (numeric vector)\n  # asp: Aspect ratio for the inclusions of the N phases (numeric vector)\n  # x: Fraction of each phase. Solid, then fluid phase (numeric vector)\n  # ro1: Density of the original rock (numeric scalar)\n  # P_water: Proportion of water (numeric scalar)\n  #\n  # Returns:\n  # kbr: Effective bulk modulus (numeric scalar)\n  # mubr: Effective shear modulus (numeric scalar)\n  # vp: P-wave velocity (numeric scalar)\n  # vs: S-wave velocity (numeric scalar)\n  # ro2: Final rock density after fluid substitution (numeric scalar)\n  # k2: Rock bulk modulus after fluid substitution (numeric scalar)\n  \n  # Ensure inputs are column vectors (equivalent to MATLAB's k(:), mu(:), etc.)\n  k <- as.vector(k)\n  mu <- as.vector(mu)\n  asp <- as.vector(asp)\n  x <- as.vector(x)\n  \n  # Modify aspect ratios of inclusions that are equal to 1\n  asp[asp == 1] <- 0.99\n  \n  # Initialize variables\n  theta <- numeric(length(asp))\n  fn <- numeric(length(asp))\n  \n  # Compute theta and fn for oblate and prolate spheroids\n  obdx <- which(asp < 1)\n  prdx <- which(asp > 1)\n  \n  theta[obdx] <- (asp[obdx] / ((1 - asp[obdx]^2)^(3/2))) * (acos(asp[obdx]) - asp[obdx] * sqrt(1 - asp[obdx]^2))\n  fn[obdx] <- (asp[obdx]^2 / (1 - asp[obdx]^2)) * (3 * theta[obdx] - 2)\n  \n  theta[prdx] <- (asp[prdx] / ((asp[prdx]^2 - 1)^(3/2))) * (asp[prdx] * sqrt(asp[prdx]^2 - 1) - acosh(asp[prdx]))\n  fn[prdx] <- (asp[prdx]^2 / (asp[prdx]^2 - 1)) * (2 - 3 * theta[prdx])\n  \n  # Initialize initial bulk and shear moduli\n  ksc <- sum(k * x)\n  musc <- sum(mu * x)\n  \n  # Initialize iteration parameters\n  knew <- 0\n  tol <- 1e-6 * k[1]\n  del <- abs(ksc - knew)\n  niter <- 0\n  \n  # Iterative solution for effective moduli\n  while (del > abs(tol) && niter < 3000) {\n    nusc <- (3 * ksc - 2 * musc) / (2 * (3 * ksc + musc))\n    a <- mu / musc - 1\n    b <- (1 / 3) * (k / ksc - mu / musc)\n    r <- (1 - 2 * nusc) / (2 * (1 - nusc))\n    \n    f1 <- 1 + a * ((3 / 2) * (fn + theta) - r * ((3 / 2) * fn + (5 / 2) * theta - (4 / 3)))\n    f2 <- 1 + a * (1 + (3 / 2) * (fn + theta) - (r / 2) * (3 * fn + 5 * theta)) + b * (3 - 4 * r)\n    f2 <- f2 + (a / 2) * (a + 3 * b) * (3 - 4 * r) * (fn + theta - r * (fn - theta + 2 * theta^2))\n    f3 <- 1 + a * (1 - (fn + (3 / 2) * theta) + r * (fn + theta))\n    f4 <- 1 + (a / 4) * (fn + 3 * theta - r * (fn - theta))\n    f5 <- a * (-fn + r * (fn + theta - (4 / 3))) + b * theta * (3 - 4 * r)\n    f6 <- 1 + a * (1 + fn - r * (fn + theta)) + b * (1 - theta) * (3 - 4 * r)\n    f7 <- 2 + (a / 4) * (3 * fn + 9 * theta - r * (3 * fn + 5 * theta)) + b * theta * (3 - 4 * r)\n    f8 <- a * (1 - 2 * r + (fn / 2) * (r - 1) + (theta / 2) * (5 * r - 3)) + b * (1 - theta) * (3 - 4 * r)\n    f9 <- a * ((r - 1) * fn - r * theta) + b * theta * (3 - 4 * r)\n    \n    p <- 3 * f1 / f2\n    q <- (2 / f3) + (1 / f4) + ((f4 * f5 + f6 * f7 - f8 * f9) / (f2 * f4))\n    \n    p <- p / 3\n    q <- q / 5\n    \n    # Update moduli\n    knew <- sum(x * k * p) / sum(x * p)\n    munew <- sum(x * mu * q) / sum(x * q)\n    \n    del <- abs(ksc - knew)\n    ksc <- knew\n    musc <- munew\n    niter <- niter + 1\n  }\n  \n  kbr <- ksc\n  mubr <- musc\n  \n  # Density and fluid substitution\n  rofl1 <- 0.020    # density of gas\n  kfl1 <- 0         # bulk modulus of gas\n  ro_water <- 1000   # density of water\n  k_water <- 2.2e9  # bulk modulus of water\n  P_gas <- 1 - P_water\n  rofl2 <- P_gas * rofl1 + P_water * ro_water\n  kfl2 <- P_gas * kfl1 + P_water * k_water\n  k0 <- k[1]        # bulk modulus of solid mineral phase\n  phi <- x[2]       # porosity of rock\n  k1 <- kbr         # dry bulk modulus\n  \n  # Perform fluid substitution using Gassmann equation\n  a <- k1 / (k0 - k1) - kfl1 / (phi * (k0 - kfl1)) + kfl2 / (phi * (k0 - kfl2))\n  k2 <- k0 * a / (1 + a)   # bulk modulus after fluid substitution\n  \n  # Compute final density after fluid substitution\n  ro2 <- ro1 - phi * rofl1 + phi * rofl2\n  \n  # Compute seismic velocities after fluid substitution\n  mu2 <- mubr\n  vp <- sqrt((k2 + (4 / 3) * mu2) / ro2)\n  vs <- sqrt(mu2 / ro2)\n  \n  return(list(kbr = kbr, mubr = mubr, vp = vp, vs = vs, ro2 = ro2, k2 = k2))\n}\n\nmyBerry <- function(theta, H = 3) {\n  # Extract parameters from theta\n  asp <- c(1, theta[1])   # Aspect ratio\n  x_phi <- theta[2]       # Proportion of fluid (phi)\n  rock_vol <- 1 - theta[2] # Volume of solid phase (rock)\n  x <- c(rock_vol, x_phi)  # Fraction of phases\n  rock_density <- theta[6] * rock_vol  # Density of solid phase (basalt)\n  gas_density <- 0.020 * x_phi         # Density of fluid phase (gas)\n  rhob1 <- rock_density + gas_density  # Bulk density\n  \n  # Percentage of water in pore space\n  P_water <- theta[3]\n  \n  # Bulk and shear moduli (scaled by 1e9 as per MATLAB code)\n  k <- c(theta[4] * 1e9, 0)  # Bulk modulus (first element)\n  mu <- c(theta[5] * 1e9, 0) # Shear modulus (first element)\n  \n  # Call berryscm function (ensure berryscm function is defined in your R environment)\n  result <- berry_scm(k, mu, asp, x, rhob1, P_water)\n  \n  # Return the required output from berryscm\n  return(result)\n}"
  },
  {
    "objectID": "notes/waterOnMars.html#liquid-water-in-the-martian-mid-crust",
    "href": "notes/waterOnMars.html#liquid-water-in-the-martian-mid-crust",
    "title": "Water on Mars",
    "section": "Liquid water in the Martian mid-crust",
    "text": "Liquid water in the Martian mid-crust\n\nThe Interior Exploration using Seismic Investigations, Geodesy and Heat Transport (InSight) mission from NASA sent a robotic lander to Mars on May 5, 2018. The goal of the mission, which ended December 2022, was to study the crust, mantle and core of Mars.\n\n\nCode\nlibrary(tidyverse)\n\n\n\nData\nWatch the video here to see the data that InSight collects from earthquakes on Mars.\n\nseismic_data = data.frame(y = c(4.1, 2.5, 2589),\n                          sd = c(0.2, 0.3, 157))\n\n\n\nParameter Table\nHere our parameter vector \\(\\theta = (\\alpha, \\phi, \\gamma_w, \\kappa_m, \\mu_m, \\rho_m)\\). These parameters describe the medium through which the earthquake waves propagate.\n\n\n\nSymbol in paper\nCode\nDefinition\n\n\n\n\n\\(\\alpha\\)\ntheta[1]\nPore shape aspect ratio\n\n\n\\(\\phi\\)\ntheta[2]\nPorosity\n\n\n\\(\\gamma_w\\)\ntheta[3]\nWater saturation (%)\n\n\n\\(\\kappa_m\\)\ntheta[4]\nMineral bulk modulus (GPa)\n\n\n\\(\\mu_m\\)\ntheta[5]\nMineral shear modulus (GPa)\n\n\n\\(\\rho_m\\)\ntheta[6]\nMineral density (kg/m\\(^3\\))\n\n\n\n\n\n\n\n\nBerryman self-consistent rock physics model\nThe function \\(f(\\theta)\\) that forward simulates data given our set of parameters is myBerry in the code below. This function serves as a wrapper to the berry_scm function, also defined below.\n\n\nShow Berryman functions\nberry_scm <- function(k, mu, asp, x, ro1, P_water) {\n  # BERRYSCM - Effective elastic moduli for multi-component composite\n  # using Berryman's Self-Consistent (Coherent Potential Approximation) method.\n  # \n  # Arguments:\n  # k: Bulk moduli of the N constituent phases (numeric vector)\n  # mu: Shear moduli of the N constituent phases (numeric vector)\n  # asp: Aspect ratio for the inclusions of the N phases (numeric vector)\n  # x: Fraction of each phase. Solid, then fluid phase (numeric vector)\n  # ro1: Density of the original rock (numeric scalar)\n  # P_water: Proportion of water (numeric scalar)\n  #\n  # Returns:\n  # kbr: Effective bulk modulus (numeric scalar)\n  # mubr: Effective shear modulus (numeric scalar)\n  # vp: P-wave velocity (numeric scalar)\n  # vs: S-wave velocity (numeric scalar)\n  # ro2: Final rock density after fluid substitution (numeric scalar)\n  # k2: Rock bulk modulus after fluid substitution (numeric scalar)\n  \n  # Ensure inputs are column vectors (equivalent to MATLAB's k(:), mu(:), etc.)\n  k <- as.vector(k)\n  mu <- as.vector(mu)\n  asp <- as.vector(asp)\n  x <- as.vector(x)\n  \n  # Modify aspect ratios of inclusions that are equal to 1\n  asp[asp == 1] <- 0.99\n  \n  # Initialize variables\n  theta <- numeric(length(asp))\n  fn <- numeric(length(asp))\n  \n  # Compute theta and fn for oblate and prolate spheroids\n  obdx <- which(asp < 1)\n  prdx <- which(asp > 1)\n  \n  theta[obdx] <- (asp[obdx] / ((1 - asp[obdx]^2)^(3/2))) * (acos(asp[obdx]) - asp[obdx] * sqrt(1 - asp[obdx]^2))\n  fn[obdx] <- (asp[obdx]^2 / (1 - asp[obdx]^2)) * (3 * theta[obdx] - 2)\n  \n  theta[prdx] <- (asp[prdx] / ((asp[prdx]^2 - 1)^(3/2))) * (asp[prdx] * sqrt(asp[prdx]^2 - 1) - acosh(asp[prdx]))\n  fn[prdx] <- (asp[prdx]^2 / (asp[prdx]^2 - 1)) * (2 - 3 * theta[prdx])\n  \n  # Initialize initial bulk and shear moduli\n  ksc <- sum(k * x)\n  musc <- sum(mu * x)\n  \n  # Initialize iteration parameters\n  knew <- 0\n  tol <- 1e-6 * k[1]\n  del <- abs(ksc - knew)\n  niter <- 0\n  \n  # Iterative solution for effective moduli\n  while (del > abs(tol) && niter < 3000) {\n    nusc <- (3 * ksc - 2 * musc) / (2 * (3 * ksc + musc))\n    a <- mu / musc - 1\n    b <- (1 / 3) * (k / ksc - mu / musc)\n    r <- (1 - 2 * nusc) / (2 * (1 - nusc))\n    \n    f1 <- 1 + a * ((3 / 2) * (fn + theta) - r * ((3 / 2) * fn + (5 / 2) * theta - (4 / 3)))\n    f2 <- 1 + a * (1 + (3 / 2) * (fn + theta) - (r / 2) * (3 * fn + 5 * theta)) + b * (3 - 4 * r)\n    f2 <- f2 + (a / 2) * (a + 3 * b) * (3 - 4 * r) * (fn + theta - r * (fn - theta + 2 * theta^2))\n    f3 <- 1 + a * (1 - (fn + (3 / 2) * theta) + r * (fn + theta))\n    f4 <- 1 + (a / 4) * (fn + 3 * theta - r * (fn - theta))\n    f5 <- a * (-fn + r * (fn + theta - (4 / 3))) + b * theta * (3 - 4 * r)\n    f6 <- 1 + a * (1 + fn - r * (fn + theta)) + b * (1 - theta) * (3 - 4 * r)\n    f7 <- 2 + (a / 4) * (3 * fn + 9 * theta - r * (3 * fn + 5 * theta)) + b * theta * (3 - 4 * r)\n    f8 <- a * (1 - 2 * r + (fn / 2) * (r - 1) + (theta / 2) * (5 * r - 3)) + b * (1 - theta) * (3 - 4 * r)\n    f9 <- a * ((r - 1) * fn - r * theta) + b * theta * (3 - 4 * r)\n    \n    p <- 3 * f1 / f2\n    q <- (2 / f3) + (1 / f4) + ((f4 * f5 + f6 * f7 - f8 * f9) / (f2 * f4))\n    \n    p <- p / 3\n    q <- q / 5\n    \n    # Update moduli\n    knew <- sum(x * k * p) / sum(x * p)\n    munew <- sum(x * mu * q) / sum(x * q)\n    \n    del <- abs(ksc - knew)\n    ksc <- knew\n    musc <- munew\n    niter <- niter + 1\n  }\n  \n  kbr <- ksc\n  mubr <- musc\n  \n  # Density and fluid substitution\n  rofl1 <- 0.020    # density of gas\n  kfl1 <- 0         # bulk modulus of gas\n  ro_water <- 1000   # density of water\n  k_water <- 2.2e9  # bulk modulus of water\n  P_gas <- 1 - P_water\n  rofl2 <- P_gas * rofl1 + P_water * ro_water\n  kfl2 <- P_gas * kfl1 + P_water * k_water\n  k0 <- k[1]        # bulk modulus of solid mineral phase\n  phi <- x[2]       # porosity of rock\n  k1 <- kbr         # dry bulk modulus\n  \n  # Perform fluid substitution using Gassmann equation\n  a <- k1 / (k0 - k1) - kfl1 / (phi * (k0 - kfl1)) + kfl2 / (phi * (k0 - kfl2))\n  k2 <- k0 * a / (1 + a)   # bulk modulus after fluid substitution\n  \n  # Compute final density after fluid substitution\n  ro2 <- ro1 - phi * rofl1 + phi * rofl2\n  \n  # Compute seismic velocities after fluid substitution\n  mu2 <- mubr\n  vp <- sqrt((k2 + (4 / 3) * mu2) / ro2)\n  vs <- sqrt(mu2 / ro2)\n  \n  return(list(kbr = kbr, mubr = mubr, vp = vp, vs = vs, ro2 = ro2, k2 = k2))\n}\n\nmyBerry <- function(theta, H = 3) {\n  # Extract parameters from theta\n  asp <- c(1, theta[1])   # Aspect ratio\n  x_phi <- theta[2]       # Proportion of fluid (phi)\n  rock_vol <- 1 - theta[2] # Volume of solid phase (rock)\n  x <- c(rock_vol, x_phi)  # Fraction of phases\n  rock_density <- theta[6] * rock_vol  # Density of solid phase (basalt)\n  gas_density <- 0.020 * x_phi         # Density of fluid phase (gas)\n  rhob1 <- rock_density + gas_density  # Bulk density\n  \n  # Percentage of water in pore space\n  P_water <- theta[3]\n  \n  # Bulk and shear moduli (scaled by 1e9 as per MATLAB code)\n  k <- c(theta[4] * 1e9, 0)  # Bulk modulus (first element)\n  mu <- c(theta[5] * 1e9, 0) # Shear modulus (first element)\n  \n  # Call berryscm function (ensure berryscm function is defined in your R environment)\n  result <- berry_scm(k, mu, asp, x, rhob1, P_water)\n  \n  # Return the required output from berryscm\n  return(result)\n}"
  },
  {
    "objectID": "notes/waterOnMars.html#example-liquid-water-in-the-martian-mid-crust",
    "href": "notes/waterOnMars.html#example-liquid-water-in-the-martian-mid-crust",
    "title": "Water on Mars",
    "section": "Example: Liquid water in the Martian mid-crust",
    "text": "Example: Liquid water in the Martian mid-crust\n\nThe Interior Exploration using Seismic Investigations, Geodesy and Heat Transport (InSight) mission from NASA sent a robotic lander to Mars on May 5, 2018. The goal of the mission, which ended December 2022, was to study the crust, mantle and core of Mars.\n\n\nCode\nlibrary(tidyverse)\n\n\n\nData\nWatch the video here to see the data that InSight collects from earthquakes on Mars.\n\nseismic_data = data.frame(y = c(4.1, 2.5, 2589),\n                          sd = c(0.2, 0.3, 157))\n\n\n\nParameter Table\nHere our parameter vector \\(\\theta = (\\alpha, \\phi, \\gamma_w, \\kappa_m, \\mu_m, \\rho_m)\\). These parameters describe the medium through which the earthquake waves propagate.\n\n\n\nSymbol in paper\nCode\nDefinition\n\n\n\n\n\\(\\alpha\\)\ntheta[1]\nPore shape aspect ratio\n\n\n\\(\\phi\\)\ntheta[2]\nPorosity\n\n\n\\(\\gamma_w\\)\ntheta[3]\nWater saturation (%)\n\n\n\\(\\kappa_m\\)\ntheta[4]\nMineral bulk modulus (GPa)\n\n\n\\(\\mu_m\\)\ntheta[5]\nMineral shear modulus (GPa)\n\n\n\\(\\rho_m\\)\ntheta[6]\nMineral density (kg/m\\(^3\\))\n\n\n\n\n\n\n\n\nBerryman self-consistent rock physics model\nThe function \\(f(\\theta)\\) that forward simulates data given our set of parameters is myBerry in the code below. This function serves as a wrapper to the berry_scm function, also defined below.\n\n\nShow Berryman functions\nberry_scm <- function(k, mu, asp, x, ro1, P_water) {\n  # BERRYSCM - Effective elastic moduli for multi-component composite\n  # using Berryman's Self-Consistent (Coherent Potential Approximation) method.\n  # \n  # Arguments:\n  # k: Bulk moduli of the N constituent phases (numeric vector)\n  # mu: Shear moduli of the N constituent phases (numeric vector)\n  # asp: Aspect ratio for the inclusions of the N phases (numeric vector)\n  # x: Fraction of each phase. Solid, then fluid phase (numeric vector)\n  # ro1: Density of the original rock (numeric scalar)\n  # P_water: Proportion of water (numeric scalar)\n  #\n  # Returns:\n  # kbr: Effective bulk modulus (numeric scalar)\n  # mubr: Effective shear modulus (numeric scalar)\n  # vp: P-wave velocity (numeric scalar)\n  # vs: S-wave velocity (numeric scalar)\n  # ro2: Final rock density after fluid substitution (numeric scalar)\n  # k2: Rock bulk modulus after fluid substitution (numeric scalar)\n  \n  # Ensure inputs are column vectors (equivalent to MATLAB's k(:), mu(:), etc.)\n  k <- as.vector(k)\n  mu <- as.vector(mu)\n  asp <- as.vector(asp)\n  x <- as.vector(x)\n  \n  # Modify aspect ratios of inclusions that are equal to 1\n  asp[asp == 1] <- 0.99\n  \n  # Initialize variables\n  theta <- numeric(length(asp))\n  fn <- numeric(length(asp))\n  \n  # Compute theta and fn for oblate and prolate spheroids\n  obdx <- which(asp < 1)\n  prdx <- which(asp > 1)\n  \n  theta[obdx] <- (asp[obdx] / ((1 - asp[obdx]^2)^(3/2))) * (acos(asp[obdx]) - asp[obdx] * sqrt(1 - asp[obdx]^2))\n  fn[obdx] <- (asp[obdx]^2 / (1 - asp[obdx]^2)) * (3 * theta[obdx] - 2)\n  \n  theta[prdx] <- (asp[prdx] / ((asp[prdx]^2 - 1)^(3/2))) * (asp[prdx] * sqrt(asp[prdx]^2 - 1) - acosh(asp[prdx]))\n  fn[prdx] <- (asp[prdx]^2 / (asp[prdx]^2 - 1)) * (2 - 3 * theta[prdx])\n  \n  # Initialize initial bulk and shear moduli\n  ksc <- sum(k * x)\n  musc <- sum(mu * x)\n  \n  # Initialize iteration parameters\n  knew <- 0\n  tol <- 1e-6 * k[1]\n  del <- abs(ksc - knew)\n  niter <- 0\n  \n  # Iterative solution for effective moduli\n  while (del > abs(tol) && niter < 3000) {\n    nusc <- (3 * ksc - 2 * musc) / (2 * (3 * ksc + musc))\n    a <- mu / musc - 1\n    b <- (1 / 3) * (k / ksc - mu / musc)\n    r <- (1 - 2 * nusc) / (2 * (1 - nusc))\n    \n    f1 <- 1 + a * ((3 / 2) * (fn + theta) - r * ((3 / 2) * fn + (5 / 2) * theta - (4 / 3)))\n    f2 <- 1 + a * (1 + (3 / 2) * (fn + theta) - (r / 2) * (3 * fn + 5 * theta)) + b * (3 - 4 * r)\n    f2 <- f2 + (a / 2) * (a + 3 * b) * (3 - 4 * r) * (fn + theta - r * (fn - theta + 2 * theta^2))\n    f3 <- 1 + a * (1 - (fn + (3 / 2) * theta) + r * (fn + theta))\n    f4 <- 1 + (a / 4) * (fn + 3 * theta - r * (fn - theta))\n    f5 <- a * (-fn + r * (fn + theta - (4 / 3))) + b * theta * (3 - 4 * r)\n    f6 <- 1 + a * (1 + fn - r * (fn + theta)) + b * (1 - theta) * (3 - 4 * r)\n    f7 <- 2 + (a / 4) * (3 * fn + 9 * theta - r * (3 * fn + 5 * theta)) + b * theta * (3 - 4 * r)\n    f8 <- a * (1 - 2 * r + (fn / 2) * (r - 1) + (theta / 2) * (5 * r - 3)) + b * (1 - theta) * (3 - 4 * r)\n    f9 <- a * ((r - 1) * fn - r * theta) + b * theta * (3 - 4 * r)\n    \n    p <- 3 * f1 / f2\n    q <- (2 / f3) + (1 / f4) + ((f4 * f5 + f6 * f7 - f8 * f9) / (f2 * f4))\n    \n    p <- p / 3\n    q <- q / 5\n    \n    # Update moduli\n    knew <- sum(x * k * p) / sum(x * p)\n    munew <- sum(x * mu * q) / sum(x * q)\n    \n    del <- abs(ksc - knew)\n    ksc <- knew\n    musc <- munew\n    niter <- niter + 1\n  }\n  \n  kbr <- ksc\n  mubr <- musc\n  \n  # Density and fluid substitution\n  rofl1 <- 0.020    # density of gas\n  kfl1 <- 0         # bulk modulus of gas\n  ro_water <- 1000   # density of water\n  k_water <- 2.2e9  # bulk modulus of water\n  P_gas <- 1 - P_water\n  rofl2 <- P_gas * rofl1 + P_water * ro_water\n  kfl2 <- P_gas * kfl1 + P_water * k_water\n  k0 <- k[1]        # bulk modulus of solid mineral phase\n  phi <- x[2]       # porosity of rock\n  k1 <- kbr         # dry bulk modulus\n  \n  # Perform fluid substitution using Gassmann equation\n  a <- k1 / (k0 - k1) - kfl1 / (phi * (k0 - kfl1)) + kfl2 / (phi * (k0 - kfl2))\n  k2 <- k0 * a / (1 + a)   # bulk modulus after fluid substitution\n  \n  # Compute final density after fluid substitution\n  ro2 <- ro1 - phi * rofl1 + phi * rofl2\n  \n  # Compute seismic velocities after fluid substitution\n  mu2 <- mubr\n  vp <- sqrt((k2 + (4 / 3) * mu2) / ro2)\n  vs <- sqrt(mu2 / ro2)\n  \n  return(list(kbr = kbr, mubr = mubr, vp = vp, vs = vs, ro2 = ro2, k2 = k2))\n}\n\nmyBerry <- function(theta, H = 3) {\n  # Extract parameters from theta\n  asp <- c(1, theta[1])   # Aspect ratio\n  x_phi <- theta[2]       # Proportion of fluid (phi)\n  rock_vol <- 1 - theta[2] # Volume of solid phase (rock)\n  x <- c(rock_vol, x_phi)  # Fraction of phases\n  rock_density <- theta[6] * rock_vol  # Density of solid phase (basalt)\n  gas_density <- 0.020 * x_phi         # Density of fluid phase (gas)\n  rhob1 <- rock_density + gas_density  # Bulk density\n  \n  # Percentage of water in pore space\n  P_water <- theta[3]\n  \n  # Bulk and shear moduli (scaled by 1e9 as per MATLAB code)\n  k <- c(theta[4] * 1e9, 0)  # Bulk modulus (first element)\n  mu <- c(theta[5] * 1e9, 0) # Shear modulus (first element)\n  \n  # Call berryscm function (ensure berryscm function is defined in your R environment)\n  result <- berry_scm(k, mu, asp, x, rhob1, P_water)\n  \n  # Return the required output from berryscm\n  return(result)\n}"
  },
  {
    "objectID": "notes/waterOnMars.html#example-liquid-water-in-the-martian-mid-crust-1",
    "href": "notes/waterOnMars.html#example-liquid-water-in-the-martian-mid-crust-1",
    "title": "Water on Mars",
    "section": "Example: Liquid water in the Martian mid-crust 1",
    "text": "Example: Liquid water in the Martian mid-crust 1\n\n\n\nInSight. Photo credit: NASA. Image retrieved from https://science.nasa.gov/mission/insight/\n\n\nThe Interior Exploration using Seismic Investigations, Geodesy and Heat Transport (InSight) mission from NASA sent a robotic lander to Mars on May 5, 2018. The goal of the mission, which ended December 2022, was to study the crust, mantle and core of Mars.\n\nData\nWatch the video here to see the data that InSight collects from earthquakes on Mars.\n\nseismic_data = data.frame(y = c(4.1, 2.5, 2589),\n                          sd = c(0.2, 0.3, 157))\n\n\n\nParameter Table\nHere our parameter vector \\(\\theta = (\\alpha, \\phi, \\gamma_w, \\kappa_m, \\mu_m, \\rho_m)\\). These parameters describe the medium through which the earthquake waves propagate.\n\n\n\nSymbol in paper\nCode\nDefinition\n\n\n\n\n\\(\\alpha\\)\ntheta[1]\nPore shape aspect ratio\n\n\n\\(\\phi\\)\ntheta[2]\nPorosity\n\n\n\\(\\gamma_w\\)\ntheta[3]\nWater saturation (%)\n\n\n\\(\\kappa_m\\)\ntheta[4]\nMineral bulk modulus (GPa)\n\n\n\\(\\mu_m\\)\ntheta[5]\nMineral shear modulus (GPa)\n\n\n\\(\\rho_m\\)\ntheta[6]\nMineral density (kg/m\\(^3\\))\n\n\n\n\n\n\n\n\nBerryman self-consistent rock physics model\nThe function \\(f(\\theta)\\) that forward simulates data given our set of parameters is myBerry in the code below. This function serves as a wrapper to the berry_scm function, also defined below. Functions are translated into R from original MATLAB code by Wright, Morzfeld and Manga from the paper and publicly available on GitHub. 2\n\n\nShow Berryman functions\nberry_scm <- function(k, mu, asp, x, ro1, P_water) {\n  # BERRYSCM - Effective elastic moduli for multi-component composite\n  # using Berryman's Self-Consistent (Coherent Potential Approximation) method.\n  # \n  # Arguments:\n  # k: Bulk moduli of the N constituent phases (numeric vector)\n  # mu: Shear moduli of the N constituent phases (numeric vector)\n  # asp: Aspect ratio for the inclusions of the N phases (numeric vector)\n  # x: Fraction of each phase. Solid, then fluid phase (numeric vector)\n  # ro1: Density of the original rock (numeric scalar)\n  # P_water: Proportion of water (numeric scalar)\n  #\n  # Returns:\n  # kbr: Effective bulk modulus (numeric scalar)\n  # mubr: Effective shear modulus (numeric scalar)\n  # vp: P-wave velocity (numeric scalar)\n  # vs: S-wave velocity (numeric scalar)\n  # ro2: Final rock density after fluid substitution (numeric scalar)\n  # k2: Rock bulk modulus after fluid substitution (numeric scalar)\n  \n  # Ensure inputs are column vectors (equivalent to MATLAB's k(:), mu(:), etc.)\n  k <- as.vector(k)\n  mu <- as.vector(mu)\n  asp <- as.vector(asp)\n  x <- as.vector(x)\n  \n  # Modify aspect ratios of inclusions that are equal to 1\n  asp[asp == 1] <- 0.99\n  \n  # Initialize variables\n  theta <- numeric(length(asp))\n  fn <- numeric(length(asp))\n  \n  # Compute theta and fn for oblate and prolate spheroids\n  obdx <- which(asp < 1)\n  prdx <- which(asp > 1)\n  \n  theta[obdx] <- (asp[obdx] / ((1 - asp[obdx]^2)^(3/2))) * (acos(asp[obdx]) - asp[obdx] * sqrt(1 - asp[obdx]^2))\n  fn[obdx] <- (asp[obdx]^2 / (1 - asp[obdx]^2)) * (3 * theta[obdx] - 2)\n  \n  theta[prdx] <- (asp[prdx] / ((asp[prdx]^2 - 1)^(3/2))) * (asp[prdx] * sqrt(asp[prdx]^2 - 1) - acosh(asp[prdx]))\n  fn[prdx] <- (asp[prdx]^2 / (asp[prdx]^2 - 1)) * (2 - 3 * theta[prdx])\n  \n  # Initialize initial bulk and shear moduli\n  ksc <- sum(k * x)\n  musc <- sum(mu * x)\n  \n  # Initialize iteration parameters\n  knew <- 0\n  tol <- 1e-6 * k[1]\n  del <- abs(ksc - knew)\n  niter <- 0\n  \n  # Iterative solution for effective moduli\n  while (del > abs(tol) && niter < 3000) {\n    nusc <- (3 * ksc - 2 * musc) / (2 * (3 * ksc + musc))\n    a <- mu / musc - 1\n    b <- (1 / 3) * (k / ksc - mu / musc)\n    r <- (1 - 2 * nusc) / (2 * (1 - nusc))\n    \n    f1 <- 1 + a * ((3 / 2) * (fn + theta) - r * ((3 / 2) * fn + (5 / 2) * theta - (4 / 3)))\n    f2 <- 1 + a * (1 + (3 / 2) * (fn + theta) - (r / 2) * (3 * fn + 5 * theta)) + b * (3 - 4 * r)\n    f2 <- f2 + (a / 2) * (a + 3 * b) * (3 - 4 * r) * (fn + theta - r * (fn - theta + 2 * theta^2))\n    f3 <- 1 + a * (1 - (fn + (3 / 2) * theta) + r * (fn + theta))\n    f4 <- 1 + (a / 4) * (fn + 3 * theta - r * (fn - theta))\n    f5 <- a * (-fn + r * (fn + theta - (4 / 3))) + b * theta * (3 - 4 * r)\n    f6 <- 1 + a * (1 + fn - r * (fn + theta)) + b * (1 - theta) * (3 - 4 * r)\n    f7 <- 2 + (a / 4) * (3 * fn + 9 * theta - r * (3 * fn + 5 * theta)) + b * theta * (3 - 4 * r)\n    f8 <- a * (1 - 2 * r + (fn / 2) * (r - 1) + (theta / 2) * (5 * r - 3)) + b * (1 - theta) * (3 - 4 * r)\n    f9 <- a * ((r - 1) * fn - r * theta) + b * theta * (3 - 4 * r)\n    \n    p <- 3 * f1 / f2\n    q <- (2 / f3) + (1 / f4) + ((f4 * f5 + f6 * f7 - f8 * f9) / (f2 * f4))\n    \n    p <- p / 3\n    q <- q / 5\n    \n    # Update moduli\n    knew <- sum(x * k * p) / sum(x * p)\n    munew <- sum(x * mu * q) / sum(x * q)\n    \n    del <- abs(ksc - knew)\n    ksc <- knew\n    musc <- munew\n    niter <- niter + 1\n  }\n  \n  kbr <- ksc\n  mubr <- musc\n  \n  # Density and fluid substitution\n  rofl1 <- 0.020    # density of gas\n  kfl1 <- 0         # bulk modulus of gas\n  ro_water <- 1000   # density of water\n  k_water <- 2.2e9  # bulk modulus of water\n  P_gas <- 1 - P_water\n  rofl2 <- P_gas * rofl1 + P_water * ro_water\n  kfl2 <- P_gas * kfl1 + P_water * k_water\n  k0 <- k[1]        # bulk modulus of solid mineral phase\n  phi <- x[2]       # porosity of rock\n  k1 <- kbr         # dry bulk modulus\n  \n  # Perform fluid substitution using Gassmann equation\n  a <- k1 / (k0 - k1) - kfl1 / (phi * (k0 - kfl1)) + kfl2 / (phi * (k0 - kfl2))\n  k2 <- k0 * a / (1 + a)   # bulk modulus after fluid substitution\n  \n  # Compute final density after fluid substitution\n  ro2 <- ro1 - phi * rofl1 + phi * rofl2\n  \n  # Compute seismic velocities after fluid substitution\n  mu2 <- mubr\n  vp <- sqrt((k2 + (4 / 3) * mu2) / ro2)\n  vs <- sqrt(mu2 / ro2)\n  \n  return(list(kbr = kbr, mubr = mubr, vp = vp, vs = vs, ro2 = ro2, k2 = k2))\n}\n\nmyBerry <- function(theta, H = 3) {\n  # Extract parameters from theta\n  asp <- c(1, theta[1])   # Aspect ratio\n  x_phi <- theta[2]       # Proportion of fluid (phi)\n  rock_vol <- 1 - theta[2] # Volume of solid phase (rock)\n  x <- c(rock_vol, x_phi)  # Fraction of phases\n  rock_density <- theta[6] * rock_vol  # Density of solid phase (basalt)\n  gas_density <- 0.020 * x_phi         # Density of fluid phase (gas)\n  rhob1 <- rock_density + gas_density  # Bulk density\n  \n  # Percentage of water in pore space\n  P_water <- theta[3]\n  \n  # Bulk and shear moduli (scaled by 1e9 as per MATLAB code)\n  k <- c(theta[4] * 1e9, 0)  # Bulk modulus (first element)\n  mu <- c(theta[5] * 1e9, 0) # Shear modulus (first element)\n  \n  # Call berryscm function (ensure berryscm function is defined in your R environment)\n  result <- berry_scm(k, mu, asp, x, rhob1, P_water)\n  \n  # Return the required output from berryscm\n  return(result)\n}\n\n\n\n\n\n\n\n\n\n\nLikelihood and prior\n\nExercise\n\n\nWhy does a normal likelihood make sense?\n\nWhat priors are chosen for each unknown?"
  },
  {
    "objectID": "notes/waterOnMars.html#priors",
    "href": "notes/waterOnMars.html#priors",
    "title": "Water on Mars",
    "section": "Priors",
    "text": "Priors\n\nExercise\n\n\nWhat priors are chosen for each unknown?"
  },
  {
    "objectID": "notes/waterOnMars.html#discussion",
    "href": "notes/waterOnMars.html#discussion",
    "title": "Water on Mars",
    "section": "Discussion",
    "text": "Discussion\n\nPrior sensitivity\n\nExercise\n\n\nDiscuss with a neighbor: do you think the results are sensitive to the choice of prior?\n\n\n\n\nAlternate priorMarginal posteriors\n\n\n\n\\(beta(1/2, 1/2)\\) prior on water saturation.\n\n\nlogPosterior = function(theta) {\n  result = myBerry(theta)\n  y = c(result$vp / 1000, result$vs / 1000, result$ro2)\n  if(result$vp < result$vs) {\n    return(-Inf)\n  }\n  else{\n  return(\n    sum(dnorm(y, mean = seismic_data$y, sd = seismic_data$sd,\n            log = TRUE)) + \n      dbeta(theta[3], .5, .5, log = TRUE)\n  )\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel selection\nLet \\(M_1\\) be the hypothesis that there is water present under the surface of Mars and let \\(M_0\\) be the hypothesis that there is not. Really, we want to select between these two hypotheses.\nTo do this, consider the ratio of posterior probabilities\n\\[\n\\begin{aligned}\n\\underbrace{\\frac{Pr(M_1 | data)}{Pr(M_0 |data)}}_{\\text{posterior odds}}\n=\n\\underbrace{\\frac{p({data} | M_1)}{p(data | M_0}}_{\\text{Bayes factor}}\n\\underbrace{\\frac{p(M_1)}{p(M_0)}}_{\\text{prior odds}}\n\\end{aligned}\n\\]\nThe Bayes factor tells us how the data supports one hypothesis over another. Notice that we cannot compute the Bayes factor directly. We must expand,\n\\[\n\\begin{aligned}\np(data | M_i) &= \\int p(data, \\theta | M_i) d\\theta\\\\\n&= \\int p(data | \\theta) p(\\theta | M_i) d \\theta\n\\end{aligned}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nNotice that the Bayes factor does depend on the prior \\(p(\\theta | M_i)\\)."
  },
  {
    "objectID": "notes/waterOnMars.html#results-1",
    "href": "notes/waterOnMars.html#results-1",
    "title": "Water on Mars",
    "section": "Results",
    "text": "Results\n\nBelow I assume \\(M_0\\) is true, i.e.¬†there is no water on Mars and thus theta3 is fixed to 0. I sample from the posterior of the other five parameters.\n\n\nShameless code duplication\nset.seed(360)\ndtruncnormL = function(x, a, b, mean, sd) {\n  log(dtruncnorm(x, a=a, b=b, mean = mean, sd = sd))\n}\n\nrcnorm<-function(n, mean=0, sd=1, a=-Inf, b=Inf){\n  u = runif(n, pnorm((a - mean) / sd), pnorm((b - mean) / sd))\n  mean + (sd * qnorm(u))\n}\n\nlogPosterior = function(theta) {\n  result = myBerry(theta)\n  y = c(result$vp / 1000, result$vs / 1000, result$ro2)\n  if(result$vp < result$vs) {\n    return(-Inf)\n  }\n  else{\n  return(\n    sum(dnorm(y, mean = seismic_data$y, sd = seismic_data$sd,\n            log = TRUE)) #+ \n      # dbeta(theta[3], .1, .5, log = TRUE)\n  )\n  }\n}\n\n# THETA[1] = alpha\n# THETA[2] = porosity\n# THETA[3] = saturation\n# THETA[4] = kappa_m\n# THETA[5] = mu_m\n# THETA[6] = rho_m\n\n# starting point\nalpha = 0.5\nporosity = 0.25 #runif(1, 0.05, 0.50)\nsaturation = 0 #0 to 1\nkappa_m = 78\nmu_m = 33\nrhom = 3000\ntheta = c(alpha, porosity, saturation, kappa_m, mu_m, rhom)\n\n# prior\n## indicator that parameters are in proper ranges * constant * indicator that Vp > Vs\n\n# MCMC \nS = 5000\naccept = rep(0, length(theta))\n\nTHETA2 = NULL\nfor(i in 1:S) {\n  thetaStar = theta \n  thetaStar[1] = rcnorm(1, mean = theta[1], sd = .05,\n                        a = 0.03, b = 0.99)\n   log.r = logPosterior(thetaStar) + \n     dtruncnormL(theta[1], \n                a=0.03, b=0.99, \n                mean = thetaStar[1], sd = .05) - \n     logPosterior(theta) - \n     dtruncnormL(thetaStar[1], \n                a=0.03, b=0.99, \n                mean = theta[1], sd = .05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[1] = accept[1] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[2] = rcnorm(1, mean = theta[2], sd = .05,\n                        a = 0.05, b = 0.5)\n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[2], \n                a=0.05, b=0.5, \n                mean = theta[2], sd = .05) - \n     logPosterior(theta) - \n      dtruncnormL(thetaStar[2], \n                a=0.05, b=0.5, \n                mean = theta[2], sd = .05)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[2] = accept[2] + 1 \n   }\n  \n  # thetaStar = theta\n  # thetaStar[3] = rcnorm(1, mean = theta[3], sd = 0.05,\n  #                       a = 0, b = 1)\n  # \n  # log.r = logPosterior(thetaStar) +\n  #   dtruncnormL(thetaStar[3], \n  #               a=0, b=1, \n  #               mean = theta[3], sd = 0.05) - \n  #    logPosterior(theta) - \n  #   dtruncnormL(thetaStar[3], \n  #               a=0, b=1, \n  #               mean = theta[3], sd = 0.05)\n  #  \n  #  if(log(runif(1)) < log.r)  {\n  #   theta = thetaStar\n  #   accept[3] = accept[3] + 1 \n  #  }\n  \n  thetaStar = theta\n  thetaStar[4] = rcnorm(1, mean = theta[4], sd = .5,\n                        a = 76.5, b = 80)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[4], \n                a=76.5, b=80, \n                mean = theta[4], sd = .5) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[4], \n                a=76.5, b=80, \n                mean = theta[4], sd = .5)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[4] = accept[4] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[5] = rcnorm(1, mean = theta[5], sd = 1,\n                        a = 25.6, b = 40)\n  \n  log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[5], \n                a=25.6, b=40, \n                mean = theta[5], sd = 1) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[5], \n                a=25.6, b=40, \n                mean = theta[5], sd = 1)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[5] = accept[5] + 1 \n   }\n  \n  thetaStar = theta\n  thetaStar[6] = rcnorm(1, mean = theta[6], sd = 20,\n                        a = 2689, b = 2900)\n  \n   log.r = logPosterior(thetaStar) +\n    dtruncnormL(thetaStar[6], \n                a=2689, b=2900, \n                mean = theta[6], sd = 20) - \n     logPosterior(theta) -\n    dtruncnormL(thetaStar[6], \n                a=2689, b=2900, \n                mean = theta[6], sd = 20)\n   \n   if(log(runif(1)) < log.r)  {\n    theta = thetaStar\n    accept[6] = accept[6] + 1 \n   }\n  \n   THETA2 = rbind(THETA2, theta)\n}\n\nTHETA2 = data.frame(THETA2)\ncolnames(THETA2) = c(paste0(\"theta\", 1:6))\nrownames(THETA2) = NULL\n\n\nFinally, we compute the posterior distribution of the likelihood ratio.\n\n\nCode\ngetLogLikelihood =  function(y) {\n  stopifnot(length(y) == 3)\n  sum(dnorm(y, mean = seismic_data$y, sd = seismic_data$sd,\n            log = TRUE))\n}\n\nM1 = THETA %>%\n  slice_tail(n = 2500) %>%\n  apply(MARGIN = 1, FUN = myBerry)\nM0 = THETA2 %>%\n  slice_tail(n = 2500) %>%\n  apply(MARGIN = 1, FUN = myBerry)\n\nX1 = NULL\nfor (i in 1:length(M1)) {\n  x = unlist(M1[[i]][c(\"vp\", \"vs\", \"ro2\")])\n  x[1:2] = x[1:2] / 1000\n  X1 = rbind(X1, x)\n}\n\nX0 = NULL\nfor (i in 1:length(M0)) {\n  x = unlist(M0[[i]][c(\"vp\", \"vs\", \"ro2\")])\n  x[1:2] = x[1:2] / 1000\n  X0 = rbind(X0, x)\n}\n\nM1_ll = X1 %>%\n  apply(MARGIN = 1, FUN = getLogLikelihood)\n\nM0_ll = X0 %>%\n  apply(MARGIN = 1, FUN = getLogLikelihood)\n\nlogDiff = M1_ll - M0_ll\nhist(logDiff)\nmean(exp(logDiff))"
  },
  {
    "objectID": "hw/hw09.html#exercise-1",
    "href": "hw/hw09.html#exercise-1",
    "title": "Homework 9",
    "section": "Exercise 1",
    "text": "Exercise 1\n3.12 from Hoff."
  }
]