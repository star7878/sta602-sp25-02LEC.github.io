{
  "hash": "6218a2a86a62380994724d601ac2a917",
  "result": {
    "markdown": "---\ntitle: \"MCMC & confidence bands\"\nformat: \n    revealjs:\n      mainfont: Lato\n      smaller: true\n---\n\n\n# Exercises\n\n## Setup\n\nConsider the following data generative model and priors:\n\n\n$$\n\\begin{aligned}\nY_i | \\beta_0, \\beta_1, x_i &\\sim N(\\beta_0 + \\beta_1x_i, \\sigma^2)\\\\\n\\beta_0, \\beta_1 &\\sim \\text{ iid } N(0, 2)\\\\\n\\sigma^2 &\\sim \\text{inverse-gamma}(2, 3)\n\\end{aligned}\n$$\n\n\nTip: load `library(invgamma)` to use the `dinvgamma` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(invgamma)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the data\nYX = readr::read_csv(\"https://sta360-fa24.github.io/data/simulatedXY.csv\")\ny = YX$y\nx = YX$x\n```\n:::\n\n\n## Code\n\nBelow is code to sample from\n\n\n$$\np(\\beta_1, \\beta_2, \\sigma^2 | y_1, \\ldots, y_n, x_1, \\ldots, x_n)\n$$.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(360)\nlogLikelihood = function(beta0, beta1, sigma2) {\n  mu = beta0 + (beta1 * x)\n  sum(dnorm(y, mu, sqrt(sigma2), log = TRUE))\n}\n\nlogPrior = function(beta0, beta1, sigma2) {\n  dnorm(beta0, 0, sqrt(2), log = TRUE) + \n    dnorm(beta1, 0, sqrt(2), log = TRUE) +\n    dinvgamma(sigma2, 2, 3, log = TRUE)\n}\n\nlogPosterior = function(beta0, beta1, sigma2) {\n  logLikelihood(beta0, beta1, sigma2) + logPrior(beta0, beta1, sigma2)\n}\n\n\nBETA0 = NULL\nBETA1 = NULL\nSIGMA2 = NULL\n\naccept1 = 0\naccept2 = 0\naccept3 = 0\n\nS = 500\n\nbeta0_s = 0.1\nbeta1_s = 10\nsigma2_s = 1\nfor (s in 1:S) {\n  \n  ## propose and update beta0\n  beta0_proposal = rnorm(1, mean = beta0_s, .5)\n   log.r = logPosterior(beta0_proposal, beta1_s, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta0_s = beta0_proposal\n    accept1 = accept1 + 1 \n   }\n   \n   BETA0 = c(BETA0, beta0_s)\n   \n   ## propose and update beta1\n    beta1_proposal = rnorm(1, mean = beta1_s, .5)\n   log.r = logPosterior(beta0_s, beta1_proposal, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta1_s = beta1_proposal\n    accept2 = accept2 + 1 \n   }\n   \n   BETA1 = c(BETA1, beta1_s)\n   \n   ## propose and update sigma2\n   ### note: sigma2 is positive only, we want to only propose positive values\n   sigma2_proposal = 1 / rgamma(1, shape = 1, sigma2_s)\n   log.r = logPosterior(beta0_s, beta1_s, sigma2_proposal) - \n     logPosterior(beta0_s, beta1_s, sigma2_s) + \n     dinvgamma(sigma2_proposal, 1, sigma2_s, log = TRUE) - \n     dinvgamma(sigma2_s, 1, sigma2_proposal, log = TRUE) \n   \n   if(log(runif(1)) < log.r)  {\n    sigma2_s = sigma2_proposal\n    accept3 = accept3 + 1 \n   }\n   \n   SIGMA2 = c(SIGMA2, sigma2_s)\n   \n}\n```\n:::\n\n\n\n## Exercise 1\n\n- Is a Metropolis or Metropolis-Hastings algorithm used to sample $\\sigma^2$? Hint: What is the proposal distribution used to sample $\\sigma^2$? Is it symmetric? Can you show that it is or is not symmetric in R? \n\n- Create a trace plot for each parameter. How many iterations does it take for $\\beta_1$ to converge to the posterior? What is going on with the $\\sigma^2$ trace plot?\n\n- Report the effective sample size of each parameter, $\\beta_0$, $\\beta_1$, and $\\sigma^2$. Hint: load the `coda` package and check the lecture notes.\n\n## Exercise 2 \n\n- Go back and increase the length of the chain, $S$, until you obtain approximately 200 ESS for each parameter. Re-examine your plots and discuss.\n\n- Report the posterior median and a 90% confidence interval for each unknown. Why is the median a better summary than the mean for $\\sigma^2$?\n\n## Exercise 3\n\n- Plot the posterior mean $\\theta(x)$ vs $x$ where $\\theta(x) \\equiv \\beta_0 + \\beta_1 x$.\n\n- Additionally, plot a 95% confidence band for $\\theta(x)$ vs $x$ and interpret your plot.\n\n# Solutions\n\n## Solution 1, 2\n\n::: panel-tabset\n\n## Algo.\n\nA Metropolis-Hastings algorithm. inverse-gamma proposal is asymmetric. \n\nShowing with `R` code: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma2_proposal = 1.5\nsigma2_s = 1\ndinvgamma(sigma2_proposal, 1, sigma2_s) == \n  dinvgamma(sigma2_s, 1, sigma2_proposal)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\n## Trace plots (img)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lab-conf-band_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n\n- $\\beta_1$ converges to the target after about 50 iterations.\n\n- Some samples from inv-gamma are huge, the huge variance results in a flatter likelihood and still get accepted.\n\n\n## Trace plots (code)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\nparameterDF = data.frame(BETA0, BETA1, SIGMA2)\nn = nrow(parameterDF)\n\np0 = parameterDF %>%\n  ggplot(aes(x = 1:n)) +\n  geom_line(aes(y = BETA0)) +\n  theme_bw() +\n  labs(x = \"iteration\", y = \"beta0\")\n\np1 = parameterDF %>%\n  ggplot(aes(x = 1:n)) +\n  geom_line(aes(y = BETA1)) +\n  theme_bw() +\n  labs(x = \"iteration\", y = \"beta1\")\n\np2 = parameterDF %>%\n  ggplot(aes(x = 1:n)) +\n  geom_line(aes(y = SIGMA2)) +\n  theme_bw() +\n  labs(x = \"iteration\", y = \"sigma2\")\n\np0 + p1 + p2\n```\n:::\n\n\n## ESS\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(coda) \n\nparameterDF = data.frame(BETA0, BETA1, SIGMA2)\napply(parameterDF, 2, effectiveSize)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   BETA0    BETA1   SIGMA2 \n20.70251 12.72726 40.55270 \n```\n:::\n:::\n\n\n## Longer run\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(360)\nlogLikelihood = function(beta0, beta1, sigma2) {\n  mu = beta0 + (beta1 * x)\n  sum(dnorm(y, mu, sqrt(sigma2), log = TRUE))\n}\n\nlogPrior = function(beta0, beta1, sigma2) {\n  dnorm(beta0, 0, sqrt(2), log = TRUE) + \n    dnorm(beta1, 0, sqrt(2), log = TRUE) +\n    dinvgamma(sigma2, 2, 3, log = TRUE)\n}\n\nlogPosterior = function(beta0, beta1, sigma2) {\n  logLikelihood(beta0, beta1, sigma2) + logPrior(beta0, beta1, sigma2)\n}\n\n\nBETA0 = NULL\nBETA1 = NULL\nSIGMA2 = NULL\n\naccept1 = 0\naccept2 = 0\naccept3 = 0\n\nS = 10000\n\nbeta0_s = 0.1\nbeta1_s = 10\nsigma2_s = 1\nfor (s in 1:S) {\n  \n  ## propose and update beta0\n  beta0_proposal = rnorm(1, mean = beta0_s, .5)\n   log.r = logPosterior(beta0_proposal, beta1_s, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta0_s = beta0_proposal\n    accept1 = accept1 + 1 \n   }\n   \n   BETA0 = c(BETA0, beta0_s)\n   \n   ## propose and update beta1\n    beta1_proposal = rnorm(1, mean = beta1_s, .5)\n   log.r = logPosterior(beta0_s, beta1_proposal, sigma2_s) - \n     logPosterior(beta0_s, beta1_s, sigma2_s)\n   \n   if(log(runif(1)) < log.r)  {\n    beta1_s = beta1_proposal\n    accept2 = accept2 + 1 \n   }\n   \n   BETA1 = c(BETA1, beta1_s)\n   \n   ## propose and update sigma2\n   ### note: sigma2 is positive only, we want to only propose positive values\n   sigma2_proposal = 1 / rgamma(1, shape = 1, sigma2_s)\n   log.r = logPosterior(beta0_s, beta1_s, sigma2_proposal) - \n     logPosterior(beta0_s, beta1_s, sigma2_s) + \n     dinvgamma(sigma2_proposal, 1, sigma2_s, log = TRUE) - \n     dinvgamma(sigma2_s, 1, sigma2_proposal, log = TRUE) \n   \n   if(log(runif(1)) < log.r)  {\n    sigma2_s = sigma2_proposal\n    accept3 = accept3 + 1 \n   }\n   \n   SIGMA2 = c(SIGMA2, sigma2_s)\n   \n}\n\nparameterDF = data.frame(BETA0, BETA1, SIGMA2)\napply(parameterDF, 2, effectiveSize)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   BETA0    BETA1   SIGMA2 \n405.7465 241.8751 826.7580 \n```\n:::\n:::\n\n\n## Post medians\n\n\n::: {.cell}\n::: {.cell-output-display}\n|       |post. median |CI               |\n|:------|:------------|:----------------|\n|beta0  |0.5          |(-1.1, 1.9)      |\n|beta1  |2            |(-0.1, 2.5)      |\n|sigma2 |13.4         |(7.3, 2537295.4) |\n:::\n:::\n\n\n:::\n\n\n## Solution 3\n\n::: panel-tabset\n\n## plot\n\n::: {.cell}\n::: {.cell-output-display}\n![](lab-conf-band_files/figure-revealjs/unnamed-chunk-11-1.png){width=960}\n:::\n:::\n\n\nThe red line shows our posterior expectation of $\\theta(x)$ for each $x$. \nThe black bands show our 95% confidence interval $\\theta(x)$.\n\n## code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_theta_CI = function(X) {\n     f = BETA0 + (BETA1 * X)\n     return(quantile(f, c(0.025, 0.975)))\n}\n\nget_theta_mean = function(X) {\n  f = BETA0 + (BETA1 * X)\n  return(mean(f))\n}\n\nxlo = min(x)\nxhi = max(x)\nxVal = seq(xlo, xhi, by = 0.01)\nlower = NULL\nupper = NULL\nM = NULL\n   \nfor (i in seq_along(xVal)) {\n  theta_CI = get_theta_CI(xVal[i])\n  lower = c(lower, theta_CI[[1]])\n  upper = c(upper, theta_CI[[2]])\n  M = c(M, get_theta_mean(xVal[i]))\n}\n\ndf = data.frame(xVal, lower, upper, M)\ndf %>%\n  ggplot(aes(x = xVal)) +\n  geom_line(aes(y = lower)) +\n  geom_line(aes(y = upper)) +\n  geom_line(aes(y = M), col = \"red\") +\n  theme_bw() +\n  labs(y = \"theta\",\n       x = \"x\")\n```\n:::\n\n:::\n\n<!-- ## Exercise 1 -->\n\n<!-- Let $p(\\theta_1, \\theta_2 | \\mathbf{y})$ be our **target distribution**, i.e. the distribution we are interested in sampling. -->\n\n<!-- We construct a Gibbs sampler and look at the trace plots of $\\theta_1$ and $\\theta_2$, produced below. Chat with your neighbor, describe what you observe. Has the chain **converged** for each parameter? How well is the sampler **mixing**? Do you think the parameters are correlated or uncorrelated?  -->\n\n<!-- Write down a description of the plots below, in 2 paragraphs or less, for a reader who has **not taken this class**. You may assume the reader has taken a course on probability (e.g. similar to STA230/240). -->\n\n\n<!-- ```{r} -->\n<!-- #| echo: false -->\n<!-- #| warning: false -->\n<!-- library(tidyverse) -->\n<!-- library(latex2exp) -->\n<!-- library(patchwork) -->\n<!-- beta0 = c(rnorm(750, mean = 10, sd = 1), rnorm(250, mean = 50, sd = 4)) -->\n<!-- beta1 = c(rnorm(750, mean = 45, sd = 2), rnorm(250, mean = 53, sd = 1)) -->\n\n<!-- df = data.frame(beta0, beta1) -->\n<!-- p1 = df %>% -->\n<!-- ggplot(aes(x = 1:1000)) + -->\n<!--   geom_line(aes(y = beta0)) + -->\n<!--   theme_bw() + -->\n<!--   labs(y = TeX(\"$\\\\theta_1$\"), x = \"iteration\") -->\n\n<!-- p2 = df %>% -->\n<!-- ggplot(aes(x = 1:1000)) + -->\n<!--   geom_line(aes(y = beta1)) + -->\n<!--   theme_bw() + -->\n<!--   labs(y = TeX(\"$\\\\theta_2$\"), x = \"iteration\") -->\n\n<!-- p1 + p2 -->\n<!-- ``` -->\n\n<!-- ## Exercise 2 -->\n\n<!-- Based on the first 1000 iterations of your Gibbs sampler shown on the previous slide, which of the following joint densities is the most plausible for $\\theta_1, \\theta_2 | \\mathbf{y}$? Why? Hint: it may help to think about where your sampler starts and imagine a particle moving through space according to conditional updates. -->\n\n<!-- ```{r} -->\n<!-- #| echo: false -->\n<!-- #| warning: false -->\n\n<!-- beta0_true = c(rnorm(3000, mean = 10, sd = 1), rnorm(1000, mean = 50, sd = 4)) -->\n<!-- beta1_true = c(rnorm(3000, mean = 45, sd = 2), rnorm(1000, mean = 53, sd = 1)) -->\n\n<!-- beta0_base =  c(rnorm(2000, mean = 10, sd = 1), rnorm(1000, mean = 50, sd = 4), -->\n<!--                  rnorm(1000, mean = 80, sd = 3)) -->\n\n<!-- beta1_base = c(rnorm(2000, mean = 45, sd = 2), rnorm(1000, mean = 53, sd = 1), -->\n<!--                 rnorm(1000, mean = 35, sd = 2)) -->\n\n\n<!-- df_base = data.frame(beta0 = beta0_base, beta1 = beta1_base) -->\n<!-- df_true = data.frame(beta0 = beta0_true, beta1 = beta1_true) -->\n\n<!-- p1 = df_true %>% -->\n<!--   mutate(beta0 = beta0 / 10, -->\n<!--          beta1 = beta1 / 10) %>% -->\n<!--   ggplot(aes(x = beta0, y = beta1)) +  -->\n<!--   geom_bin2d(bins = 25) + -->\n<!--   theme_bw() +  -->\n<!--   labs(y = TeX(\"$\\\\theta_2$\"),  -->\n<!--        x = TeX(\"$\\\\theta_1$\")) -->\n\n<!-- df2 = df_base -->\n\n<!-- df2$beta0 = rnorm(4000, mean = 50, sd = 4) -->\n\n<!-- p2 = df2 %>% -->\n<!--   ggplot(aes(x = beta0, y = beta1)) + -->\n<!--   geom_bin2d(bins = 25) + -->\n<!--   theme_bw() + -->\n<!--   labs(y = TeX(\"$\\\\theta_2$\"), -->\n<!--        x = TeX(\"$\\\\theta_1$\")) -->\n\n<!-- df3 = df_base -->\n<!-- df3$beta0 = rnorm(4000, mean = 35, sd = 15) -->\n<!-- df3$beta1 = rnorm(4000, mean = 48, sd = 10) -->\n\n\n<!-- p3 = df3 %>% -->\n<!--   ggplot(aes(x = beta0, y = beta1)) + -->\n<!--   geom_bin2d(bins = 25) + -->\n<!--   theme_bw() + -->\n<!--   labs(y = TeX(\"$\\\\theta_2$\"), -->\n<!--        x = TeX(\"$\\\\theta_1$\")) -->\n\n<!-- df4 = df_base -->\n\n<!-- p4 = df4 %>% -->\n<!--   ggplot(aes(x = beta0, y = beta1)) + -->\n<!--   geom_bin2d(bins = 25) + -->\n<!--   theme_bw() + -->\n<!--   labs(y = TeX(\"$\\\\theta_2$\"), -->\n<!--        x = TeX(\"$\\\\theta_1$\")) -->\n\n\n<!-- (p1 + p2) / (p3 + p4) + -->\n<!--   plot_annotation(title = \"Joint densities\", tag_levels = c(\"A\")) -->\n<!-- ``` -->\n\n<!-- ## Exercise 3 -->\n\n\n\n",
    "supporting": [
      "lab-conf-band_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}